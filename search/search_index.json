{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Starling \u00b6 The purpose of Starling is to update the control systems within the Bristol Robotics Laboratory Flight Arena in order to reduce the barrier to entry for those who wish to experiment with, and fly real drones. This project takes recent advancements in cloud computing, namely containerisation (docker) and container orchestration (kubernetes) as the core framework to manage networking and drone deployment, whether virtual (using SITL) or on real drones running companion computers (e.g. raspberry pi). However a key requirement of the system is to allow users to be able to operate drones without needing to know more about the udnerlying implementation. This systems provides a number of key features. Primary support for ROS2 and MavLink Built in simulation stack based on Gazebo Quick transition from simulation to flying controllers on real drones. Documentation \u00b6 Please refer to the documentation at https://docs.starlinguas.dev/ for detailed instructions and explanations of how to use this system. The documentation is built using MKDocs and can be served locally. Install requirements pip install -r docs/requirements.txt In the project root, run make docs or make serve-docs if you want live viewing (serving to localhost:8000 ) Setup \u00b6 For the simplest usage, a simple Command Line Interface and examples are available in the Murmuration repository . Refer to the documentation for furhter details. git clone https://github.com/StarlingUAS/Murmuration.git For inspecting and developing core Starling clone the repository recursively so that all submodules are included. git clone --recurse-submodules https://github.com/StarlingUAS/ProjectStarling.git Features \u00b6 Drone Sandbox \u00b6 Starling encapsulates much of the set up and configuration required for you to get your drone based experiments up and running. It encapsulates the installation and running of simulators (e.g. Gazebo), the configuration of Software-In-The-Loop (SITL) for common autopilot software (e.g. PX4 or Ardupilot) and the deployment of these systems across a network of machines. This streamlines the controller development process, and hopefully allows you to develop your controllers and experiments much faster and more directly, hopefully without having to deal with lower level configuration or networking issues. Simulation to Reality \u00b6 Due to using containerisation, all these packages are availble to be deployed on any machine at a moments notice. This includes the companion computer on your vehicle. This way you can write and test code against the simulator to later be executed on the real vehicles with no change on your end. Single and Multiple Drones \u00b6 Starling supports both single and multiple drone centralised and decentralised applications out of the box as a nice side effect of the architecture of the system. Multiple drones are generated by simply scaling the drone process, in theory allowing for as many drones as your machine can handle. By virtue of the architecture, the system also includes failure recovery and detection mechanics. Contact \u00b6 The initial developers of this project are: Mickey Li - email - github Robert Clarke - email - github FAQs \u00b6 Please see documentation, or raise an Issue on the github repo License \u00b6 This project is released under the MIT license. Please see the License file for more details.","title":"Overview"},{"location":"#project-starling","text":"The purpose of Starling is to update the control systems within the Bristol Robotics Laboratory Flight Arena in order to reduce the barrier to entry for those who wish to experiment with, and fly real drones. This project takes recent advancements in cloud computing, namely containerisation (docker) and container orchestration (kubernetes) as the core framework to manage networking and drone deployment, whether virtual (using SITL) or on real drones running companion computers (e.g. raspberry pi). However a key requirement of the system is to allow users to be able to operate drones without needing to know more about the udnerlying implementation. This systems provides a number of key features. Primary support for ROS2 and MavLink Built in simulation stack based on Gazebo Quick transition from simulation to flying controllers on real drones.","title":"Project Starling"},{"location":"#documentation","text":"Please refer to the documentation at https://docs.starlinguas.dev/ for detailed instructions and explanations of how to use this system. The documentation is built using MKDocs and can be served locally. Install requirements pip install -r docs/requirements.txt In the project root, run make docs or make serve-docs if you want live viewing (serving to localhost:8000 )","title":"Documentation"},{"location":"#setup","text":"For the simplest usage, a simple Command Line Interface and examples are available in the Murmuration repository . Refer to the documentation for furhter details. git clone https://github.com/StarlingUAS/Murmuration.git For inspecting and developing core Starling clone the repository recursively so that all submodules are included. git clone --recurse-submodules https://github.com/StarlingUAS/ProjectStarling.git","title":"Setup"},{"location":"#features","text":"","title":"Features"},{"location":"#drone-sandbox","text":"Starling encapsulates much of the set up and configuration required for you to get your drone based experiments up and running. It encapsulates the installation and running of simulators (e.g. Gazebo), the configuration of Software-In-The-Loop (SITL) for common autopilot software (e.g. PX4 or Ardupilot) and the deployment of these systems across a network of machines. This streamlines the controller development process, and hopefully allows you to develop your controllers and experiments much faster and more directly, hopefully without having to deal with lower level configuration or networking issues.","title":"Drone Sandbox"},{"location":"#simulation-to-reality","text":"Due to using containerisation, all these packages are availble to be deployed on any machine at a moments notice. This includes the companion computer on your vehicle. This way you can write and test code against the simulator to later be executed on the real vehicles with no change on your end.","title":"Simulation to Reality"},{"location":"#single-and-multiple-drones","text":"Starling supports both single and multiple drone centralised and decentralised applications out of the box as a nice side effect of the architecture of the system. Multiple drones are generated by simply scaling the drone process, in theory allowing for as many drones as your machine can handle. By virtue of the architecture, the system also includes failure recovery and detection mechanics.","title":"Single and Multiple Drones"},{"location":"#contact","text":"The initial developers of this project are: Mickey Li - email - github Robert Clarke - email - github","title":"Contact"},{"location":"#faqs","text":"Please see documentation, or raise an Issue on the github repo","title":"FAQs"},{"location":"#license","text":"This project is released under the MIT license. Please see the License file for more details.","title":"License"},{"location":"details/adding_physical_drone/","text":"Setting up a new Vehicles for the flight arena \u00b6 Setting up a new Vehicles for the flight arena Vehicle Specification Aerial Vehicles Adding a new vehicle to Starling Companion Computer Boot Image Vehicle Specific Setup Adding vehicle as a node. Vehicle Specification \u00b6 Starling can be made compatible with almost all vehicles both ground and aerial. In the majority of cases, a vehicle is valid if it has enough compute to run docker containers and connect to the network. The recommended minimum is a Raspberry Pi 4 with 2Gb RAM with WiFi connectivity. The compute should be sufficient such that all the containers can be run safely without saturating the CPU, Memory or Netowrk Bandwidth. Aerial Vehicles \u00b6 The current system is designed with a PX4/Ardupilot compatible flight control (such as a Pixhawk or PixRacer) running in tandem with a companion computer such as a Raspberry Pi. The current risk assessment covers multi-vehicle flight for vehicles < 1kg maximum take off mass. Adding a new vehicle to Starling \u00b6 Note: You will need SSH access to the flight arena server. Companion Computer Boot Image \u00b6 We have created a custom image which contains the minimum general elements to be compatible with Starling. The image has been generated using the code in the starling-64-mod branch of UobFlightLab/pi-gen . We have released a precompiled image file which can be flashed onto a USB or SD-card and booted from using a tool such as the Raspberry Pi Imager The image is based on Ubuntu 20.04 for ARM chips (Raspberry Pi), and has the following elements: Password-less sudo Automatic connectivity to the Flight Arena LAN k3s airgapped installation files placed into root with k3s_install.sh executable GPIO drivers installed To ensure connectivity with the pixhawk, 99-px4fmu.rules has been added to /lib/udev/rules.d to ensure the pixhwak is connected via /dev/px4fmu . Chrony library has been installed and that /etc/chonry/chonry.conf is configured to use the flight arena server at 192.168.10.80 (see the BRL flight arena doc) The Docker Daemon registry mirrors has been set via /etc/docker/daemon.json to use the flight arena server at 192.168.10.80:5000 to ensure the drone can pull docker images from the internet. (see the BRL flight arena doc) Vehicle Specific Setup \u00b6 A vehicle ID number from 10 to 255 must be chosen. IDs below 10 are reserved. It must be checked that IDs do not clash with existing vehicles (i.e. the Coex Clover drones take ids 11 - 14). If the vehicle is going to be used with PX4 or Ardupilot, the flight controller system id should be chaned to the vehicle ID. e.g. setting MAV_SYS_ID in QGroundControl for Px4 based systems. If the vehicle is going to be used with the Mavros Container , a /etc/starling/vehicle.config file is required. An example is the following: VEHICLE_FCU_URL=serial:///dev/px4fmu:115200 VEHICLE_FIRMWARE=px4 VEHICLE_MAVLINK_SYSID=23 VEHICLE_NAME=clover23 Adding vehicle as a node. \u00b6 The vehicle can now be added as a node to the cluster. Ensure that the vehicle is only connected to the LAN either by WIFI or Ethernet. Identify the IP address of the vehicle by going to the DHCP server on 192.168.10.252 , or other means SSH into the flying server and cd into the ProjectStarling/scripts directory Run the agent setup script start_k3s_agent.sh . This will ssh into the vehicle and run the setup script with the keys held by the server. See this doc for more details. ./start_k3s_agent.sh <remote username> <remote ip address> <node name> e.g. ./start_k8_agent.sh ubuntu 192.168.0.110 clover1 Add the following tags to the node either through the edit functionality in the kubernetes dashboard (the pencil in the upper right corner) or manually through the command line. The tag keys must be fixed, but the values can be varied depending on application. These are required for automatic deployment of containers. starling.dev/type: vehicle starling.dev/vehicle-class: rotary starling.dev/vehicle-type: clover The vehicle should now be visible under nodes on the Kubernetes dashboard.","title":"Adding A New Vehicle"},{"location":"details/adding_physical_drone/#setting-up-a-new-vehicles-for-the-flight-arena","text":"Setting up a new Vehicles for the flight arena Vehicle Specification Aerial Vehicles Adding a new vehicle to Starling Companion Computer Boot Image Vehicle Specific Setup Adding vehicle as a node.","title":"Setting up a new Vehicles for the flight arena"},{"location":"details/adding_physical_drone/#vehicle-specification","text":"Starling can be made compatible with almost all vehicles both ground and aerial. In the majority of cases, a vehicle is valid if it has enough compute to run docker containers and connect to the network. The recommended minimum is a Raspberry Pi 4 with 2Gb RAM with WiFi connectivity. The compute should be sufficient such that all the containers can be run safely without saturating the CPU, Memory or Netowrk Bandwidth.","title":"Vehicle Specification"},{"location":"details/adding_physical_drone/#aerial-vehicles","text":"The current system is designed with a PX4/Ardupilot compatible flight control (such as a Pixhawk or PixRacer) running in tandem with a companion computer such as a Raspberry Pi. The current risk assessment covers multi-vehicle flight for vehicles < 1kg maximum take off mass.","title":"Aerial Vehicles"},{"location":"details/adding_physical_drone/#adding-a-new-vehicle-to-starling","text":"Note: You will need SSH access to the flight arena server.","title":"Adding a new vehicle to Starling"},{"location":"details/adding_physical_drone/#companion-computer-boot-image","text":"We have created a custom image which contains the minimum general elements to be compatible with Starling. The image has been generated using the code in the starling-64-mod branch of UobFlightLab/pi-gen . We have released a precompiled image file which can be flashed onto a USB or SD-card and booted from using a tool such as the Raspberry Pi Imager The image is based on Ubuntu 20.04 for ARM chips (Raspberry Pi), and has the following elements: Password-less sudo Automatic connectivity to the Flight Arena LAN k3s airgapped installation files placed into root with k3s_install.sh executable GPIO drivers installed To ensure connectivity with the pixhawk, 99-px4fmu.rules has been added to /lib/udev/rules.d to ensure the pixhwak is connected via /dev/px4fmu . Chrony library has been installed and that /etc/chonry/chonry.conf is configured to use the flight arena server at 192.168.10.80 (see the BRL flight arena doc) The Docker Daemon registry mirrors has been set via /etc/docker/daemon.json to use the flight arena server at 192.168.10.80:5000 to ensure the drone can pull docker images from the internet. (see the BRL flight arena doc)","title":"Companion Computer Boot Image"},{"location":"details/adding_physical_drone/#vehicle-specific-setup","text":"A vehicle ID number from 10 to 255 must be chosen. IDs below 10 are reserved. It must be checked that IDs do not clash with existing vehicles (i.e. the Coex Clover drones take ids 11 - 14). If the vehicle is going to be used with PX4 or Ardupilot, the flight controller system id should be chaned to the vehicle ID. e.g. setting MAV_SYS_ID in QGroundControl for Px4 based systems. If the vehicle is going to be used with the Mavros Container , a /etc/starling/vehicle.config file is required. An example is the following: VEHICLE_FCU_URL=serial:///dev/px4fmu:115200 VEHICLE_FIRMWARE=px4 VEHICLE_MAVLINK_SYSID=23 VEHICLE_NAME=clover23","title":"Vehicle Specific Setup"},{"location":"details/adding_physical_drone/#adding-vehicle-as-a-node","text":"The vehicle can now be added as a node to the cluster. Ensure that the vehicle is only connected to the LAN either by WIFI or Ethernet. Identify the IP address of the vehicle by going to the DHCP server on 192.168.10.252 , or other means SSH into the flying server and cd into the ProjectStarling/scripts directory Run the agent setup script start_k3s_agent.sh . This will ssh into the vehicle and run the setup script with the keys held by the server. See this doc for more details. ./start_k3s_agent.sh <remote username> <remote ip address> <node name> e.g. ./start_k8_agent.sh ubuntu 192.168.0.110 clover1 Add the following tags to the node either through the edit functionality in the kubernetes dashboard (the pencil in the upper right corner) or manually through the command line. The tag keys must be fixed, but the values can be varied depending on application. These are required for automatic deployment of containers. starling.dev/type: vehicle starling.dev/vehicle-class: rotary starling.dev/vehicle-type: clover The vehicle should now be visible under nodes on the Kubernetes dashboard.","title":"Adding vehicle as a node."},{"location":"details/background/","text":"Background \u00b6 Motivation \u00b6 As robotic and autonomous systems proliferate into the wider world, there is a need to address the difficulties of system development and deployment at scale. There is evidence that industry is directly facing these challenges through the use of cloud computing, continuous integration and similar systems inspired from very successful and agile software development processes. This is made clear through offerings such as Amazon's AWS Robomaker , Google's cloud robotics platforms and so on. However, there is a great lack of such systems in most academic settings. The result's oriented attitude of many labs often leads to each researcher building a bespoke solution in order to evaluate, validate or prove their goals. These bespoke solutions are often inflexible, not extensible, difficult to understand and, importantly, reuse, with any level of confidence. This becomes especially difficult when coupled with hardware, such as UAVs, where many operational details have been implicitly assumed or ignored for favour of getting the experiment running as quick as possible. In addition these solutions are often poorly structured and maintained with little to no documentation meaning that it is difficult for researchers to build upon these systems. This is an exceptionally large hurdle to researchers who do not have strong software backgrounds, but wish to perform real world experiments which could improve the quality of research outputs. This is not to say that it is impossible for a research system to be developed into a reusable platform. There are many examples of research systems being ubiquitous within a group or being released outside the lab. For instance, the Robotarium at Georgia Tech , the Multi-Robot Systems Group at the Czech Technical University with their experimental system , and the PX4 autopilot which began it's life as a collaboration between a number of labs at ETH Zurich. But what we see is that it takes a concerted effort and many years of coincidental work which provide incremental improvements to the system each time. Inspired by the increasing adoption of paradigms from cloud and distributed computing, this work aims to provide a UAV experimentation system which: Supports single or multiple drones with low (control) or high level (path planning) experimentation. Supports the transition between simulation to indoor flight to outdoor flight. Provides a simple and easy to use interface for researchers to implement experimentation without knowledge of hardware specifics. System Overview \u00b6 Starling makes use of containerisation ( Docker ) and container orchestration technologies (Kubernetes) to enable consistent deployment environments regardless of underlying platform - whether simulated or real. Many previous systems also aim for a similar capability, but the use of containerisation in particular allows Starling to abstract away from hardware whilst minimising setup and configuration. This is key to increase the efficiency for the development and testing of UAV systems in our resource and time constrained research laboratory. Starling has been primarily been devised with the following two insights: Firstly, a key concept of Starling is to treat individual vehicles as if they are individual nodes in a compute cluster. A compute cluster is traditionally a physical set of servers within a data centre, where each node is one hardware server with its own resources. Starling therefore considers a group of UAVs as a mobile compute cluster where each UAV is a 'flying computer'. With this viewpoint, actions such as software deployment, scaling up and down, self-healing and others can start to have a physical meaning within a UAV application. Secondly, Starling embraces the modularity inherent in using both ROS and containerisation. ROS provides the framework for communication, and containers provide the portable foundation upon which it runs. In particular Starling provides a core set of containers which have been identified as being mandatory when running a vehicle. These include a pre-configured container which runs MAVROS, as well as defining environment variables and protocols to ensure that vehicles are unique and can communicate in a scalable manner with both its hardware and other vehicles without additional user configuration. With this modularity through containers, we can then exploit all of the recent advances in compute cluster deployment and control for use in a single and multi UAV testing environment. Technology Stack \u00b6 This system has been tested on linux ubuntu 20.04 . It should be compatible with any linux distribution compatible with Docker. It may work on windows and MacOS, however it is untested and there may be networking issues See Link . This system also assumes the use of ROS2 Foxy for communication between nodes, MavLink for autopilot communication with Mavros used as the bridge. PX4 is currently the main tested firmware, although ardupilot is also on the list. The core container images (i.e. container executables) of this system are all hosted in the uobflightlabstarling docker hub repository: https://hub.docker.com/orgs/uobflightlabstarling/repositories In our system each Docker Container can contain one or more ROS2 nodes. For example the uobflightlabstarling / starling-mavros container contains Mavros and its dependencies and require no modification by most users. Each docker container can be run by using Docker run... . Multiple containers can be run simultaneously using the docker-compose tool and specifying a yaml configuration file.","title":"Background"},{"location":"details/background/#background","text":"","title":"Background"},{"location":"details/background/#motivation","text":"As robotic and autonomous systems proliferate into the wider world, there is a need to address the difficulties of system development and deployment at scale. There is evidence that industry is directly facing these challenges through the use of cloud computing, continuous integration and similar systems inspired from very successful and agile software development processes. This is made clear through offerings such as Amazon's AWS Robomaker , Google's cloud robotics platforms and so on. However, there is a great lack of such systems in most academic settings. The result's oriented attitude of many labs often leads to each researcher building a bespoke solution in order to evaluate, validate or prove their goals. These bespoke solutions are often inflexible, not extensible, difficult to understand and, importantly, reuse, with any level of confidence. This becomes especially difficult when coupled with hardware, such as UAVs, where many operational details have been implicitly assumed or ignored for favour of getting the experiment running as quick as possible. In addition these solutions are often poorly structured and maintained with little to no documentation meaning that it is difficult for researchers to build upon these systems. This is an exceptionally large hurdle to researchers who do not have strong software backgrounds, but wish to perform real world experiments which could improve the quality of research outputs. This is not to say that it is impossible for a research system to be developed into a reusable platform. There are many examples of research systems being ubiquitous within a group or being released outside the lab. For instance, the Robotarium at Georgia Tech , the Multi-Robot Systems Group at the Czech Technical University with their experimental system , and the PX4 autopilot which began it's life as a collaboration between a number of labs at ETH Zurich. But what we see is that it takes a concerted effort and many years of coincidental work which provide incremental improvements to the system each time. Inspired by the increasing adoption of paradigms from cloud and distributed computing, this work aims to provide a UAV experimentation system which: Supports single or multiple drones with low (control) or high level (path planning) experimentation. Supports the transition between simulation to indoor flight to outdoor flight. Provides a simple and easy to use interface for researchers to implement experimentation without knowledge of hardware specifics.","title":"Motivation"},{"location":"details/background/#system-overview","text":"Starling makes use of containerisation ( Docker ) and container orchestration technologies (Kubernetes) to enable consistent deployment environments regardless of underlying platform - whether simulated or real. Many previous systems also aim for a similar capability, but the use of containerisation in particular allows Starling to abstract away from hardware whilst minimising setup and configuration. This is key to increase the efficiency for the development and testing of UAV systems in our resource and time constrained research laboratory. Starling has been primarily been devised with the following two insights: Firstly, a key concept of Starling is to treat individual vehicles as if they are individual nodes in a compute cluster. A compute cluster is traditionally a physical set of servers within a data centre, where each node is one hardware server with its own resources. Starling therefore considers a group of UAVs as a mobile compute cluster where each UAV is a 'flying computer'. With this viewpoint, actions such as software deployment, scaling up and down, self-healing and others can start to have a physical meaning within a UAV application. Secondly, Starling embraces the modularity inherent in using both ROS and containerisation. ROS provides the framework for communication, and containers provide the portable foundation upon which it runs. In particular Starling provides a core set of containers which have been identified as being mandatory when running a vehicle. These include a pre-configured container which runs MAVROS, as well as defining environment variables and protocols to ensure that vehicles are unique and can communicate in a scalable manner with both its hardware and other vehicles without additional user configuration. With this modularity through containers, we can then exploit all of the recent advances in compute cluster deployment and control for use in a single and multi UAV testing environment.","title":"System Overview"},{"location":"details/background/#technology-stack","text":"This system has been tested on linux ubuntu 20.04 . It should be compatible with any linux distribution compatible with Docker. It may work on windows and MacOS, however it is untested and there may be networking issues See Link . This system also assumes the use of ROS2 Foxy for communication between nodes, MavLink for autopilot communication with Mavros used as the bridge. PX4 is currently the main tested firmware, although ardupilot is also on the list. The core container images (i.e. container executables) of this system are all hosted in the uobflightlabstarling docker hub repository: https://hub.docker.com/orgs/uobflightlabstarling/repositories In our system each Docker Container can contain one or more ROS2 nodes. For example the uobflightlabstarling / starling-mavros container contains Mavros and its dependencies and require no modification by most users. Each docker container can be run by using Docker run... . Multiple containers can be run simultaneously using the docker-compose tool and specifying a yaml configuration file.","title":"Technology Stack"},{"location":"details/build_system/","text":"Project Starling Development and Build System \u00b6 Project Starling Development and Build System Development Workflow Building Building locally and for ARM using buildx bake The GitHub Actions Workflows Releases Development Releases Images on DockerHub Updating the cache Development Workflow \u00b6 As this project gets larger and releases get pushed out, the standard single master branch is not longer viable. Therefore we start using a more scalable (and safer) method of project management (read this article for more info). master branch is reserved solely for core releases and should always remain stable. Master should only be updated and modified via pull requests, and each pull request should correspond to a new tagged release. A release will then trigger the continuous integration github action (see below) to update all of the docker images. dev branch is for general development and should always remain ahead of the master branch. All feature branches should merge into dev when verified. When enough features have been added, dev can raise a PR to merge into master. For testing dev releases can be created to trigger dev builds on \":nightly\" tags. Building \u00b6 Building locally and for ARM using buildx bake \u00b6 At present the build system is based on Docker's buildx bake , using a bake.hcl file to configure the build. The docker-bake.hcl file contains definitions for each image to be built and which platforms they are to be built for. At the moment, bake doesn't have a way of setting dependencies between images so the images need to be built in the correct order. At present, there are three groups defined in the bake.hcl file to set the ordering. These are named stage1 through stage3 . The build_local.sh script calls buildx bake for each of these stages in turn, ensuring that all dependencies are in place for the next stage of the build. Another aspect of this lack of dependency tracking is that images that extend other images need to explicitly use the the correct version. In order to keep this portable, Dockerfiles that depend on an existing image have had a VERSION build argument added. This is appended to the end of the name of the parent Docker image. The default value is latest , which is equivalent to leaving it blank in Docker tooling. To create a set of images with matching tags, use an environment variable BAKE_VERSION . This will be appended as a tag to the images and passed as the VERSION build argument to dockerfiles that depend on other starling-* images. Multiplatform support is more complicated. The local docker image store cannot handle multi-platform images so buildx 's default docker driver cannot be used. build_local_multiplatform.sh deals with this by spawning a new builder using the docker-container driver alongside a local container registry that the builder interacts with. This significantly complicates things. To setup your local machine to do the multiplatform builds, you need to install the required emulators for the arm64 builds. Luckily someone has already done the hard work. All that should be required is: docker run --privileged --rm tonistiigi/binfmt --install arm64 Similar to the VERSION argument outlined above, the use of a local registry requires a REGISTRY build argument be used in the Dockerfiles. Again this is provided by the bake.hcl script. It defaults to blank, which is equivalent to using Docker Hub. Once built, the images can be pulled from the local registry using a localhost:5000/ prefix. Override by setting BAKE_REGISTRY in the environment before calling bake . The bake.hcl script takes values from the environment to be passed on to the Dockerfiles. Two of these are the BAKE_VERSION and BAKE_REGISTRY arguments outlined above. BAKE_RELEASENAME can also be supplied to the bake.hcl script. BAKE_RELEASENAME defaults to blank. If it is set, all images will be tagged with both the tag specified by BAKE_VERSION (or latest if that is not set) and that specified by BAKE_RELEASENAME . Finally, there are some further options to control caching. The main options are BAKE_CACHETO_NAME and BAKE_CACHEFROM_NAME , which by default are blank. These exists to allow local builds to be cached from online sources and pushed to a local registry or to allow local builds to be used to populate the online caches. When they are blank, no caches will be used. This allows for one-directional caching, e.g. refreshing the online caches from local builds. Two further caching options are available: BAKE_CACHETO_REGISTRY and BAKE_CACHEFROM_REGISTRY . These control the destination and source registries for the cache images. By default, they will be blank, equivalent to using Docker Hub. The GitHub Actions Workflows \u00b6 Releases \u00b6 When a tag of the form vX.Y.Z is pushed to the repo, a workflow will be started. This workflow builds all the images from the bake.hcl script, tags them with both the version tag and :latest , and pushes them to Docker Hub. Development Releases \u00b6 A similar workflow exists for development images. A tag of the form vX.Y.Z-dev will cause the images to be built and tagged with both the tag name and :nightly , before being pushed to DockerHub. These additional tags are controlled by setting the BAKE_RELEASENAME environment variable. The actions workflows attempt to make use of buildx 's cache to repository. The :cache and :cache-dev tags are used for each image for the caches. This is done by setting the BAKE_CACHENAME variable. There's likely the opportunity to streamline the two workflows into one, which should reduce maintainence. Images on DockerHub \u00b6 The set of images are automatically updated on DockerHub. Each image will have a set of tags: :latest tracking the most recent push to master branch :nightly tracking the most recent push to the dev branch :vX.Y.Z fixed release tags :${BRANCH} tracking most recent push to PR branches while active Updating the cache \u00b6 The builds can be run locally to update the cache if GitHub is timing out. starling-mavros is usually the culprit. The command below will update the nightly tag on Docker Hub, both caching from and to the dev caches. BAKE_VERSION=nightly BAKE_CACHEFROM_NAME=cache-dev BAKE_CACHETO_NAME=cache-dev docker buildx bake -f buildtools/docker-bake.hcl --push starling-mavros","title":"Build System"},{"location":"details/build_system/#project-starling-development-and-build-system","text":"Project Starling Development and Build System Development Workflow Building Building locally and for ARM using buildx bake The GitHub Actions Workflows Releases Development Releases Images on DockerHub Updating the cache","title":"Project Starling Development and Build System"},{"location":"details/build_system/#development-workflow","text":"As this project gets larger and releases get pushed out, the standard single master branch is not longer viable. Therefore we start using a more scalable (and safer) method of project management (read this article for more info). master branch is reserved solely for core releases and should always remain stable. Master should only be updated and modified via pull requests, and each pull request should correspond to a new tagged release. A release will then trigger the continuous integration github action (see below) to update all of the docker images. dev branch is for general development and should always remain ahead of the master branch. All feature branches should merge into dev when verified. When enough features have been added, dev can raise a PR to merge into master. For testing dev releases can be created to trigger dev builds on \":nightly\" tags.","title":"Development Workflow"},{"location":"details/build_system/#building","text":"","title":"Building"},{"location":"details/build_system/#building-locally-and-for-arm-using-buildx-bake","text":"At present the build system is based on Docker's buildx bake , using a bake.hcl file to configure the build. The docker-bake.hcl file contains definitions for each image to be built and which platforms they are to be built for. At the moment, bake doesn't have a way of setting dependencies between images so the images need to be built in the correct order. At present, there are three groups defined in the bake.hcl file to set the ordering. These are named stage1 through stage3 . The build_local.sh script calls buildx bake for each of these stages in turn, ensuring that all dependencies are in place for the next stage of the build. Another aspect of this lack of dependency tracking is that images that extend other images need to explicitly use the the correct version. In order to keep this portable, Dockerfiles that depend on an existing image have had a VERSION build argument added. This is appended to the end of the name of the parent Docker image. The default value is latest , which is equivalent to leaving it blank in Docker tooling. To create a set of images with matching tags, use an environment variable BAKE_VERSION . This will be appended as a tag to the images and passed as the VERSION build argument to dockerfiles that depend on other starling-* images. Multiplatform support is more complicated. The local docker image store cannot handle multi-platform images so buildx 's default docker driver cannot be used. build_local_multiplatform.sh deals with this by spawning a new builder using the docker-container driver alongside a local container registry that the builder interacts with. This significantly complicates things. To setup your local machine to do the multiplatform builds, you need to install the required emulators for the arm64 builds. Luckily someone has already done the hard work. All that should be required is: docker run --privileged --rm tonistiigi/binfmt --install arm64 Similar to the VERSION argument outlined above, the use of a local registry requires a REGISTRY build argument be used in the Dockerfiles. Again this is provided by the bake.hcl script. It defaults to blank, which is equivalent to using Docker Hub. Once built, the images can be pulled from the local registry using a localhost:5000/ prefix. Override by setting BAKE_REGISTRY in the environment before calling bake . The bake.hcl script takes values from the environment to be passed on to the Dockerfiles. Two of these are the BAKE_VERSION and BAKE_REGISTRY arguments outlined above. BAKE_RELEASENAME can also be supplied to the bake.hcl script. BAKE_RELEASENAME defaults to blank. If it is set, all images will be tagged with both the tag specified by BAKE_VERSION (or latest if that is not set) and that specified by BAKE_RELEASENAME . Finally, there are some further options to control caching. The main options are BAKE_CACHETO_NAME and BAKE_CACHEFROM_NAME , which by default are blank. These exists to allow local builds to be cached from online sources and pushed to a local registry or to allow local builds to be used to populate the online caches. When they are blank, no caches will be used. This allows for one-directional caching, e.g. refreshing the online caches from local builds. Two further caching options are available: BAKE_CACHETO_REGISTRY and BAKE_CACHEFROM_REGISTRY . These control the destination and source registries for the cache images. By default, they will be blank, equivalent to using Docker Hub.","title":"Building locally and for ARM using buildx bake"},{"location":"details/build_system/#the-github-actions-workflows","text":"","title":"The GitHub Actions Workflows"},{"location":"details/build_system/#releases","text":"When a tag of the form vX.Y.Z is pushed to the repo, a workflow will be started. This workflow builds all the images from the bake.hcl script, tags them with both the version tag and :latest , and pushes them to Docker Hub.","title":"Releases"},{"location":"details/build_system/#development-releases","text":"A similar workflow exists for development images. A tag of the form vX.Y.Z-dev will cause the images to be built and tagged with both the tag name and :nightly , before being pushed to DockerHub. These additional tags are controlled by setting the BAKE_RELEASENAME environment variable. The actions workflows attempt to make use of buildx 's cache to repository. The :cache and :cache-dev tags are used for each image for the caches. This is done by setting the BAKE_CACHENAME variable. There's likely the opportunity to streamline the two workflows into one, which should reduce maintainence.","title":"Development Releases"},{"location":"details/build_system/#images-on-dockerhub","text":"The set of images are automatically updated on DockerHub. Each image will have a set of tags: :latest tracking the most recent push to master branch :nightly tracking the most recent push to the dev branch :vX.Y.Z fixed release tags :${BRANCH} tracking most recent push to PR branches while active","title":"Images on DockerHub"},{"location":"details/build_system/#updating-the-cache","text":"The builds can be run locally to update the cache if GitHub is timing out. starling-mavros is usually the culprit. The command below will update the nightly tag on Docker Hub, both caching from and to the dev caches. BAKE_VERSION=nightly BAKE_CACHEFROM_NAME=cache-dev BAKE_CACHETO_NAME=cache-dev docker buildx bake -f buildtools/docker-bake.hcl --push starling-mavros","title":"Updating the cache"},{"location":"details/docker/","text":"Docker \u00b6 Docker Documentation \u00b6 For an introduction to Docker & containers, see the official docs . Containerisation \u00b6 With traditional methods, code is developed in a specific computing environment which, when transferred to a new location, often results in bugs and errors. For example, when a developer transfers code from a desktop computer to a virtual machine (VM) or from a Linux to a Windows operating system. This is especially true when developing systems for hardware. For drones in particular, existing UAV software development often involved a lot of system specific solutions tied to the hardware. For example, most drones and controlling computers are configured only for their designated applications, tying both computer and drone to that application. It also meant that offline testing, and testing away from the physical drone was impossible, hampering the development process as well. Containerisation eliminates this problem by bundling the application code together with the related configuration files, libraries, and dependencies required for it to run. This single package of software or \u201ccontainer\u201d is abstracted away from the host operating system, and hence, it stands alone and becomes portable\u2014able to run across any platform or cloud (UAV included), free of issues. Under the hood, containerisation is a form of system virtualisation where applications are run in isolated user spaces. This is also why they are often referred to as lightweight as containerisation does not virtualise the entirety of an operating system for each application, instead allowing for containers to share use of the host machine's operating system kernel. Because of this, containers are inherently smaller in capacity than a full VM and require less start-up time and system resources. In essence each container can be thought of simply as an easy to use 'executable' which is run by a runtime engine, in our case Docker. In addition the abstraction from the host operating system makes containerised applications portable and able to run uniformly and consistently across any platform or cloud. Containers can be easily transported from a desktop computer to a virtual machine (VM) or from a Linux to a Windows operating system, making it perfect for software development on any machine. DockerFiles \u00b6 A Docker container is specified by the user using the definition of a Dockerfile. An example Dockerfile is given here in this code snippet. FROM ${REGISTRY}uobflightlabstarling/starling-controller-base:${VERSION} COPY example_controller_python /ros_ws/src/example_controller_python RUN . /ros_ws/install/setup.sh && colcon build CMD [ \"/bin/bash\" ] A Dockerfile is then built to create an executable which can then be run. Note the difference between build time commands and runtime commands. A built Dockerfile comprises of layers, where, in general, each command specified in the Dockerfile describes a layer. Our example describes a container image with 4 layers, one for each command. This is important because layers can be downloaded once and shared between among multiple containers, again reducing runtime overhead - especially on resource constrained platforms such as the companion computers on UAVs.","title":"Understanding Docker"},{"location":"details/docker/#docker","text":"","title":"Docker"},{"location":"details/docker/#docker-documentation","text":"For an introduction to Docker & containers, see the official docs .","title":"Docker Documentation"},{"location":"details/docker/#containerisation","text":"With traditional methods, code is developed in a specific computing environment which, when transferred to a new location, often results in bugs and errors. For example, when a developer transfers code from a desktop computer to a virtual machine (VM) or from a Linux to a Windows operating system. This is especially true when developing systems for hardware. For drones in particular, existing UAV software development often involved a lot of system specific solutions tied to the hardware. For example, most drones and controlling computers are configured only for their designated applications, tying both computer and drone to that application. It also meant that offline testing, and testing away from the physical drone was impossible, hampering the development process as well. Containerisation eliminates this problem by bundling the application code together with the related configuration files, libraries, and dependencies required for it to run. This single package of software or \u201ccontainer\u201d is abstracted away from the host operating system, and hence, it stands alone and becomes portable\u2014able to run across any platform or cloud (UAV included), free of issues. Under the hood, containerisation is a form of system virtualisation where applications are run in isolated user spaces. This is also why they are often referred to as lightweight as containerisation does not virtualise the entirety of an operating system for each application, instead allowing for containers to share use of the host machine's operating system kernel. Because of this, containers are inherently smaller in capacity than a full VM and require less start-up time and system resources. In essence each container can be thought of simply as an easy to use 'executable' which is run by a runtime engine, in our case Docker. In addition the abstraction from the host operating system makes containerised applications portable and able to run uniformly and consistently across any platform or cloud. Containers can be easily transported from a desktop computer to a virtual machine (VM) or from a Linux to a Windows operating system, making it perfect for software development on any machine.","title":"Containerisation"},{"location":"details/docker/#dockerfiles","text":"A Docker container is specified by the user using the definition of a Dockerfile. An example Dockerfile is given here in this code snippet. FROM ${REGISTRY}uobflightlabstarling/starling-controller-base:${VERSION} COPY example_controller_python /ros_ws/src/example_controller_python RUN . /ros_ws/install/setup.sh && colcon build CMD [ \"/bin/bash\" ] A Dockerfile is then built to create an executable which can then be run. Note the difference between build time commands and runtime commands. A built Dockerfile comprises of layers, where, in general, each command specified in the Dockerfile describes a layer. Our example describes a container image with 4 layers, one for each command. This is important because layers can be downloaded once and shared between among multiple containers, again reducing runtime overhead - especially on resource constrained platforms such as the companion computers on UAVs.","title":"DockerFiles"},{"location":"details/flight_arena/","text":"Flight Arena Architecture and Set Up \u00b6 Flight Arena Architecture and Set Up Architecture Flight Arena Networking Starling Mavlink Broadcast ROS2 DDS Broadcast Kubernetes Control Layer Flight Arena Details Using kubectl from Remote Machines Automatic Container Deployment via DaemonSets Time Synchronisation Installation Config Vehicle side Ground side Restart Troubleshooting Docker Local Registry as a Pass Through Cache Vehicle Side Ground Side Architecture \u00b6 A side effect of using kubernetes and docker is that the system is mostly network agnostic. As long as all elements of the system are on the same physical network, all elements should be able to communicate. Flight Arena Networking \u00b6 The flight arena (FA) runs on its own airgapped local network. The FA network exists on the CIDR subnet 192.168.10.0/24 . There is a external dhcp server sitting on 192.168.10.252 which (as of writing) is configured to serve addresses to new devices from 192.168.10.105 - 192.168.10.199 . Addresses below 105 are reserved for the flight arena machines, server and the VICON motion tracking system. The flight arena machines then all have 2 network cards, one connecting to the flight arena network (grey cables), and one connecting to the internet via the UWE network (blue cables). These machines run from 192.168.10.83 - 192.168.10.85 . The flight arena server has been set up on 192.168.10.80 (this can also be accessed via flyingserver.local ) and is host centralised aspects of the system. Starling \u00b6 The kubernetes master node is set to run on the flight arena server on 192.168.10.80 or flyingserver.local . Any worker nodes (drones) must have the master node registered. Any machines/ work laptops that want to submit deployments to the cluster must have the cluster server set to the address of the arena server within their k3s.yaml config file ( ~/.kube/config/k3s.yaml ). Note: Security and proper cluster authentication should be set up in the future As all pods should be running net=host all traffic moves over the 192.168.10.0/24 subnet. The figure above shows an example application in action. There are essentially three independent communication channels: Mavlink broadcast over the network on port 14550 UDP. ROS2 DDS broadcast over the network. Kuberenetes control layer networks ( 10.42.0.0/24 ) Mavlink Broadcast \u00b6 When a drone (simulated or physical) is connected as a node and the starling-mavros container is run on it, mavros will begin broadcasting to port 14550. This broadcast is conducted over the flight arena network and any machine on the network can pick up these broadcasts. Any drone can be flown directly through mavlink if desired. ROS2 DDS Broadcast \u00b6 It is envisioned that the majority of user facing communication is conducted through ROS2. For instance communication between controllers, or controlling external hardware on the drones such as LED lights. In ROS2, communication is conducted through the DDS middleware protocol . DDS is a standard for machine-to-machine high-performance, real-time, scalable communication which uses a publish-subscribe pattern over UDP. Each ROS2 node will broadcast its list of available publishers to the whole network. Another node can subscribe and data transfer between the two will commence. This means that any machine or laptop connected to the flight arena network and has ROS2 foxy running on the operating system will have access to the list of topics. These external machines do not have to be part of the cluster . This gives a user-friendly way of developing and interacting with the cluster. Kubernetes Control Layer \u00b6 Kubernetes generates its own network layer between its nodes on 10.42.0.0/24 . These are primarily extra deployments, daemonsets and services which facilitate pod failure recovery, automatic deployment and web based user interface port forwarding. Flight Arena Details \u00b6 Using kubectl from Remote Machines \u00b6 To use kubectl from a remote machine, a copy of the k3s.yaml from the master machine is needed. On the machine you want to use kubectl from, run: export KUBECONFIG=/path/to/k3s.yaml This adds the KUBECONFIG variable to the environment. To make it permanent, it needs to be added to the .bashrc . Once this is done, kubectl can be used as normal and will connect to the master. To test, run: kubectl cluster-info You should see Kubernetes control plane is running at... reflecting your Kubernetes master. Automatic Container Deployment via DaemonSets \u00b6 There are 3 daemonsets configured to perform automatic deployment of containers on any node which satisfies the daemonsets constraints on tags. This should be updated as more are added or they are taken away. Deployment of starling-mavros on any node with tag starling.dev/vehicle-class:rotary Deployment of starling-vicon on any node with tag starling.dev/type: vehicle Deployment of starling-clover on any node with tag starling.dev/vehicle-type: clover Time Synchronisation \u00b6 When running the Vicon node on a separate PC, the clock of the onboard PC needs to be closely synched to allow the position estimates to be used by PX4. You can do this with chrony . Installation \u00b6 One liner if package is available: sudo apt install chrony Config \u00b6 chrony 's configuration is in /etc/chrony/chrony.conf . The configuration needed depends on which side of the system is being configured. Vehicle side \u00b6 On the vehicle, chrony needs to be configured to add a time server on the local network. This can be done by adding the below to the config file: # Use local time source server ${GROUND_PC_IP} iburst prefer Ground side \u00b6 On the ground, chrony is configured to accept connections from clients on the local network: # Allow clients to connect to this computer allow ${LOCAL_SUBNET_PREFIX} # e.g. 192.168.10 bindaddress ${GROUND_PC_IP} local stratum 10 Restart \u00b6 Once the edits to the config file have been made, restart chrony through systemd : sudo systemctl restart chrony Troubleshooting \u00b6 chronyc sources will show the current status of the computer's time sync. chronyc makestep should force the computer to synchronise with the server. Docker Local Registry as a Pass Through Cache \u00b6 In order for the drones on the flight arena network to pull docker images from docker hub, the flight arena server has a docker registry running as a pull through cache. The drones must know to attempt to pull images from the local flight server first. Vehicle Side \u00b6 On the vehicle, the docker daemon needs to be updated to use the flight arena server as a registry mirror. Add the following into /etc/docker/daemon.json : { \"registry_mirrors\": [\"192.168.10.80:5000\"] } Ground Side \u00b6 The flight arena server has a registry docker image running that is configured using the following guide in docker docs","title":"BRL Flight Arena"},{"location":"details/flight_arena/#flight-arena-architecture-and-set-up","text":"Flight Arena Architecture and Set Up Architecture Flight Arena Networking Starling Mavlink Broadcast ROS2 DDS Broadcast Kubernetes Control Layer Flight Arena Details Using kubectl from Remote Machines Automatic Container Deployment via DaemonSets Time Synchronisation Installation Config Vehicle side Ground side Restart Troubleshooting Docker Local Registry as a Pass Through Cache Vehicle Side Ground Side","title":"Flight Arena Architecture and Set Up"},{"location":"details/flight_arena/#architecture","text":"A side effect of using kubernetes and docker is that the system is mostly network agnostic. As long as all elements of the system are on the same physical network, all elements should be able to communicate.","title":"Architecture"},{"location":"details/flight_arena/#flight-arena-networking","text":"The flight arena (FA) runs on its own airgapped local network. The FA network exists on the CIDR subnet 192.168.10.0/24 . There is a external dhcp server sitting on 192.168.10.252 which (as of writing) is configured to serve addresses to new devices from 192.168.10.105 - 192.168.10.199 . Addresses below 105 are reserved for the flight arena machines, server and the VICON motion tracking system. The flight arena machines then all have 2 network cards, one connecting to the flight arena network (grey cables), and one connecting to the internet via the UWE network (blue cables). These machines run from 192.168.10.83 - 192.168.10.85 . The flight arena server has been set up on 192.168.10.80 (this can also be accessed via flyingserver.local ) and is host centralised aspects of the system.","title":"Flight Arena Networking"},{"location":"details/flight_arena/#starling","text":"The kubernetes master node is set to run on the flight arena server on 192.168.10.80 or flyingserver.local . Any worker nodes (drones) must have the master node registered. Any machines/ work laptops that want to submit deployments to the cluster must have the cluster server set to the address of the arena server within their k3s.yaml config file ( ~/.kube/config/k3s.yaml ). Note: Security and proper cluster authentication should be set up in the future As all pods should be running net=host all traffic moves over the 192.168.10.0/24 subnet. The figure above shows an example application in action. There are essentially three independent communication channels: Mavlink broadcast over the network on port 14550 UDP. ROS2 DDS broadcast over the network. Kuberenetes control layer networks ( 10.42.0.0/24 )","title":"Starling"},{"location":"details/flight_arena/#mavlink-broadcast","text":"When a drone (simulated or physical) is connected as a node and the starling-mavros container is run on it, mavros will begin broadcasting to port 14550. This broadcast is conducted over the flight arena network and any machine on the network can pick up these broadcasts. Any drone can be flown directly through mavlink if desired.","title":"Mavlink Broadcast"},{"location":"details/flight_arena/#ros2-dds-broadcast","text":"It is envisioned that the majority of user facing communication is conducted through ROS2. For instance communication between controllers, or controlling external hardware on the drones such as LED lights. In ROS2, communication is conducted through the DDS middleware protocol . DDS is a standard for machine-to-machine high-performance, real-time, scalable communication which uses a publish-subscribe pattern over UDP. Each ROS2 node will broadcast its list of available publishers to the whole network. Another node can subscribe and data transfer between the two will commence. This means that any machine or laptop connected to the flight arena network and has ROS2 foxy running on the operating system will have access to the list of topics. These external machines do not have to be part of the cluster . This gives a user-friendly way of developing and interacting with the cluster.","title":"ROS2 DDS Broadcast"},{"location":"details/flight_arena/#kubernetes-control-layer","text":"Kubernetes generates its own network layer between its nodes on 10.42.0.0/24 . These are primarily extra deployments, daemonsets and services which facilitate pod failure recovery, automatic deployment and web based user interface port forwarding.","title":"Kubernetes Control Layer"},{"location":"details/flight_arena/#flight-arena-details","text":"","title":"Flight Arena Details"},{"location":"details/flight_arena/#using-kubectl-from-remote-machines","text":"To use kubectl from a remote machine, a copy of the k3s.yaml from the master machine is needed. On the machine you want to use kubectl from, run: export KUBECONFIG=/path/to/k3s.yaml This adds the KUBECONFIG variable to the environment. To make it permanent, it needs to be added to the .bashrc . Once this is done, kubectl can be used as normal and will connect to the master. To test, run: kubectl cluster-info You should see Kubernetes control plane is running at... reflecting your Kubernetes master.","title":"Using kubectl from Remote Machines"},{"location":"details/flight_arena/#automatic-container-deployment-via-daemonsets","text":"There are 3 daemonsets configured to perform automatic deployment of containers on any node which satisfies the daemonsets constraints on tags. This should be updated as more are added or they are taken away. Deployment of starling-mavros on any node with tag starling.dev/vehicle-class:rotary Deployment of starling-vicon on any node with tag starling.dev/type: vehicle Deployment of starling-clover on any node with tag starling.dev/vehicle-type: clover","title":"Automatic Container Deployment via DaemonSets"},{"location":"details/flight_arena/#time-synchronisation","text":"When running the Vicon node on a separate PC, the clock of the onboard PC needs to be closely synched to allow the position estimates to be used by PX4. You can do this with chrony .","title":"Time Synchronisation"},{"location":"details/flight_arena/#installation","text":"One liner if package is available: sudo apt install chrony","title":"Installation"},{"location":"details/flight_arena/#config","text":"chrony 's configuration is in /etc/chrony/chrony.conf . The configuration needed depends on which side of the system is being configured.","title":"Config"},{"location":"details/flight_arena/#vehicle-side","text":"On the vehicle, chrony needs to be configured to add a time server on the local network. This can be done by adding the below to the config file: # Use local time source server ${GROUND_PC_IP} iburst prefer","title":"Vehicle side"},{"location":"details/flight_arena/#ground-side","text":"On the ground, chrony is configured to accept connections from clients on the local network: # Allow clients to connect to this computer allow ${LOCAL_SUBNET_PREFIX} # e.g. 192.168.10 bindaddress ${GROUND_PC_IP} local stratum 10","title":"Ground side"},{"location":"details/flight_arena/#restart","text":"Once the edits to the config file have been made, restart chrony through systemd : sudo systemctl restart chrony","title":"Restart"},{"location":"details/flight_arena/#troubleshooting","text":"chronyc sources will show the current status of the computer's time sync. chronyc makestep should force the computer to synchronise with the server.","title":"Troubleshooting"},{"location":"details/flight_arena/#docker-local-registry-as-a-pass-through-cache","text":"In order for the drones on the flight arena network to pull docker images from docker hub, the flight arena server has a docker registry running as a pull through cache. The drones must know to attempt to pull images from the local flight server first.","title":"Docker Local Registry as a Pass Through Cache"},{"location":"details/flight_arena/#vehicle-side_1","text":"On the vehicle, the docker daemon needs to be updated to use the flight arena server as a registry mirror. Add the following into /etc/docker/daemon.json : { \"registry_mirrors\": [\"192.168.10.80:5000\"] }","title":"Vehicle Side"},{"location":"details/flight_arena/#ground-side_1","text":"The flight arena server has a registry docker image running that is configured using the following guide in docker docs","title":"Ground Side"},{"location":"details/implementation_ap/","text":"ArduPilot Implementation \u00b6 ArduPilot support consists of the SITL images, some mavros configuration, and","title":"ArduPilot Implementation"},{"location":"details/implementation_ap/#ardupilot-implementation","text":"ArduPilot support consists of the SITL images, some mavros configuration, and","title":"ArduPilot Implementation"},{"location":"details/kubernetes-dashboard/","text":"Kubernetes Dashboard \u00b6 Kubernetes Dashboard Quick GIF Logging In Navigating the Dashboard Quick GIF \u00b6 Logging In \u00b6 Connecting to https://localhost:31771 may show a security problem. This is caused by certificate issues. Accept the risk and continue if you are comfortable (this is serving locally anyhow :) ) This will give the following log on page The token can be found from the initial start of the dashboard, or can be accessed using the following command: kubectl -n kubernetes-dashboard describe secret admin-user-token # or if using the starling cli starling utils get-dashboard-token Navigating the Dashboard \u00b6 Once logged in, the dashboard frontpage looks like the following. This can be accessed via the 'Workloads' button. It shows the current status of the cluster. On the left hand side there is a navigation bar showing the various 'resources' or computational loads which are running on the cluster. The key types of resources for us is the Deployment , Pod , Stateful Set and Services . See kubernetes docs page for more details. For example, selecting the Pods on the left hand panel opens up the currently running pods on the cluster. Here you can see starling-px4-0 , gazebo-v1- pods which run the simulated drone and the gazebo simulator, as well as a numver of other pods running networking and other functions. Any particular Pod can be inspected by clicking on the Pods name. For example, inspecting the starling-px4-0 gives the following page. It specifies several useful details including: pod name, pod creation time, pod labels, ip address and a number of other things You can access both the container logs and also access the container shell to directly execute commands for testing. The buttons for both are in the top right corner of the UI. Once you have clicked the logs icon, the logs are streamed from the container inside the pod. For example here we see the starling-px4-sitl container within the starling-px4-sitl-0 pod. The terminal can be scrolled up and down, but has maximum size. If the terminal output has reached maximum size, the arrows in the bottom right can be used to navigate terminal 'pages'. Also note that the terminal does not update in real time. To force update the terminal, click on one of the arrows in the bottom right, or select the auto-update option in the options menu in the top right next to the downloads symbol. Finally, if a pod has multiple containers running within it, the logs of each container can be viewed using the drop down menu accessed by clicking the container name.","title":"Kubernetes Dashboard Guide"},{"location":"details/kubernetes-dashboard/#kubernetes-dashboard","text":"Kubernetes Dashboard Quick GIF Logging In Navigating the Dashboard","title":"Kubernetes Dashboard"},{"location":"details/kubernetes-dashboard/#quick-gif","text":"","title":"Quick GIF"},{"location":"details/kubernetes-dashboard/#logging-in","text":"Connecting to https://localhost:31771 may show a security problem. This is caused by certificate issues. Accept the risk and continue if you are comfortable (this is serving locally anyhow :) ) This will give the following log on page The token can be found from the initial start of the dashboard, or can be accessed using the following command: kubectl -n kubernetes-dashboard describe secret admin-user-token # or if using the starling cli starling utils get-dashboard-token","title":"Logging In"},{"location":"details/kubernetes-dashboard/#navigating-the-dashboard","text":"Once logged in, the dashboard frontpage looks like the following. This can be accessed via the 'Workloads' button. It shows the current status of the cluster. On the left hand side there is a navigation bar showing the various 'resources' or computational loads which are running on the cluster. The key types of resources for us is the Deployment , Pod , Stateful Set and Services . See kubernetes docs page for more details. For example, selecting the Pods on the left hand panel opens up the currently running pods on the cluster. Here you can see starling-px4-0 , gazebo-v1- pods which run the simulated drone and the gazebo simulator, as well as a numver of other pods running networking and other functions. Any particular Pod can be inspected by clicking on the Pods name. For example, inspecting the starling-px4-0 gives the following page. It specifies several useful details including: pod name, pod creation time, pod labels, ip address and a number of other things You can access both the container logs and also access the container shell to directly execute commands for testing. The buttons for both are in the top right corner of the UI. Once you have clicked the logs icon, the logs are streamed from the container inside the pod. For example here we see the starling-px4-sitl container within the starling-px4-sitl-0 pod. The terminal can be scrolled up and down, but has maximum size. If the terminal output has reached maximum size, the arrows in the bottom right can be used to navigate terminal 'pages'. Also note that the terminal does not update in real time. To force update the terminal, click on one of the arrows in the bottom right, or select the auto-update option in the options menu in the top right next to the downloads symbol. Finally, if a pod has multiple containers running within it, the logs of each container can be viewed using the drop down menu accessed by clicking the container name.","title":"Navigating the Dashboard"},{"location":"details/kubernetes-deployment/","text":"Deployment with kubernetes and K3S \u00b6 NOTE This is now a little out of date and is superceeded by using KIND and the starling CLI . Please see that for local testing while this is being updated. Contents \u00b6 Deployment with kubernetes and K3S Contents Quick Reference to files: Files Using files Installation instructions Running instructions Laptop Pi / Drone / Agent Setup script via ssh Manual, old setup method. Post actions Dashboard This system is intended to run as a cloud platform. We utilse k3s as our kubernetes manager. Key concepts are as follows: - pods are a k8 concept. A pod contains one or more containers and has its own ip address. Containers within a pod communicate over localhost - node is the machine (physical e.g. pi or virtual machine) upon which pods are run. (Separate from 'ros2 nodes' or 'ros nodes') - kubectl is the command line program required to interface with kubernetes. - cni container networking interface (default is flannel for k3s) is the underlying networking for all containers - dds/ fast-rtps Is the default communications middleware for ros2 comms. Refer to the kubernetes notes with the onenote notebook for more usage information. Quick Reference to files: \u00b6 The .yaml files in this directory are all kubernetes configurations for various combinations of systems. The cpu architecture refers to where the containers have been specified to run - amd64 specifies for running on master machine, and arm64 specifies for running on the raspberry pi node over the network (see below for setup). All of these config files pull their images from the uobflightlabstarling docker hub . Files \u00b6 k8.gazebo-iris.amd64.yaml :- Currently runs the starling-sim-iris image and a Service which exposes the gzweb statically on localhost:8080 . This service has a cluster internal hostname of sim-gazebo.gazebo-srv k8.px4-sitl.amd64.yaml :- Currently runs a pod containing two containers starling-sim-px4-sitl - emulating px4-sitl. Talks to GCS software on port 14550 with replies on 18570. starling-mavros - contains a ROS2 mavros node connected via udp://localhost:14540 to sitl. Talks to GCS on udp broadcast port 14553. k8.ap-sitl.amd64.yaml :- [Needs updating]Currently runs a pod containing two containers starling-ardupilot-sitl - emulating ardupilot-sitl. Talks to GCS over 14553 as well. starling-mavros - contains a ROS2 mavros node connected via tcp://localhost:5762 to sitl. Talks to GCS on udp broadcast port 14553. k8.mavros.arm64.yaml :- A mavros node designed to run on the raspberry pi/ drone control computer. This pod contains a single starling-mavros container. It reads of a px4 pixhawk assumed to be talking over usb serial connection /dev/px4fmu (set up via udev symlinks). Currently assumes mavlink sysid is 1. k8.ros_monitor.amd64.yaml :- runs starling-mavros and a network-tools container. Can be used for debugging ROS2 and networking issues Using files \u00b6 Once k3s has been installed (see below, or run ./run_k3s.sh in the home directory), these configurations can be used in the cluster as follows: # Applying/ Creating them sudo k3s kubectl apply -f <filename.yaml> # Deleting the deployment sudo k3s kubectl delete -f <filename.yaml> -f <filename.yaml> This can also be done in the gui dashboard application. Note: Local images can be used if imagePullPolicy is set to ifNotPresent instead of Always . In that case it will attempt to find a local image with the given image name. arm64 images must be cross compiled using docker buildx (make multi-arch or make cross-compile or similar in the relevant docker files) and always pulled from docker hub. Installation instructions \u00b6 It is recommended that you use the ./run_k3s.sh script in the root of the repository. This script can be re-run at any time after install. If k3s is already installed and the relevant pods are running it will not do anything Install k3s using the install script, this will fetch k3s and run the kubernetes master node immediately in the background: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--docker\" sh - For the raspberry pi, ensure docker is installed, and then these instructions are the same on the raspberry pi (64 bit os). For testing purposes (inside testing dir), the containers have already been built for both amd64 and arm64 and uploaded onto hub.docker: mickeyli789/ros_demo . Also recommended you alias kubectl (kubernetes cli) in your bashrc alias kubectl='sudo k3s kubectl Running instructions \u00b6 Laptop \u00b6 It is recommended that you use the ./run_k3s.sh script in the root of the repository. This script will download the latest version of k3s run the master kubernetes server using docker (instead of containerd if you need access to local images) ' curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--docker\" sh - ' (will run in background as systemd - check systemctl status k3s ) This will open up a server with entrypoint on 0.0.0.0:6443 which corresponds to <host local ip address>:6443 Pi / Drone / Agent \u00b6 First ensure that the pi has been correctly set up with an airgapped installation of k3s, see here for installation instructions . Follow the Manually Deploy Images Method. The script below assumes that the the images file and the k3s executable are in the user home directory. Setup script via ssh \u00b6 Identify the ip address of the pi, the root enabled (possibly password disabled) username. Then from this directory run ./start_k3s_agent.sh <remote username> <remote ip address> <node name> e.g. ./start_k8_agent.sh ubuntu 192.168.0.110 clover1 You can specify the k3s server address by setting the environment variable before calling: K3S_SERVER=https://192.168.0.63:6443 k3s_agent ubuntu 192.168.0.96 raspi1 Manual, old setup method. \u00b6 First SSH onto the pi First ensure you run k3s-killall.sh to make sure there is no master server running as you only want k3s agent to run. The K3S_TOKEN is the contents of the file /var/lib/rancher/k3s/server/node-token K3S_TOKEN=<contents of the file /var/lib/rancher/k3s/server/node-token> #e.g. K3S_TOKEN=K103b62838822f40f3e41j51f10cb127236f2c3014c120ede19263da9f33fbfc859::server:2dcbb32a4cad16e20d714d88dbce4af8 K3S_SERVER=https://<Your main machine ip address>:6443 K3S_NODE_NAME=clover1 echo \"Killing all k3s services and instances first\" k3s-killall.sh echo \"Starting k3s agent only\" sudo k3s agent -t ${K3S_TOKEN} -s ${K3S_SERVER} --node-name ${K3S_NODE_NAME} The Pi should now be setup Consider running the above using screen or somehow in the background just in case your ssh connection is unstable or you want to close it. Post actions \u00b6 If you want to stop kubernetes completely, the internet install script comes with two options which are on the PATH and can be run in the terminal. 1. k3s-killall.sh will stop all k3s nodes and the systemd 2. k3s-uninstall.sh will delete everything k3s and remove the systemd Dashboard \u00b6 See the k3s docs for info on how to run Are started automatically in the ./run_k3s.sh script.","title":"Example Kubernetes Deployments"},{"location":"details/kubernetes-deployment/#deployment-with-kubernetes-and-k3s","text":"NOTE This is now a little out of date and is superceeded by using KIND and the starling CLI . Please see that for local testing while this is being updated.","title":"Deployment with kubernetes and K3S"},{"location":"details/kubernetes-deployment/#contents","text":"Deployment with kubernetes and K3S Contents Quick Reference to files: Files Using files Installation instructions Running instructions Laptop Pi / Drone / Agent Setup script via ssh Manual, old setup method. Post actions Dashboard This system is intended to run as a cloud platform. We utilse k3s as our kubernetes manager. Key concepts are as follows: - pods are a k8 concept. A pod contains one or more containers and has its own ip address. Containers within a pod communicate over localhost - node is the machine (physical e.g. pi or virtual machine) upon which pods are run. (Separate from 'ros2 nodes' or 'ros nodes') - kubectl is the command line program required to interface with kubernetes. - cni container networking interface (default is flannel for k3s) is the underlying networking for all containers - dds/ fast-rtps Is the default communications middleware for ros2 comms. Refer to the kubernetes notes with the onenote notebook for more usage information.","title":"Contents"},{"location":"details/kubernetes-deployment/#quick-reference-to-files","text":"The .yaml files in this directory are all kubernetes configurations for various combinations of systems. The cpu architecture refers to where the containers have been specified to run - amd64 specifies for running on master machine, and arm64 specifies for running on the raspberry pi node over the network (see below for setup). All of these config files pull their images from the uobflightlabstarling docker hub .","title":"Quick Reference to files:"},{"location":"details/kubernetes-deployment/#files","text":"k8.gazebo-iris.amd64.yaml :- Currently runs the starling-sim-iris image and a Service which exposes the gzweb statically on localhost:8080 . This service has a cluster internal hostname of sim-gazebo.gazebo-srv k8.px4-sitl.amd64.yaml :- Currently runs a pod containing two containers starling-sim-px4-sitl - emulating px4-sitl. Talks to GCS software on port 14550 with replies on 18570. starling-mavros - contains a ROS2 mavros node connected via udp://localhost:14540 to sitl. Talks to GCS on udp broadcast port 14553. k8.ap-sitl.amd64.yaml :- [Needs updating]Currently runs a pod containing two containers starling-ardupilot-sitl - emulating ardupilot-sitl. Talks to GCS over 14553 as well. starling-mavros - contains a ROS2 mavros node connected via tcp://localhost:5762 to sitl. Talks to GCS on udp broadcast port 14553. k8.mavros.arm64.yaml :- A mavros node designed to run on the raspberry pi/ drone control computer. This pod contains a single starling-mavros container. It reads of a px4 pixhawk assumed to be talking over usb serial connection /dev/px4fmu (set up via udev symlinks). Currently assumes mavlink sysid is 1. k8.ros_monitor.amd64.yaml :- runs starling-mavros and a network-tools container. Can be used for debugging ROS2 and networking issues","title":"Files"},{"location":"details/kubernetes-deployment/#using-files","text":"Once k3s has been installed (see below, or run ./run_k3s.sh in the home directory), these configurations can be used in the cluster as follows: # Applying/ Creating them sudo k3s kubectl apply -f <filename.yaml> # Deleting the deployment sudo k3s kubectl delete -f <filename.yaml> -f <filename.yaml> This can also be done in the gui dashboard application. Note: Local images can be used if imagePullPolicy is set to ifNotPresent instead of Always . In that case it will attempt to find a local image with the given image name. arm64 images must be cross compiled using docker buildx (make multi-arch or make cross-compile or similar in the relevant docker files) and always pulled from docker hub.","title":"Using files"},{"location":"details/kubernetes-deployment/#installation-instructions","text":"It is recommended that you use the ./run_k3s.sh script in the root of the repository. This script can be re-run at any time after install. If k3s is already installed and the relevant pods are running it will not do anything Install k3s using the install script, this will fetch k3s and run the kubernetes master node immediately in the background: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--docker\" sh - For the raspberry pi, ensure docker is installed, and then these instructions are the same on the raspberry pi (64 bit os). For testing purposes (inside testing dir), the containers have already been built for both amd64 and arm64 and uploaded onto hub.docker: mickeyli789/ros_demo . Also recommended you alias kubectl (kubernetes cli) in your bashrc alias kubectl='sudo k3s kubectl","title":"Installation instructions"},{"location":"details/kubernetes-deployment/#running-instructions","text":"","title":"Running instructions"},{"location":"details/kubernetes-deployment/#laptop","text":"It is recommended that you use the ./run_k3s.sh script in the root of the repository. This script will download the latest version of k3s run the master kubernetes server using docker (instead of containerd if you need access to local images) ' curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--docker\" sh - ' (will run in background as systemd - check systemctl status k3s ) This will open up a server with entrypoint on 0.0.0.0:6443 which corresponds to <host local ip address>:6443","title":"Laptop"},{"location":"details/kubernetes-deployment/#pi-drone-agent","text":"First ensure that the pi has been correctly set up with an airgapped installation of k3s, see here for installation instructions . Follow the Manually Deploy Images Method. The script below assumes that the the images file and the k3s executable are in the user home directory.","title":"Pi / Drone / Agent"},{"location":"details/kubernetes-deployment/#setup-script-via-ssh","text":"Identify the ip address of the pi, the root enabled (possibly password disabled) username. Then from this directory run ./start_k3s_agent.sh <remote username> <remote ip address> <node name> e.g. ./start_k8_agent.sh ubuntu 192.168.0.110 clover1 You can specify the k3s server address by setting the environment variable before calling: K3S_SERVER=https://192.168.0.63:6443 k3s_agent ubuntu 192.168.0.96 raspi1","title":"Setup script via ssh"},{"location":"details/kubernetes-deployment/#manual-old-setup-method","text":"First SSH onto the pi First ensure you run k3s-killall.sh to make sure there is no master server running as you only want k3s agent to run. The K3S_TOKEN is the contents of the file /var/lib/rancher/k3s/server/node-token K3S_TOKEN=<contents of the file /var/lib/rancher/k3s/server/node-token> #e.g. K3S_TOKEN=K103b62838822f40f3e41j51f10cb127236f2c3014c120ede19263da9f33fbfc859::server:2dcbb32a4cad16e20d714d88dbce4af8 K3S_SERVER=https://<Your main machine ip address>:6443 K3S_NODE_NAME=clover1 echo \"Killing all k3s services and instances first\" k3s-killall.sh echo \"Starting k3s agent only\" sudo k3s agent -t ${K3S_TOKEN} -s ${K3S_SERVER} --node-name ${K3S_NODE_NAME} The Pi should now be setup Consider running the above using screen or somehow in the background just in case your ssh connection is unstable or you want to close it.","title":"Manual, old setup method."},{"location":"details/kubernetes-deployment/#post-actions","text":"If you want to stop kubernetes completely, the internet install script comes with two options which are on the PATH and can be run in the terminal. 1. k3s-killall.sh will stop all k3s nodes and the systemd 2. k3s-uninstall.sh will delete everything k3s and remove the systemd","title":"Post actions"},{"location":"details/kubernetes-deployment/#dashboard","text":"See the k3s docs for info on how to run Are started automatically in the ./run_k3s.sh script.","title":"Dashboard"},{"location":"details/kubernetes/","text":"Kubernetes \u00b6 Kubernetes Kubernetes in Starling Kubernetes Deployment Types Local Testing using Kubernetes In Docker (KIND) Flight Arena K3S Practical Details Root/Non-root access Access Across Machines Configuration Files Kubernetes in Starling \u00b6 Containers allow the developer to abstract away from the specific hardware that the software will be running on. This is great for local development where a user can simply spin up the docker containers required, but what about in \u2019production\u2019? Where do we actually run these containers. The deployment of containers to a computation cluster - such as the cloud, a network of servers or a group of flying UAVs is not a trivial task. Such a task must include managing available resources, creating and managing the network connection within the cluster, service scalability, and services such as failure recovery (self healing) and logging. Thankfully this is handled by a class of applications known as container orchestration platforms. This project uses Kubernetes as its container orchestration platform which is an industry standard for container deployments within a server farm. In Kubernetes in particular, each physical piece of server hardware capable of running a container is referred to as a Node . The kubernetes control plane containers then take in user created configurations to decide where to run a particular container. This yaml file is known as a KubeConfig file and specifies: Defines which container images and which versions make up the application, and where they are located (in what registry) Provisions required storage and hardware capabilities using mounted volumes and labels along with the concept of taints and tolerations Defines and secures network configurations Kubernetes Deployment Types \u00b6 There are also a number of different types of deployment which are used throughout Starling, all of which are specified by Kubernetes Configuration. Pod - A single deployment of a set of containers to a particular node. Deployment - A single self-healing pod deployment which can be scaled. StatefulSet - A deployment with which the hostnames follow ordinal enumeration upon scaling. DaemonSet - A pod is deployed to every node which satisfies the taints and tolerations. Service - A special pod which manages networking and service access. There are a number of different versions of kubernetes. Due to the low-power, low-compute nature of UAV companion computers we have chosen to use k3s [^1], a lightweight variant of kubernetes often used in edge and iot computing applications. In addition, to streamline the transitory development process from container to kubernetes, we also make use of kind [^2] which allows the running of kubernetes inside a docker container. The kubectl [^3] command line tool is primarily used to interact with a kubernetes cluster. Local Testing using Kubernetes In Docker (KIND) \u00b6 In order to test kubernetes locally, we have decided to use Kubernetes In Docker (Kind). Kind is a useful tool which runs Kubernetes between multiple docker containers, just as if the containers themselves were kubernetes physical nodes. This allows us to test deployments locally on a system which resembles the actual flight arena. The Starling CLI takes care of many of the details of how kind is setup in such a way as to mirror the real vehicles. This includes ensuring that vehicles have access to virtual vehicle.config files and other measures. It also means that any deployed daemonsets can be tested to ensure that they deploy correctly to the real vehicles. See the CLI Documentation for further information. Flight Arena K3S Practical Details \u00b6 Root/Non-root access \u00b6 By default the k3s installation creates a configuration file k3s.yaml which stores access tokens, the IP address of the master servers and various other information. By default this is stored in /etc/rancher/k3s/k3s.yaml . Any time k3s is invoked, i.e. any time you call k3s kubectl ... , this configuration file is accessed and read. However because the configuration file is stored in the system directory /etc/ , by default any call to k3s would need sudo , i.e. sudo k3s kubectl ... . This can be avoided by moving the k3s.yaml file to your local user directory. The recommended location is ~/.kube/config/k3s.yaml , but it can be put anyway. The only requirement is that the KUBECONFIG environment variable should be set to the location of the configuration file. This can be achieved by adding the following line somewhere into your ~/.bashrc (or ~/.zshrc if using zsh): export KUBECONFIG=~/.kube/config/k3s.yaml Note: Do not forget to source your bashrc after making the change to see the change in your terminal source ~/.bashrc Once this is set, any call of k3s ... will not require sudo . Access Across Machines \u00b6 If you are in the BRL Flight Arena, or you are running a multiple machine cluster, you may want to be able to run and test containers on a different physical machine to the machine running the cluster master node. As detailed in the above section on non-root access , k3s generates the k3s.yaml configruation file. Importantly this file tells k3s what the ip of the master node is as shown in this example snippet of a k3s file: apiVersion: v1 clusters: - cluster: certificate-authority-data: <long data string> server: https://127.0.0.1:6443 ### <---- This line! name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: <another long data string> client-key-data: < secret client key!> To therefore access the master node (and the dashboard and kubectl etc) on another machine, you must. Copy the k3s.yaml file off of the server onto your own machine. (For the BRL, see below) In the k3s.yaml change the server ip from 127.0.0.1 to the ip address of the master machine. Ensure that either KUBECONFIG is set, or you have replaced the filee at ~/.kube/config/k3s.yaml with your own modified one (may need sudo) Once that is set up verify that you can get the dashboard access token and log in. k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token Also then verify that you can run any of the kubernetes deployments, and that they will be deployed to the master cluster (and not your own). Note: Even if the cluster is not running on your local machine, you will still need to install the k3s binaries. The command run by run_k3s.sh or ./scripts/start_k3s.sh both download and then automatically start k3s in the background of your machine. To stop it from starting, pass the --do-not-start option to either command. If you have already have k3s running, run k3s-killall.sh which will stop all k3s processes without uninstalling it entirely (so you still get access to the k3s command line tools). In the BRL, to get the k3s.yaml file, you should simply be able to scp (ssh copy) the file from the master machine ~/.kube/config/k3s.yaml to your local machine, and change the IP address. Configuration Files \u00b6","title":"Understanding Kubernetes"},{"location":"details/kubernetes/#kubernetes","text":"Kubernetes Kubernetes in Starling Kubernetes Deployment Types Local Testing using Kubernetes In Docker (KIND) Flight Arena K3S Practical Details Root/Non-root access Access Across Machines Configuration Files","title":"Kubernetes"},{"location":"details/kubernetes/#kubernetes-in-starling","text":"Containers allow the developer to abstract away from the specific hardware that the software will be running on. This is great for local development where a user can simply spin up the docker containers required, but what about in \u2019production\u2019? Where do we actually run these containers. The deployment of containers to a computation cluster - such as the cloud, a network of servers or a group of flying UAVs is not a trivial task. Such a task must include managing available resources, creating and managing the network connection within the cluster, service scalability, and services such as failure recovery (self healing) and logging. Thankfully this is handled by a class of applications known as container orchestration platforms. This project uses Kubernetes as its container orchestration platform which is an industry standard for container deployments within a server farm. In Kubernetes in particular, each physical piece of server hardware capable of running a container is referred to as a Node . The kubernetes control plane containers then take in user created configurations to decide where to run a particular container. This yaml file is known as a KubeConfig file and specifies: Defines which container images and which versions make up the application, and where they are located (in what registry) Provisions required storage and hardware capabilities using mounted volumes and labels along with the concept of taints and tolerations Defines and secures network configurations","title":"Kubernetes in Starling"},{"location":"details/kubernetes/#kubernetes-deployment-types","text":"There are also a number of different types of deployment which are used throughout Starling, all of which are specified by Kubernetes Configuration. Pod - A single deployment of a set of containers to a particular node. Deployment - A single self-healing pod deployment which can be scaled. StatefulSet - A deployment with which the hostnames follow ordinal enumeration upon scaling. DaemonSet - A pod is deployed to every node which satisfies the taints and tolerations. Service - A special pod which manages networking and service access. There are a number of different versions of kubernetes. Due to the low-power, low-compute nature of UAV companion computers we have chosen to use k3s [^1], a lightweight variant of kubernetes often used in edge and iot computing applications. In addition, to streamline the transitory development process from container to kubernetes, we also make use of kind [^2] which allows the running of kubernetes inside a docker container. The kubectl [^3] command line tool is primarily used to interact with a kubernetes cluster.","title":"Kubernetes Deployment Types"},{"location":"details/kubernetes/#local-testing-using-kubernetes-in-docker-kind","text":"In order to test kubernetes locally, we have decided to use Kubernetes In Docker (Kind). Kind is a useful tool which runs Kubernetes between multiple docker containers, just as if the containers themselves were kubernetes physical nodes. This allows us to test deployments locally on a system which resembles the actual flight arena. The Starling CLI takes care of many of the details of how kind is setup in such a way as to mirror the real vehicles. This includes ensuring that vehicles have access to virtual vehicle.config files and other measures. It also means that any deployed daemonsets can be tested to ensure that they deploy correctly to the real vehicles. See the CLI Documentation for further information.","title":"Local Testing using Kubernetes In Docker (KIND)"},{"location":"details/kubernetes/#flight-arena-k3s-practical-details","text":"","title":"Flight Arena K3S Practical Details"},{"location":"details/kubernetes/#rootnon-root-access","text":"By default the k3s installation creates a configuration file k3s.yaml which stores access tokens, the IP address of the master servers and various other information. By default this is stored in /etc/rancher/k3s/k3s.yaml . Any time k3s is invoked, i.e. any time you call k3s kubectl ... , this configuration file is accessed and read. However because the configuration file is stored in the system directory /etc/ , by default any call to k3s would need sudo , i.e. sudo k3s kubectl ... . This can be avoided by moving the k3s.yaml file to your local user directory. The recommended location is ~/.kube/config/k3s.yaml , but it can be put anyway. The only requirement is that the KUBECONFIG environment variable should be set to the location of the configuration file. This can be achieved by adding the following line somewhere into your ~/.bashrc (or ~/.zshrc if using zsh): export KUBECONFIG=~/.kube/config/k3s.yaml Note: Do not forget to source your bashrc after making the change to see the change in your terminal source ~/.bashrc Once this is set, any call of k3s ... will not require sudo .","title":"Root/Non-root access"},{"location":"details/kubernetes/#access-across-machines","text":"If you are in the BRL Flight Arena, or you are running a multiple machine cluster, you may want to be able to run and test containers on a different physical machine to the machine running the cluster master node. As detailed in the above section on non-root access , k3s generates the k3s.yaml configruation file. Importantly this file tells k3s what the ip of the master node is as shown in this example snippet of a k3s file: apiVersion: v1 clusters: - cluster: certificate-authority-data: <long data string> server: https://127.0.0.1:6443 ### <---- This line! name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: <another long data string> client-key-data: < secret client key!> To therefore access the master node (and the dashboard and kubectl etc) on another machine, you must. Copy the k3s.yaml file off of the server onto your own machine. (For the BRL, see below) In the k3s.yaml change the server ip from 127.0.0.1 to the ip address of the master machine. Ensure that either KUBECONFIG is set, or you have replaced the filee at ~/.kube/config/k3s.yaml with your own modified one (may need sudo) Once that is set up verify that you can get the dashboard access token and log in. k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token Also then verify that you can run any of the kubernetes deployments, and that they will be deployed to the master cluster (and not your own). Note: Even if the cluster is not running on your local machine, you will still need to install the k3s binaries. The command run by run_k3s.sh or ./scripts/start_k3s.sh both download and then automatically start k3s in the background of your machine. To stop it from starting, pass the --do-not-start option to either command. If you have already have k3s running, run k3s-killall.sh which will stop all k3s processes without uninstalling it entirely (so you still get access to the k3s command line tools). In the BRL, to get the k3s.yaml file, you should simply be able to scp (ssh copy) the file from the master machine ~/.kube/config/k3s.yaml to your local machine, and change the IP address.","title":"Access Across Machines"},{"location":"details/kubernetes/#configuration-files","text":"","title":"Configuration Files"},{"location":"docker-images/controller-base/","text":"starling-controller-base \u00b6 This image contains the minimal dependency requirements for a user to implement their own controller. In particular it contains a number of automated steps to aid in setup. This image acts as a base to build controllers /ros.env.d \u00b6 To ease the expansion of this image, a mechanism has been built to enable expansion by volume mount. As part of the entrypoint, the image will look for any folders in /ros.env.d that contain a setup.bash file. Any such file will be sourced as part of the entrypoint. This allows for arbitrary expansion of the entrypoint by adding additional volume mounts. Environment Variables \u00b6 Name Default Value Description USE_SIMULATED_TIME false A variable which can be used in ros2 launch scripts to toggle the use_sim_time parameter of a ros node.","title":"starling-controller-base"},{"location":"docker-images/controller-base/#starling-controller-base","text":"This image contains the minimal dependency requirements for a user to implement their own controller. In particular it contains a number of automated steps to aid in setup. This image acts as a base to build controllers","title":"starling-controller-base"},{"location":"docker-images/controller-base/#rosenvd","text":"To ease the expansion of this image, a mechanism has been built to enable expansion by volume mount. As part of the entrypoint, the image will look for any folders in /ros.env.d that contain a setup.bash file. Any such file will be sourced as part of the entrypoint. This allows for arbitrary expansion of the entrypoint by adding additional volume mounts.","title":"/ros.env.d"},{"location":"docker-images/controller-base/#environment-variables","text":"Name Default Value Description USE_SIMULATED_TIME false A variable which can be used in ros2 launch scripts to toggle the use_sim_time parameter of a ros node.","title":"Environment Variables"},{"location":"docker-images/mavros/","text":"starling-mavros \u00b6 This image acts to bridge MAVLink communications to ROS2 topics. At present this is achieved by a ROS1/2 bridge and the ROS1 version of MAVROS . The ROS2 port of MAVROS is currently under development and we anticipate switching over in the future, eliminating the need for the ROS1/2 bridge. Contents \u00b6 starling-mavros Contents Overview Configuration Options Configuring the Connection Configuring the Target System Configuring the MAVROS Configuration Deployments Running on a vehicle Running under Kubernetes StatefulSet Running isolated Implementation Details Build Process Entrypoint Names Advanced Topics Adding additional MAVROS plugins Overview \u00b6 The image is a wrapper around MAVROS, as such, most of the documentation for MAVROS is of use when working with this image. The image has both mavros and mavros-extras installed so all plugins should be available. The published topics are namespaced to allow for running multiple vehicles on the same ROS network. By default, this namespace will be /vehicle_N , where N is the MAVLink system ID of the vehicle. MAVROS topics are published within this namespace, e.g. /vehicle_1/mavros/state . The image has been setup to automatically configure itself in some scenarios: Running on a vehicle Running in Kubernetes There are also some additional general nodes running in the background. In particular: Emergency Stop listener on the /emergency_stop topic of type std_msgs/msg/Empty . It is hard-wired to forcefully disarm the vehicle (stop motors) for both PX4 and Ardupilot in simulation and reality. Configuration Options \u00b6 There are many configuration options available for this image to allow it to be used flexibly across different deployment scenarios. The most important of these are those relating to the vehicle and GCS connections, and MAVROS configuration. A full summary is given in the table below. Name Default Value Description MAVROS_FCU_CONN \"udp\" Protocol for autogenerated FCU URL MAVROS_FCU_IP \"127.0.0.1\" IP for autogenerated FCU_URL MAVROS_FCU_UDP_BASE \"14830\" Base port for autogenerated FCU_URL MAVROS_TGT_SYSTEM \"auto\" Target system ID, if set to a number, this will override the automatic behaviour PX4_INSTANCE_BASE 0 Base instance for autogenerated instance matching MAVROS_TGT_FIRMWARE \"px4\" Firmware profile used by MAVROS. Only other valid value currently is \"apm\" MAVROS_GCS_URL \"udp-pb://@:14550\" MAVROS URL for ground control station connection MAVROS_FCU_URL {unset} MAVROS URL for FCU connection. Set to override automatic behaviour VEHICLE_NAMESPACE {unset} Namespace for mavros topics. Set to override default value of vehicle_${TGT_SYSTEM} MAVROS_PLUGINLISTS_PATH \"/mavros_pluginlists_px4.yaml\" Path for MAVROS pluginlists file MAVROS_CONFIG_PATH \"/mavros_config_px4.yaml\" Path for initial MAVROS configuration file MAVROS_MOD_CONFIG_PATH \"/mavros_config_mod.yaml\" Path for modified MAVROS config to be written to BRIDGE_CONFIG_PATH \"/bridge_config.yaml\" Path for initial bridge config file BRIDGE_MOD_CONFIG_PATH \"/bridge_config_mod.yaml\" Path for modified bridge config to be written to BRIDGE_LAUNCH_DELAY 2.0 Time delay in seconds between starting ROS1 mavros and the ROS1-2 bridge. Increasing this value reduces the chance of a race condition causing mavros failure due to the bridge being unable to read ros1 parameters. See this issue for more details. Recommendation is to increase the value to 10 seconds if this occurs. Configuring the Connection \u00b6 MAVROS is told to connect to a MAVLink source with a URL. This URL can be configured in a number of ways. The most straightforward is to simply set the MAVROS_FCU_URL environment variable. This will override any other behaviour. docker run -e MAVROS_FCU_URL=serial:///dev/ttyUSB0:115200 uobflightlabstarling/starling-mavros When setting MAVROS_FCU_URL directly, note that a query string ( e.g. ?ids=n,240 ) will be added during launch. You need to ensure that your input for MAVROS_FCU_URL supports this syntax. Of particular note is the need for a trailing / in most formats, but not for the serial:// format. Also note that the plain file format does not support this. See the MAVROS docs for more information on URL formats. MAVROS_FCU_URL is can also be set automatically by the container depending on its environment. If there is a vehicle.config file mounted in the image, the value of VEHICLE_FCU_URL from that file will be used as the fcu_url . This is especially useful when deploying the container onto physical vehicles. The same warning about trailing slashes above goes for the value put in vehicle.config . If there is no vehicle.config file, the image will configure itself based on the values of MAVROS_FCU_CONN , MAVROS_FCU_IP and MAVROS_FCU_UDP_BASE . An additional parameter, INSTANCE is also used in the construction of the URL. This is generated based on the container hostname and is intended for use in Kubernetes deployments to distinguish multiple instances. PX4_INSTANCE_BASE can be used to offset the fcu_url will be constructed as below: $MAVROS_FCU_CONN://$MAVROS_FCU_IP:$((MAVROS_FCU_UDP_BASE + INSTANCE))@/ With all values at default, this ends up as: udp://127.0.0.1:14830@/ Configuring the Target System \u00b6 Another important configuration option is the target system ID. This controls the target system that MAVROS sends in some messages. As for the fcu_url , the value can be explicitly overridden, this time using the MAVROS_TGT_SYSTEM environment variable. Setting this will overrise all other values. If it is set to an invalid system ID, MAVROS will be set to use a target ID of 1 . If the environment variable is left at its default value of auto , a similar flow to the fcu_url occurs: if a vehicle.config file exists, the value of VEHICLE_MAVLINK_SYSID from that file will be used. Otherwise, the value is autogenerated based on the INSTANCE parameter derived from the container hostname. Note that the INSTANCE number is 0-indexed, while MAVLink system IDs start at 1 . Therefore, the system ID is set to one more than the INSTANCE . Configuring the MAVROS Configuration \u00b6 Two sets of config.yaml and pluginlists.yaml files are installed in the root directory to provide alternatives for PX4 and ArduPilot autopilots. These are named: /mavros_config_px4.yaml and /mavros_pluginlists_px4.yaml for the PX4 versions and /mavros_config_ap.yaml and /mavros_pluginlists_ap.yaml for the ArduPilot versions. The easiest way to choose between the two is to set the MAVROS_CONFIG_PATH and MAVROS_PLUGINLISTS_PATH environment variables. By default these point to the PX4 versions. To use the ArduPilot versions set both variables as below: docker run -e MAVROS_CONFIG_PATH=/mavros_config_ap.yaml -e MAVROS_PLUGINLISTS_PATH=/mavros_pluginlists_ap.yaml ... It is also possible to mount alternative configurations into the image and use the environment variables to configure MAVROS with them. TODO: Example of mounted configuration Deployments \u00b6 Running on a vehicle \u00b6 If the container is running on a drone, it expects to be able to find the /etc/starling/vehicle.config file. This file contains some information that MAVROS needs to be able to communicate with the flight controller. An example vehicle.config file is included below. Note that the extended form of the serial URL is required for MAVROS's target \"query string\" to work. VEHICLE_FCU_URL=serial:///dev/px4fmu:115200 VEHICLE_FIRMWARE=px4 VEHICLE_MAVLINK_SYSID=23 VEHICLE_NAME=clover23 Running under Kubernetes StatefulSet \u00b6 When running as part of a Kubernetes StatefulSet, each pod's hostname has a trailing ordinal, of the form: HOSTNAME-N . The setup script parses the ordinal of its containing pod from the hostname and uses this, in combination with the PX4_INSTANCE_BASE variable to determine the INSTANCE parameter. As outlined above, this is used to set up the ports and the system ID. By default, these are configured to match those generated by a PX4 SITL instance set up with the same ordinal. If the setup script fails to get the ordinal from the hostname, it will attempt to connect to a PX4 SITL with PX4_INSTANCE=0 . INSTANCE is computed as $((PX4_INSTANCE_BASE + ORDINAL)) MAVROS_TGT_SYSTEM will end up as $((INSTANCE + 1)) Running isolated \u00b6 Default values will be used, equivalent to the Kubernetes case with ORDINAL=0 Implementation Details \u00b6 Build Process \u00b6 To ensure support for custom message types, ros1_bridge needs to be built from source with the messages types to be bridged available in both ROS1 and ROS2. The Dockerfile first installs MAVROS for ROS1 and the MAVROS messages for ROS2. There are some additional steps to complete the installation of MAVROS under ROS1. Following this, the bridge is built. This takes a long time (~20min) and gives little output, but have faith! Entrypoint \u00b6 The entrypoint to the container is the ros_entrypoint.sh script. This script sources the environment setup files for both the ROS1 and ROS2 environments. It then sources the environment setup file for the bridge, and finally the mavros_setup.sh file. This file contains the logic that sets up MAVROS in a specific way based on both environment variables and files potentially mounted in the image. The first part of this is checking for the existance of the /stc/starling/vehicle.config file. If this file exists, it is assumed that the container is running on a real vehicle. In this case, the file is sourced and values for the MAVLink system ID, vehicle name, FCU URL, and firmware type are obtained. The next phase is determining the appropriate target system ID. This is configured by the Once the setup script has run, the default behaviour is to launch the mavros_bridge.launch.xml file. This behaviour should be usable in almost all cases. This is a ROS2 launch file. It instructs ros2 launch to run the ros1_bridge node and an instance of ROS1 's roslaunch . This in turn launches the ROS1 mavros.launch file, which contains instructions to run the MAVROS node. The ROS2 mavros_bridge.launch.xml script defines a set of arguments to enable configuration of the MAVROS node. These are ususally filled by the environment variables defined above. If the configurability provided here is insufficient, the image can be run with a different command. Names \u00b6 MAVROS has a set of frame names that are embedded in its config file. To enable the use of multiple vehicles, vehicle-specific frames need to be made unique. To that end a bunch of sedding happens at the end of mavros_setup.sh to modify the frame names before they are passed on to MAVROS. A similar scenario occurs with the topic names for the parameter bridge. This is hopefully a short-term problem while MAVROS finishes its port to ROS2. Advanced Topics \u00b6 Adding additional MAVROS plugins \u00b6 This should be possible by mounting a volume with your plugin into the container. Assuming ROS tools are able to find it, MAVROS should load the plugin (if directed by the pluginlists file).","title":"starling-mavros"},{"location":"docker-images/mavros/#starling-mavros","text":"This image acts to bridge MAVLink communications to ROS2 topics. At present this is achieved by a ROS1/2 bridge and the ROS1 version of MAVROS . The ROS2 port of MAVROS is currently under development and we anticipate switching over in the future, eliminating the need for the ROS1/2 bridge.","title":"starling-mavros"},{"location":"docker-images/mavros/#contents","text":"starling-mavros Contents Overview Configuration Options Configuring the Connection Configuring the Target System Configuring the MAVROS Configuration Deployments Running on a vehicle Running under Kubernetes StatefulSet Running isolated Implementation Details Build Process Entrypoint Names Advanced Topics Adding additional MAVROS plugins","title":"Contents"},{"location":"docker-images/mavros/#overview","text":"The image is a wrapper around MAVROS, as such, most of the documentation for MAVROS is of use when working with this image. The image has both mavros and mavros-extras installed so all plugins should be available. The published topics are namespaced to allow for running multiple vehicles on the same ROS network. By default, this namespace will be /vehicle_N , where N is the MAVLink system ID of the vehicle. MAVROS topics are published within this namespace, e.g. /vehicle_1/mavros/state . The image has been setup to automatically configure itself in some scenarios: Running on a vehicle Running in Kubernetes There are also some additional general nodes running in the background. In particular: Emergency Stop listener on the /emergency_stop topic of type std_msgs/msg/Empty . It is hard-wired to forcefully disarm the vehicle (stop motors) for both PX4 and Ardupilot in simulation and reality.","title":"Overview"},{"location":"docker-images/mavros/#configuration-options","text":"There are many configuration options available for this image to allow it to be used flexibly across different deployment scenarios. The most important of these are those relating to the vehicle and GCS connections, and MAVROS configuration. A full summary is given in the table below. Name Default Value Description MAVROS_FCU_CONN \"udp\" Protocol for autogenerated FCU URL MAVROS_FCU_IP \"127.0.0.1\" IP for autogenerated FCU_URL MAVROS_FCU_UDP_BASE \"14830\" Base port for autogenerated FCU_URL MAVROS_TGT_SYSTEM \"auto\" Target system ID, if set to a number, this will override the automatic behaviour PX4_INSTANCE_BASE 0 Base instance for autogenerated instance matching MAVROS_TGT_FIRMWARE \"px4\" Firmware profile used by MAVROS. Only other valid value currently is \"apm\" MAVROS_GCS_URL \"udp-pb://@:14550\" MAVROS URL for ground control station connection MAVROS_FCU_URL {unset} MAVROS URL for FCU connection. Set to override automatic behaviour VEHICLE_NAMESPACE {unset} Namespace for mavros topics. Set to override default value of vehicle_${TGT_SYSTEM} MAVROS_PLUGINLISTS_PATH \"/mavros_pluginlists_px4.yaml\" Path for MAVROS pluginlists file MAVROS_CONFIG_PATH \"/mavros_config_px4.yaml\" Path for initial MAVROS configuration file MAVROS_MOD_CONFIG_PATH \"/mavros_config_mod.yaml\" Path for modified MAVROS config to be written to BRIDGE_CONFIG_PATH \"/bridge_config.yaml\" Path for initial bridge config file BRIDGE_MOD_CONFIG_PATH \"/bridge_config_mod.yaml\" Path for modified bridge config to be written to BRIDGE_LAUNCH_DELAY 2.0 Time delay in seconds between starting ROS1 mavros and the ROS1-2 bridge. Increasing this value reduces the chance of a race condition causing mavros failure due to the bridge being unable to read ros1 parameters. See this issue for more details. Recommendation is to increase the value to 10 seconds if this occurs.","title":"Configuration Options"},{"location":"docker-images/mavros/#configuring-the-connection","text":"MAVROS is told to connect to a MAVLink source with a URL. This URL can be configured in a number of ways. The most straightforward is to simply set the MAVROS_FCU_URL environment variable. This will override any other behaviour. docker run -e MAVROS_FCU_URL=serial:///dev/ttyUSB0:115200 uobflightlabstarling/starling-mavros When setting MAVROS_FCU_URL directly, note that a query string ( e.g. ?ids=n,240 ) will be added during launch. You need to ensure that your input for MAVROS_FCU_URL supports this syntax. Of particular note is the need for a trailing / in most formats, but not for the serial:// format. Also note that the plain file format does not support this. See the MAVROS docs for more information on URL formats. MAVROS_FCU_URL is can also be set automatically by the container depending on its environment. If there is a vehicle.config file mounted in the image, the value of VEHICLE_FCU_URL from that file will be used as the fcu_url . This is especially useful when deploying the container onto physical vehicles. The same warning about trailing slashes above goes for the value put in vehicle.config . If there is no vehicle.config file, the image will configure itself based on the values of MAVROS_FCU_CONN , MAVROS_FCU_IP and MAVROS_FCU_UDP_BASE . An additional parameter, INSTANCE is also used in the construction of the URL. This is generated based on the container hostname and is intended for use in Kubernetes deployments to distinguish multiple instances. PX4_INSTANCE_BASE can be used to offset the fcu_url will be constructed as below: $MAVROS_FCU_CONN://$MAVROS_FCU_IP:$((MAVROS_FCU_UDP_BASE + INSTANCE))@/ With all values at default, this ends up as: udp://127.0.0.1:14830@/","title":"Configuring the Connection"},{"location":"docker-images/mavros/#configuring-the-target-system","text":"Another important configuration option is the target system ID. This controls the target system that MAVROS sends in some messages. As for the fcu_url , the value can be explicitly overridden, this time using the MAVROS_TGT_SYSTEM environment variable. Setting this will overrise all other values. If it is set to an invalid system ID, MAVROS will be set to use a target ID of 1 . If the environment variable is left at its default value of auto , a similar flow to the fcu_url occurs: if a vehicle.config file exists, the value of VEHICLE_MAVLINK_SYSID from that file will be used. Otherwise, the value is autogenerated based on the INSTANCE parameter derived from the container hostname. Note that the INSTANCE number is 0-indexed, while MAVLink system IDs start at 1 . Therefore, the system ID is set to one more than the INSTANCE .","title":"Configuring the Target System"},{"location":"docker-images/mavros/#configuring-the-mavros-configuration","text":"Two sets of config.yaml and pluginlists.yaml files are installed in the root directory to provide alternatives for PX4 and ArduPilot autopilots. These are named: /mavros_config_px4.yaml and /mavros_pluginlists_px4.yaml for the PX4 versions and /mavros_config_ap.yaml and /mavros_pluginlists_ap.yaml for the ArduPilot versions. The easiest way to choose between the two is to set the MAVROS_CONFIG_PATH and MAVROS_PLUGINLISTS_PATH environment variables. By default these point to the PX4 versions. To use the ArduPilot versions set both variables as below: docker run -e MAVROS_CONFIG_PATH=/mavros_config_ap.yaml -e MAVROS_PLUGINLISTS_PATH=/mavros_pluginlists_ap.yaml ... It is also possible to mount alternative configurations into the image and use the environment variables to configure MAVROS with them. TODO: Example of mounted configuration","title":"Configuring the MAVROS Configuration"},{"location":"docker-images/mavros/#deployments","text":"","title":"Deployments"},{"location":"docker-images/mavros/#running-on-a-vehicle","text":"If the container is running on a drone, it expects to be able to find the /etc/starling/vehicle.config file. This file contains some information that MAVROS needs to be able to communicate with the flight controller. An example vehicle.config file is included below. Note that the extended form of the serial URL is required for MAVROS's target \"query string\" to work. VEHICLE_FCU_URL=serial:///dev/px4fmu:115200 VEHICLE_FIRMWARE=px4 VEHICLE_MAVLINK_SYSID=23 VEHICLE_NAME=clover23","title":"Running on a vehicle"},{"location":"docker-images/mavros/#running-under-kubernetes-statefulset","text":"When running as part of a Kubernetes StatefulSet, each pod's hostname has a trailing ordinal, of the form: HOSTNAME-N . The setup script parses the ordinal of its containing pod from the hostname and uses this, in combination with the PX4_INSTANCE_BASE variable to determine the INSTANCE parameter. As outlined above, this is used to set up the ports and the system ID. By default, these are configured to match those generated by a PX4 SITL instance set up with the same ordinal. If the setup script fails to get the ordinal from the hostname, it will attempt to connect to a PX4 SITL with PX4_INSTANCE=0 . INSTANCE is computed as $((PX4_INSTANCE_BASE + ORDINAL)) MAVROS_TGT_SYSTEM will end up as $((INSTANCE + 1))","title":"Running under Kubernetes StatefulSet"},{"location":"docker-images/mavros/#running-isolated","text":"Default values will be used, equivalent to the Kubernetes case with ORDINAL=0","title":"Running isolated"},{"location":"docker-images/mavros/#implementation-details","text":"","title":"Implementation Details"},{"location":"docker-images/mavros/#build-process","text":"To ensure support for custom message types, ros1_bridge needs to be built from source with the messages types to be bridged available in both ROS1 and ROS2. The Dockerfile first installs MAVROS for ROS1 and the MAVROS messages for ROS2. There are some additional steps to complete the installation of MAVROS under ROS1. Following this, the bridge is built. This takes a long time (~20min) and gives little output, but have faith!","title":"Build Process"},{"location":"docker-images/mavros/#entrypoint","text":"The entrypoint to the container is the ros_entrypoint.sh script. This script sources the environment setup files for both the ROS1 and ROS2 environments. It then sources the environment setup file for the bridge, and finally the mavros_setup.sh file. This file contains the logic that sets up MAVROS in a specific way based on both environment variables and files potentially mounted in the image. The first part of this is checking for the existance of the /stc/starling/vehicle.config file. If this file exists, it is assumed that the container is running on a real vehicle. In this case, the file is sourced and values for the MAVLink system ID, vehicle name, FCU URL, and firmware type are obtained. The next phase is determining the appropriate target system ID. This is configured by the Once the setup script has run, the default behaviour is to launch the mavros_bridge.launch.xml file. This behaviour should be usable in almost all cases. This is a ROS2 launch file. It instructs ros2 launch to run the ros1_bridge node and an instance of ROS1 's roslaunch . This in turn launches the ROS1 mavros.launch file, which contains instructions to run the MAVROS node. The ROS2 mavros_bridge.launch.xml script defines a set of arguments to enable configuration of the MAVROS node. These are ususally filled by the environment variables defined above. If the configurability provided here is insufficient, the image can be run with a different command.","title":"Entrypoint"},{"location":"docker-images/mavros/#names","text":"MAVROS has a set of frame names that are embedded in its config file. To enable the use of multiple vehicles, vehicle-specific frames need to be made unique. To that end a bunch of sedding happens at the end of mavros_setup.sh to modify the frame names before they are passed on to MAVROS. A similar scenario occurs with the topic names for the parameter bridge. This is hopefully a short-term problem while MAVROS finishes its port to ROS2.","title":"Names"},{"location":"docker-images/mavros/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"docker-images/mavros/#adding-additional-mavros-plugins","text":"This should be possible by mounting a volume with your plugin into the container. Assuming ROS tools are able to find it, MAVROS should load the plugin (if directed by the pluginlists file).","title":"Adding additional MAVROS plugins"},{"location":"docker-images/sim-ardupilot-gazebo/","text":"starling-sim-ardupilot-gazebo \u00b6 Based on starling-sim-base-core This image contains the base Gazebo installation and adds the ArduPilot specific plugin. Environment Variables \u00b6 Name Default Value Description AP_SYSID 1 MAVLink system ID to be used by the SITL. Can also be set to \"ordinal\" or \"ip\" AP_SYSID_BASE 1 Base system ID for ordinal-based generation AP_SITL_ADDRESS 127.0.0.1 IP address for Gazebo plugin to use to talk to ArduPilot instance AP_SITL_HOST {null} Hostname for Gazebo plugin to use to talk to ArduPilot instance. Set to override IP address. VEHICLE_NAMESPACE vehicle_${AP_SYSID} ROS2 Namespace in which to place vehicle sensor topics (e.g. gimbal_cmd) AP_SYSID \u00b6 Sets the MAVLink system ID to be used by the SITL. This must either be a value between 1 and 255 inclusive or set to \"ordinal\" or \"ip\" . Other values will cause the container to exit. When set to \"ordinal , the entrypoint script will attempt to retrieve a 0-indexed ordinal from the end of the container's hostname. The hostnames should be of the form ${HOSTNAME}-${ORDINAL} , e.g. sitl-0 , sitl-1 , sitl-3 . This behaviour supports deployment of this image as part of a Kubernetes StatefulSet. When set to \"ip\" , the entrypoint script retrieves the last octet of the container's IP address and uses that as the system ID. e.g. 172.18.0.4 will result in a system ID of 4 . At present, the system ID is set by appending it to the parameter file. If a custom parameter file is supplied, it should not contain the SYSID_THISMAV parameter. AP_SYSID_BASE \u00b6 This variable only affects the container when using the ordinal-based generation outlined above. The value set here will be added to the ordinal from the hostname to derive the MAVLink system ID. If the resultant value is not a valid system ID, i.e. it is not between 1 and 255 inclusive, the container will exit.","title":"starling-sim-ardupilot-gazebo"},{"location":"docker-images/sim-ardupilot-gazebo/#starling-sim-ardupilot-gazebo","text":"Based on starling-sim-base-core This image contains the base Gazebo installation and adds the ArduPilot specific plugin.","title":"starling-sim-ardupilot-gazebo"},{"location":"docker-images/sim-ardupilot-gazebo/#environment-variables","text":"Name Default Value Description AP_SYSID 1 MAVLink system ID to be used by the SITL. Can also be set to \"ordinal\" or \"ip\" AP_SYSID_BASE 1 Base system ID for ordinal-based generation AP_SITL_ADDRESS 127.0.0.1 IP address for Gazebo plugin to use to talk to ArduPilot instance AP_SITL_HOST {null} Hostname for Gazebo plugin to use to talk to ArduPilot instance. Set to override IP address. VEHICLE_NAMESPACE vehicle_${AP_SYSID} ROS2 Namespace in which to place vehicle sensor topics (e.g. gimbal_cmd)","title":"Environment Variables"},{"location":"docker-images/sim-ardupilot-gazebo/#ap_sysid","text":"Sets the MAVLink system ID to be used by the SITL. This must either be a value between 1 and 255 inclusive or set to \"ordinal\" or \"ip\" . Other values will cause the container to exit. When set to \"ordinal , the entrypoint script will attempt to retrieve a 0-indexed ordinal from the end of the container's hostname. The hostnames should be of the form ${HOSTNAME}-${ORDINAL} , e.g. sitl-0 , sitl-1 , sitl-3 . This behaviour supports deployment of this image as part of a Kubernetes StatefulSet. When set to \"ip\" , the entrypoint script retrieves the last octet of the container's IP address and uses that as the system ID. e.g. 172.18.0.4 will result in a system ID of 4 . At present, the system ID is set by appending it to the parameter file. If a custom parameter file is supplied, it should not contain the SYSID_THISMAV parameter.","title":"AP_SYSID"},{"location":"docker-images/sim-ardupilot-gazebo/#ap_sysid_base","text":"This variable only affects the container when using the ordinal-based generation outlined above. The value set here will be added to the ordinal from the hostname to derive the MAVLink system ID. If the resultant value is not a valid system ID, i.e. it is not between 1 and 255 inclusive, the container will exit.","title":"AP_SYSID_BASE"},{"location":"docker-images/sim-ardupilot-vehicle/","text":"starling-sim-ardupilot-copter/plane \u00b6 There are two images currently built: starling-sim-arducopter and starling-sim-arduplane . These are built against the relevant stable branch: ArduCopter-stable and ArduPlane-stable respectively. The SITL starts up with a TCP server listening on port 5760. It will not begin simulating the vehicle until a connection is made to this port. Currently, the SITL's built-in simulator is used for both copter and plane images. Environment Variables \u00b6 Name Default Value Description AP_SYSID 1 MAVLink system ID to be used by the SITL. Can also be set to \"ordinal\" or \"ip\" AP_SYSID_BASE 1 Base system ID for ordinal-based generation AP_VEHICLE {from build arg} Which vehicle is being used. Either \"copter\" or \"plane\" AP_MODEL {null} Alternate model argument. Set to override default model AP_PARAM_PATH {null} Alternate path to parameter file. Set to override default path AP_PARAM_FILE {null} Alternate parameter file name. Set to override default parameter file AP_HOME 51.4235413,-2.6708488,50,250 Start location for SITL AP_OFFSET_X 0 Start location x offset for SITL AP_OFFSET_Y 0 Start location y offset for SITL AP_DISTRIBUTE {null} If set, automatically generate AP_OFFSET_X and AP_OFFSET_Y AP_USE_GAZEBO {null} If set, use a Gazebo model as physics backend AP_SIM_ADDRESS 127.0.0.1 IP address to use for talking to Gazebo instance AP_SIM_HOST {null} Hostname to use for talking to Gazebo instance. Will override AP_SIM_ADDRESS AP_SYSID \u00b6 Sets the MAVLink system ID to be used by the SITL. This must either be a value between 1 and 255 inclusive or set to \"ordinal\" or \"ip\" . Other values will cause the container to exit. When set to \"ordinal , the entrypoint script will attempt to retrieve a 0-indexed ordinal from the end of the container's hostname. The hostnames should be of the form ${HOSTNAME}-${ORDINAL} , e.g. sitl-0 , sitl-1 , sitl-3 . This behaviour supports deployment of this image as part of a Kubernetes StatefulSet. When set to \"ip\" , the entrypoint script retrieves the last octet of the container's IP address and uses that as the system ID. e.g. 172.18.0.4 will result in a system ID of 4 . At present, the system ID is set by appending it to the parameter file. If a custom parameter file is supplied, it should not contain the SYSID_THISMAV parameter. AP_SYSID_BASE \u00b6 This variable only affects the container when using the ordinal-based generation outlined above. The value set here will be added to the ordinal from the hostname to derive the MAVLink system ID. If the resultant value is not a valid system ID, i.e. it is not between 1 and 255 inclusive, the container will exit. AP_VEHICLE \u00b6 By default this will be either \"copter\" or \"plane\" and is derived from the image's arguments at build time. This argument is used to set which executable is used and should not be overriden. It is also used to set defaults for the MODEL and PARAM_FILE variables unless they are overridden. AP_MODEL \u00b6 Used as the value of the --model argument to the ArduPilot SITL binary. For copter images, this is set to quad . For plane images, this is set to plane . AP_PARAM_PATH \u00b6 The path to the folder containing the parameter file. If left blank, the in-source folder will be used: Tools/autotest/default_params . This can be changed to allow for supplying a custom set of parameter files through a volume mount. The final folder on the path should be default_params . AP_PARAM_FILES \u00b6 Comma separated list of alternate set of parameter file paths to use. If set this overrides the default parameter files used. AP_PARAM_PATH is ignored if this is set so the absolute paths to files are needed. An additional file will be appended to set the system ID. AP_HOME , AP_OFFSET_X & AP_OFFSET_Y \u00b6 AP_HOME sets the start location for the SITL. Format is {Latitude},{Longitude},{Altitude},{Heading} AP_OFFSET_X and AP_OFFSET_Y are body frame offsets in metres from AP_HOME for this vehicle. This allows a grid of vehicles to be created using a single AP_HOME while varying the offsets. AP_DISTRIBUTE \u00b6 When set, AP_DISTRIBUTE directs the entrypoint script to automatically generate offsets. It does this based on the final value of the AP_SYSID environment variable, i.e. after it has been resolved into a number. This number then dictates the position of the vehicle within a 16 by 16 grid, with the position of AP_HOME at the (0,0) position. The grid lines are laid out at 1m separations. As a SYSID of 0 is invalid, no vehicle will be placed directly on the AP_HOME position. AP_USE_GAZEBO \u00b6 When set, AP_USE_GAZEBO will cause the SITL to connect to an instance of the ArduPilot plugin running as part of a Gazebo model. At present, this will use the default IP and port pair: 127.0.0.1:9002/9003 AP_SIM_ADDRESS \u00b6 Use this to adjust the IP address that the SITL uses to connect to the simulator backend. At present, this is only relevant when AP_USE_GAZEBO is set. AP_SIM_HOST \u00b6 When set, the container will resolve the hostname stored in this variable using getent hosts . The value from the lookup will be used in preference to any address set in AP_SIM_ADDRESS . If the hostname lookup fails, the container will exit early. Dockerfile Build Arguments \u00b6 VEHICLE \u00b6 Controls which target is built. Passed as ./waf ${VEHICLE} to build the SITL. Also set as the value of the AP_VEHICLE environment variable. By default this is set to copter BRANCH \u00b6 Set the branch of ArduPilot to checkout prior to build. By default this is set to ArduCopter-stable","title":"starling-sim-ardupilot-copter/plane"},{"location":"docker-images/sim-ardupilot-vehicle/#starling-sim-ardupilot-copterplane","text":"There are two images currently built: starling-sim-arducopter and starling-sim-arduplane . These are built against the relevant stable branch: ArduCopter-stable and ArduPlane-stable respectively. The SITL starts up with a TCP server listening on port 5760. It will not begin simulating the vehicle until a connection is made to this port. Currently, the SITL's built-in simulator is used for both copter and plane images.","title":"starling-sim-ardupilot-copter/plane"},{"location":"docker-images/sim-ardupilot-vehicle/#environment-variables","text":"Name Default Value Description AP_SYSID 1 MAVLink system ID to be used by the SITL. Can also be set to \"ordinal\" or \"ip\" AP_SYSID_BASE 1 Base system ID for ordinal-based generation AP_VEHICLE {from build arg} Which vehicle is being used. Either \"copter\" or \"plane\" AP_MODEL {null} Alternate model argument. Set to override default model AP_PARAM_PATH {null} Alternate path to parameter file. Set to override default path AP_PARAM_FILE {null} Alternate parameter file name. Set to override default parameter file AP_HOME 51.4235413,-2.6708488,50,250 Start location for SITL AP_OFFSET_X 0 Start location x offset for SITL AP_OFFSET_Y 0 Start location y offset for SITL AP_DISTRIBUTE {null} If set, automatically generate AP_OFFSET_X and AP_OFFSET_Y AP_USE_GAZEBO {null} If set, use a Gazebo model as physics backend AP_SIM_ADDRESS 127.0.0.1 IP address to use for talking to Gazebo instance AP_SIM_HOST {null} Hostname to use for talking to Gazebo instance. Will override AP_SIM_ADDRESS","title":"Environment Variables"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_sysid","text":"Sets the MAVLink system ID to be used by the SITL. This must either be a value between 1 and 255 inclusive or set to \"ordinal\" or \"ip\" . Other values will cause the container to exit. When set to \"ordinal , the entrypoint script will attempt to retrieve a 0-indexed ordinal from the end of the container's hostname. The hostnames should be of the form ${HOSTNAME}-${ORDINAL} , e.g. sitl-0 , sitl-1 , sitl-3 . This behaviour supports deployment of this image as part of a Kubernetes StatefulSet. When set to \"ip\" , the entrypoint script retrieves the last octet of the container's IP address and uses that as the system ID. e.g. 172.18.0.4 will result in a system ID of 4 . At present, the system ID is set by appending it to the parameter file. If a custom parameter file is supplied, it should not contain the SYSID_THISMAV parameter.","title":"AP_SYSID"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_sysid_base","text":"This variable only affects the container when using the ordinal-based generation outlined above. The value set here will be added to the ordinal from the hostname to derive the MAVLink system ID. If the resultant value is not a valid system ID, i.e. it is not between 1 and 255 inclusive, the container will exit.","title":"AP_SYSID_BASE"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_vehicle","text":"By default this will be either \"copter\" or \"plane\" and is derived from the image's arguments at build time. This argument is used to set which executable is used and should not be overriden. It is also used to set defaults for the MODEL and PARAM_FILE variables unless they are overridden.","title":"AP_VEHICLE"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_model","text":"Used as the value of the --model argument to the ArduPilot SITL binary. For copter images, this is set to quad . For plane images, this is set to plane .","title":"AP_MODEL"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_param_path","text":"The path to the folder containing the parameter file. If left blank, the in-source folder will be used: Tools/autotest/default_params . This can be changed to allow for supplying a custom set of parameter files through a volume mount. The final folder on the path should be default_params .","title":"AP_PARAM_PATH"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_param_files","text":"Comma separated list of alternate set of parameter file paths to use. If set this overrides the default parameter files used. AP_PARAM_PATH is ignored if this is set so the absolute paths to files are needed. An additional file will be appended to set the system ID.","title":"AP_PARAM_FILES"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_home-ap_offset_x-ap_offset_y","text":"AP_HOME sets the start location for the SITL. Format is {Latitude},{Longitude},{Altitude},{Heading} AP_OFFSET_X and AP_OFFSET_Y are body frame offsets in metres from AP_HOME for this vehicle. This allows a grid of vehicles to be created using a single AP_HOME while varying the offsets.","title":"AP_HOME, AP_OFFSET_X &amp; AP_OFFSET_Y"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_distribute","text":"When set, AP_DISTRIBUTE directs the entrypoint script to automatically generate offsets. It does this based on the final value of the AP_SYSID environment variable, i.e. after it has been resolved into a number. This number then dictates the position of the vehicle within a 16 by 16 grid, with the position of AP_HOME at the (0,0) position. The grid lines are laid out at 1m separations. As a SYSID of 0 is invalid, no vehicle will be placed directly on the AP_HOME position.","title":"AP_DISTRIBUTE"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_use_gazebo","text":"When set, AP_USE_GAZEBO will cause the SITL to connect to an instance of the ArduPilot plugin running as part of a Gazebo model. At present, this will use the default IP and port pair: 127.0.0.1:9002/9003","title":"AP_USE_GAZEBO"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_sim_address","text":"Use this to adjust the IP address that the SITL uses to connect to the simulator backend. At present, this is only relevant when AP_USE_GAZEBO is set.","title":"AP_SIM_ADDRESS"},{"location":"docker-images/sim-ardupilot-vehicle/#ap_sim_host","text":"When set, the container will resolve the hostname stored in this variable using getent hosts . The value from the lookup will be used in preference to any address set in AP_SIM_ADDRESS . If the hostname lookup fails, the container will exit early.","title":"AP_SIM_HOST"},{"location":"docker-images/sim-ardupilot-vehicle/#dockerfile-build-arguments","text":"","title":"Dockerfile Build Arguments"},{"location":"docker-images/sim-ardupilot-vehicle/#vehicle","text":"Controls which target is built. Passed as ./waf ${VEHICLE} to build the SITL. Also set as the value of the AP_VEHICLE environment variable. By default this is set to copter","title":"VEHICLE"},{"location":"docker-images/sim-ardupilot-vehicle/#branch","text":"Set the branch of ArduPilot to checkout prior to build. By default this is set to ArduCopter-stable","title":"BRANCH"},{"location":"docker-images/sim-base-core/","text":"starling-sim-base-core \u00b6 This image contains the Gazebo and gzweb installations that other Gazebo-based simulation images are built upon. It also includes gazebo-ros to enable the link between ROS and Gazebo. This image acts as a base to build other simulation environments on. /ros.env.d \u00b6 To ease the expansion of this image, a mechanism has been built to enable expansion by volume mount. As part of the entrypoint, the image will look for any folders in /ros.env.d that contain a setup.bash file. Any such file will be sourced as part of the entrypoint. This allows for arbitrary expansion of the entrypoint by adding additional volume mounts. Environment Variables \u00b6 Name Default Value Description ENABLE_VIRTUAL_FRAMEBUFFER true Enables the setup_display.sh script and starts a virtual X server. Use for simulated vehicles which have cameras on them.","title":"starling-sim-base-core"},{"location":"docker-images/sim-base-core/#starling-sim-base-core","text":"This image contains the Gazebo and gzweb installations that other Gazebo-based simulation images are built upon. It also includes gazebo-ros to enable the link between ROS and Gazebo. This image acts as a base to build other simulation environments on.","title":"starling-sim-base-core"},{"location":"docker-images/sim-base-core/#rosenvd","text":"To ease the expansion of this image, a mechanism has been built to enable expansion by volume mount. As part of the entrypoint, the image will look for any folders in /ros.env.d that contain a setup.bash file. Any such file will be sourced as part of the entrypoint. This allows for arbitrary expansion of the entrypoint by adding additional volume mounts.","title":"/ros.env.d"},{"location":"docker-images/sim-base-core/#environment-variables","text":"Name Default Value Description ENABLE_VIRTUAL_FRAMEBUFFER true Enables the setup_display.sh script and starts a virtual X server. Use for simulated vehicles which have cameras on them.","title":"Environment Variables"},{"location":"docker-images/sim-base-px4/","text":"starling-sim-base-px4 \u00b6 Based on starling-sim-base-core This image contains the base Gazebo installation and adds the PX4 specific plugins. Environment Variables \u00b6 Name Default Value Description PX4_SIM_HOST The host address of the simulation, if set, it will look up the ip address and assign to PX4_SIM_IP PX4_OFFBOARD_HOST The host adress of the offboard (mavros), if set, it will look up the ip address and assign to PX4_OFFBOARD_IP PX4_INSTANCE 0 The instance of the SITL (set to PX4 SYS_ID set to PX4_INSTACE+1 ), can be set to ordinal to automatically derive from last number of hostname (e.g. hostname-4), or set to 0 < instace < 254. PX4_HOME_LAT 51.501582 Home Latitude PX4_HOME_LON -2.551791 Home Longitude PX4_HOME_ALT 0 Home Altitude PX4_SIM_MODEL iris The PX4 simulation model which matches the available PX4 model library (see KEEP_PX4_VEHICLES ) PX4_SIM_INIT_LOC_X 0 Virtual X location for vehicle to spawn PX4_SIM_INIT_LOC_Y 0 Virtual Y location for vehicle to spawn PX4_SIM_INIT_LOC_Z 0 Virtual Z location for vehicle to spawn PX4_SIM_FORCE_USE_SET_POSITION false If multiple vehicles are spawning, by default they will spawn in a spiral. This forces the use of Init Locations PX4_SYSID_SITL_BASE 200 The base value of the minimum STIL instances actual instance is PX4_SYSID_SITL_BASE + PX4_INSTACE (not implemented yet) ENABLE_EXTERNAL_VISION If this variable exists, sets set px4 params EKF2_HGT_MODE to 3 and EKF2_AID_MASK to 24 (EV_POS+EV_YAW) for use with a simulated vicon system. KEEP_PX4_VEHICLES \"\" [ BUILD ARG ] A string of the format ! -path ./<vehicle_1> ! -path ./<vehicle_2> ... which specifies a list of extra vehicle PX4 models to keep on top of sun , gps , iris and r1_rover . REMOVE_GAZEBO_LIBRARIES \"libgazebo_video_stream_widget.so libgazebo_airspeed_plugin.so libgazebo_drop_plugin.so libgazebo_user_camera_plugin.so libgazebo_camera_manager_plugin.so libgazebo_gimbal_controller_plugin.so libgazebo_opticalflow_mockup_plugin.so libgazebo_opticalflow_plugin.so libgazebo_airship_dynamics_plugin.so libgazebo_catapult_plugin.so libgazebo_irlock_plugin.so libgazebo_parachute_plugin.so libgazebo_sonar_plugin.so libgazebo_usv_dynamics_plugin.so libgazebo_uuv_plugin.so libgazebo_wind_plugin.so\" [ BUILD ARG ] A space seperated list of libraries to be removed from the image. See below for details. REMOVE_GAZEBO_LIBRARIES \u00b6 Default list of available plugins which are built by the PX4_SITL: libgazebo_airship_dynamics_plugin.so X libgazebo_airspeed_plugin.so X libgazebo_barometer_plugin.so libgazebo_camera_manager_plugin.so X libgazebo_catapult_plugin.so X libgazebo_controller_interface.so libgazebo_drop_plugin.so X libgazebo_gimbal_controller_plugin.so X libgazebo_gps_plugin.so libgazebo_groundtruth_plugin.so libgazebo_gst_camera_plugin.so libgazebo_imu_plugin.so libgazebo_irlock_plugin.so X libgazebo_lidar_plugin.so libgazebo_magnetometer_plugin.so libgazebo_mavlink_interface.so libgazebo_motor_model.so libgazebo_multirotor_base_plugin.so libgazebo_opticalflow_mockup_plugin.so X libgazebo_opticalflow_plugin.so X libgazebo_parachute_plugin.so X libgazebo_sonar_plugin.so X libgazebo_user_camera_plugin.so X libgazebo_usv_dynamics_plugin.so X libgazebo_uuv_plugin.so X libgazebo_video_stream_widget.so X libgazebo_vision_plugin.so A space seperated list of these files can be given at build time to remove all of the plugins you dont need in order to save space. Each library is around 7 to 9Mb.","title":"starling-sim-base-px4"},{"location":"docker-images/sim-base-px4/#starling-sim-base-px4","text":"Based on starling-sim-base-core This image contains the base Gazebo installation and adds the PX4 specific plugins.","title":"starling-sim-base-px4"},{"location":"docker-images/sim-base-px4/#environment-variables","text":"Name Default Value Description PX4_SIM_HOST The host address of the simulation, if set, it will look up the ip address and assign to PX4_SIM_IP PX4_OFFBOARD_HOST The host adress of the offboard (mavros), if set, it will look up the ip address and assign to PX4_OFFBOARD_IP PX4_INSTANCE 0 The instance of the SITL (set to PX4 SYS_ID set to PX4_INSTACE+1 ), can be set to ordinal to automatically derive from last number of hostname (e.g. hostname-4), or set to 0 < instace < 254. PX4_HOME_LAT 51.501582 Home Latitude PX4_HOME_LON -2.551791 Home Longitude PX4_HOME_ALT 0 Home Altitude PX4_SIM_MODEL iris The PX4 simulation model which matches the available PX4 model library (see KEEP_PX4_VEHICLES ) PX4_SIM_INIT_LOC_X 0 Virtual X location for vehicle to spawn PX4_SIM_INIT_LOC_Y 0 Virtual Y location for vehicle to spawn PX4_SIM_INIT_LOC_Z 0 Virtual Z location for vehicle to spawn PX4_SIM_FORCE_USE_SET_POSITION false If multiple vehicles are spawning, by default they will spawn in a spiral. This forces the use of Init Locations PX4_SYSID_SITL_BASE 200 The base value of the minimum STIL instances actual instance is PX4_SYSID_SITL_BASE + PX4_INSTACE (not implemented yet) ENABLE_EXTERNAL_VISION If this variable exists, sets set px4 params EKF2_HGT_MODE to 3 and EKF2_AID_MASK to 24 (EV_POS+EV_YAW) for use with a simulated vicon system. KEEP_PX4_VEHICLES \"\" [ BUILD ARG ] A string of the format ! -path ./<vehicle_1> ! -path ./<vehicle_2> ... which specifies a list of extra vehicle PX4 models to keep on top of sun , gps , iris and r1_rover . REMOVE_GAZEBO_LIBRARIES \"libgazebo_video_stream_widget.so libgazebo_airspeed_plugin.so libgazebo_drop_plugin.so libgazebo_user_camera_plugin.so libgazebo_camera_manager_plugin.so libgazebo_gimbal_controller_plugin.so libgazebo_opticalflow_mockup_plugin.so libgazebo_opticalflow_plugin.so libgazebo_airship_dynamics_plugin.so libgazebo_catapult_plugin.so libgazebo_irlock_plugin.so libgazebo_parachute_plugin.so libgazebo_sonar_plugin.so libgazebo_usv_dynamics_plugin.so libgazebo_uuv_plugin.so libgazebo_wind_plugin.so\" [ BUILD ARG ] A space seperated list of libraries to be removed from the image. See below for details.","title":"Environment Variables"},{"location":"docker-images/sim-base-px4/#remove_gazebo_libraries","text":"Default list of available plugins which are built by the PX4_SITL: libgazebo_airship_dynamics_plugin.so X libgazebo_airspeed_plugin.so X libgazebo_barometer_plugin.so libgazebo_camera_manager_plugin.so X libgazebo_catapult_plugin.so X libgazebo_controller_interface.so libgazebo_drop_plugin.so X libgazebo_gimbal_controller_plugin.so X libgazebo_gps_plugin.so libgazebo_groundtruth_plugin.so libgazebo_gst_camera_plugin.so libgazebo_imu_plugin.so libgazebo_irlock_plugin.so X libgazebo_lidar_plugin.so libgazebo_magnetometer_plugin.so libgazebo_mavlink_interface.so libgazebo_motor_model.so libgazebo_multirotor_base_plugin.so libgazebo_opticalflow_mockup_plugin.so X libgazebo_opticalflow_plugin.so X libgazebo_parachute_plugin.so X libgazebo_sonar_plugin.so X libgazebo_user_camera_plugin.so X libgazebo_usv_dynamics_plugin.so X libgazebo_uuv_plugin.so X libgazebo_video_stream_widget.so X libgazebo_vision_plugin.so A space seperated list of these files can be given at build time to remove all of the plugins you dont need in order to save space. Each library is around 7 to 9Mb.","title":"REMOVE_GAZEBO_LIBRARIES"},{"location":"docker-images/sim-iris-ap/","text":"starling-sim-iris-ap \u00b6 Based on staring-sim-ardupilot-gazebo See for additional environment variables. Overview \u00b6 This image adds the launch files required to launch an Iris model with the ArduPilot plugin using Gazebo. The iris ardupilot model depends on the iris with standoffs and a gimbal_small_2d . The original gimbal in the gazebo model library is out of date, therefore an updated one is included in this image. It also includes an updated plugin which allows control of the gimbal and streams the camera image. Environment Variables \u00b6 Name Default Value Description CAMERA_NAME camera ROS2 Name of the camera, camera topic is $VEHICLE_NAMESPACE/$CAMERA_NAME/image_raw . CAMERA_HEIGHT 480 Height resolution of camera image CAMERA_WIDTH 640 Width resolution of camera image GIMBAL_INITIAL_ANGLE 0.785 Initial angle (radians) of the gimbal. 0.0 Angle is forwards, pi/2 is down. Exposed Topics \u00b6 Name Topic Description $VEHICLE_NAMESPACE/$CAMERA_NAME/image_raw sensor_msgs/msg/Image Image topic from the camera attached to the gimbal $VEHICLE_NAMESPACE/$CAMERA_NAME/camera_info sensor_msgs/msg/CameraInfo Camera Info topic from the camera attached to the gimbal $VEHICLE_NAMESPACE/gimbal_tilt_cmd std_msgs/msg/Float32 The target angle (radians) of the gimbal camera tilt [0.0, 3.14]. $VEHICLE_NAMESPACE/gimbal_tilt_status std_msgs/msg/Float32 The current angle (radians) of the gimbal camera tilt [0.0, 3.14].","title":"starling-sim-iris-ap"},{"location":"docker-images/sim-iris-ap/#starling-sim-iris-ap","text":"Based on staring-sim-ardupilot-gazebo See for additional environment variables.","title":"starling-sim-iris-ap"},{"location":"docker-images/sim-iris-ap/#overview","text":"This image adds the launch files required to launch an Iris model with the ArduPilot plugin using Gazebo. The iris ardupilot model depends on the iris with standoffs and a gimbal_small_2d . The original gimbal in the gazebo model library is out of date, therefore an updated one is included in this image. It also includes an updated plugin which allows control of the gimbal and streams the camera image.","title":"Overview"},{"location":"docker-images/sim-iris-ap/#environment-variables","text":"Name Default Value Description CAMERA_NAME camera ROS2 Name of the camera, camera topic is $VEHICLE_NAMESPACE/$CAMERA_NAME/image_raw . CAMERA_HEIGHT 480 Height resolution of camera image CAMERA_WIDTH 640 Width resolution of camera image GIMBAL_INITIAL_ANGLE 0.785 Initial angle (radians) of the gimbal. 0.0 Angle is forwards, pi/2 is down.","title":"Environment Variables"},{"location":"docker-images/sim-iris-ap/#exposed-topics","text":"Name Topic Description $VEHICLE_NAMESPACE/$CAMERA_NAME/image_raw sensor_msgs/msg/Image Image topic from the camera attached to the gimbal $VEHICLE_NAMESPACE/$CAMERA_NAME/camera_info sensor_msgs/msg/CameraInfo Camera Info topic from the camera attached to the gimbal $VEHICLE_NAMESPACE/gimbal_tilt_cmd std_msgs/msg/Float32 The target angle (radians) of the gimbal camera tilt [0.0, 3.14]. $VEHICLE_NAMESPACE/gimbal_tilt_status std_msgs/msg/Float32 The current angle (radians) of the gimbal camera tilt [0.0, 3.14].","title":"Exposed Topics"},{"location":"docker-images/starling-vicon/","text":"starling-vicon \u00b6 This image contains a Vicon UDP to ROS2 bridge. It has some additional configuration to work with Starling, in that it reads the /etc/starling/vehicle.config file. This listens on port 51001, (the default Vicon UDP port) and retrieves the pose information for a target object. This pose information is then forwarded to the appropriate MAVROS topic. Contents \u00b6 starling-vicon Contents Environment Variables Behaviour Notes Running on a vehicle Environment Variables \u00b6 Name Default Value Description VEHICLE_NAME {unset} Vicon target object name. VEHICLE_NAMESPACE {unset} Namespace for mavros topics. Set to override default value of vehicle_${VEHICLE_MAVLINK_SYSID} When unset, both VEHICLE_NAME and VEHICLE_NAMESPACE are derived from the vehicle.config file, reading the name and MAVLink system ID defined there. Behaviour Notes \u00b6 Running on a vehicle \u00b6 Ensure /etc/starling/vehicle.config is mounted. The container is then configured from the contents of that file.","title":"starling-vicon"},{"location":"docker-images/starling-vicon/#starling-vicon","text":"This image contains a Vicon UDP to ROS2 bridge. It has some additional configuration to work with Starling, in that it reads the /etc/starling/vehicle.config file. This listens on port 51001, (the default Vicon UDP port) and retrieves the pose information for a target object. This pose information is then forwarded to the appropriate MAVROS topic.","title":"starling-vicon"},{"location":"docker-images/starling-vicon/#contents","text":"starling-vicon Contents Environment Variables Behaviour Notes Running on a vehicle","title":"Contents"},{"location":"docker-images/starling-vicon/#environment-variables","text":"Name Default Value Description VEHICLE_NAME {unset} Vicon target object name. VEHICLE_NAMESPACE {unset} Namespace for mavros topics. Set to override default value of vehicle_${VEHICLE_MAVLINK_SYSID} When unset, both VEHICLE_NAME and VEHICLE_NAMESPACE are derived from the vehicle.config file, reading the name and MAVLink system ID defined there.","title":"Environment Variables"},{"location":"docker-images/starling-vicon/#behaviour-notes","text":"","title":"Behaviour Notes"},{"location":"docker-images/starling-vicon/#running-on-a-vehicle","text":"Ensure /etc/starling/vehicle.config is mounted. The container is then configured from the contents of that file.","title":"Running on a vehicle"},{"location":"docker-images/ui/","text":"Rosbridge-suite \u00b6 Builds a container that serves a ros2 web-bridge using the rosbridge-suite The rosbridge is known as a 'json' bridge as it converts topics into json to be sent via websocket. Installation \u00b6 This is built using the ui target in the system Makefile to the container uobflightlabstarling/rosbridge-suite : cd <root of ProjectStarling> make ui Running \u00b6 This bridge is run using with default port 9090 : Linux : docker run -it --rm --net=host uobflightlabstarling/rosbridge-suite Windows : docker run -it --rm --network=projectstarling_default -p 9090:9090 uobflightlabstarling/rosbridge-suite The port can be specified by passing the environment variable ROSBRIDGE_PORT , i.e. docker run -it --rm --net=host -e ROSBRIDGE_PORT=<port> -p <localport>:<port> uobflightlabstarling/rosbridge-suite Application \u00b6 This can be used with any application which supports the ros2 web bridge suite. An example ui application is given in StarlingUAS/starling_ui_example A more complex system would be the dashboard created by foxglove studios : https://studio.foxglove.dev/? Note: Starling doesn't quite support foxglove just yet, there is a known bug here: https://github.com/foxglove/studio/issues/2035. If this is fixed it will be compatible. Kubernetes Deployment \u00b6 The UI can also be run within the kubernetes deployment and network. To start as a kubernetes Deployment simply apply the kubernetes.yaml file in this directory. sudo k3s kubectl apply -f kubernetes.yaml Notes \u00b6 This dockerfile installs mavros-foxy and gazebo-foxy messages. If other messages are required then either this container needs to be rebuilt with the new messages included (e.g. a build step) or they need to be injected in using the ros.env.d pipeline.","title":"starling-ui"},{"location":"docker-images/ui/#rosbridge-suite","text":"Builds a container that serves a ros2 web-bridge using the rosbridge-suite The rosbridge is known as a 'json' bridge as it converts topics into json to be sent via websocket.","title":"Rosbridge-suite"},{"location":"docker-images/ui/#installation","text":"This is built using the ui target in the system Makefile to the container uobflightlabstarling/rosbridge-suite : cd <root of ProjectStarling> make ui","title":"Installation"},{"location":"docker-images/ui/#running","text":"This bridge is run using with default port 9090 : Linux : docker run -it --rm --net=host uobflightlabstarling/rosbridge-suite Windows : docker run -it --rm --network=projectstarling_default -p 9090:9090 uobflightlabstarling/rosbridge-suite The port can be specified by passing the environment variable ROSBRIDGE_PORT , i.e. docker run -it --rm --net=host -e ROSBRIDGE_PORT=<port> -p <localport>:<port> uobflightlabstarling/rosbridge-suite","title":"Running"},{"location":"docker-images/ui/#application","text":"This can be used with any application which supports the ros2 web bridge suite. An example ui application is given in StarlingUAS/starling_ui_example A more complex system would be the dashboard created by foxglove studios : https://studio.foxglove.dev/? Note: Starling doesn't quite support foxglove just yet, there is a known bug here: https://github.com/foxglove/studio/issues/2035. If this is fixed it will be compatible.","title":"Application"},{"location":"docker-images/ui/#kubernetes-deployment","text":"The UI can also be run within the kubernetes deployment and network. To start as a kubernetes Deployment simply apply the kubernetes.yaml file in this directory. sudo k3s kubectl apply -f kubernetes.yaml","title":"Kubernetes Deployment"},{"location":"docker-images/ui/#notes","text":"This dockerfile installs mavros-foxy and gazebo-foxy messages. If other messages are required then either this container needs to be rebuilt with the new messages included (e.g. a build step) or they need to be injected in using the ros.env.d pipeline.","title":"Notes"},{"location":"guide/cli/","text":"Starling CLI \u00b6 This guide/tutorial provides a top to bottom set of instructions on how to use the Starling Command Line Interface present within the Murmuration repostiroy . This guide will be useful to those who have completed the first introduction and wish to start using starling more actively, especially with multiple vehicles. Starling CLI Background Installation Only local development (step 1) Multi-Vehicle Local testing (step 2) Multi-Vehicle Multi-Computer Setup Docker-Compose Usage Kind Usage Starting or stopping the kind cluster Monitoring: Running the simulator Deploying containers Docker-Compose Examples PX4 Examples Core System Simple Offboard with Trajectory Follower UI System Ardupilot Examples Core System Running External Examples Kubernetes Examples Background \u00b6 The starling cli is comprised of a number of bash and python scripts to help streamline the usage of common aspects of the Starling vehicle controller simulation stack. It is envisioned as aiding the process of following the Starling application development workflow: Developing and testing the application locally in containers on a single simulated drone Developing and testing the application locally within a simulated network archietcture of the flight arena using single or multiple simulated drones Developing and testing the application on real vehicles within the flight arena. The only requirement of the starling cli is a terminal running a shell and git. However please make sure you are running on a suitable computer as some of the applications are quite resource intensive. Installation \u00b6 There are no installation steps apart from cloneing this github repository into your local workspace. git clone https://github.com/StarlingUAS/Murmuration.git In the root of the repository, there is the core cli script named starling . starling includes a further installation script to help install further requirements. This installation script will need to be run using root. See the following guide on which arguments you should give. If running within Murmuration, swap starling for ./starling . However for convenience, you can put starling onto your path. This can be done by adding export PATH=<Path to murmuration>:$PATH into ~/.bashrc and source ~/.bashrc , or running the same command locally in the terminal. Then you can use the below commands verbatim. Only local development (step 1) \u00b6 To install the bare minimum requirements of starling, run the following. sudo starling install This will Update the system Install python3, pip, curl, udmap and a python utility called click Add the Mumuration directory to your path so you can run starling from anywhere Install docker Multi-Vehicle Local testing (step 2) \u00b6 For this we utilise kubernetes in docker, a.k.a kind link : sudo starling install kind This is an application which will allow us to test out kubernetes deployments within a docker container without needing to install kubernetes elements locally. It also allows windows and mac users to develop starling applications as well. Multi-Vehicle Multi-Computer Setup \u00b6 The final step allows you to setup the setup used in the flight arena using multiple physical machines. For this, we utilise k3s. This is not recommended for most users. Most should be able to use kind for testing, to then deploy straight in the flight arena. sudo starling install k3s Docker-Compose Usage \u00b6 The starling utility is made up of a number of useful scripts which can be used together to run the simulation and your own controllers. Using starling with docker-compose is very simple and essentially a drop in replacement. Once starling has installed, a docker compose configuration yaml file can be run with the following command: starling deploy -f <docker compose file> # e.g. starling deploy -f docker-compose.px4.yaml # or if the system needs to build a local container starling deploy -f docker-compose.px4.yaml --build The container can then be stopped by pressing CTRL+C . If other docker-compose commands are required, you can just use the docker-compose cli. If you want to run a container standalone, please refer to normal usage of docker run .... . Kind Usage \u00b6 The starling utility is made up of a number of useful scripts which can be used together to run the simulation and your own controllers Starting or stopping the kind cluster \u00b6 Running the following will give you a single drone cluster starling start kind If you are looking to experiment with a multi-drone system, you can specify the number using the -n option. For example for a 3 node (drone) cluster starling start kind -n 3 To stop the cluster, simply run the following. Note that this will delete all changes within the kind cluster and maybe require re-setup. starling stop kind Monitoring: \u00b6 For more details, see the following tutorial for an illustrated guide. Once started, you can start the monitoring dashboard using the following command: starling start dashboard This will start up the kubernetes dashboard . To access the dashboard, open up a browser and go to https://localhost:31771. Note the browser may not like the security, we promise it is safe to access! If you click 'advanced options' and 'continue to website' you should be able to access the website. To log on, the above command should show something like the following: The Dashboard is available at https://localhost:31771 You will need the dashboard token, to access it. Copy and paste the token from below -----BEGIN DASHBOARD TOKEN----- <LONG TOKEN> -----END DASHBOARD TOKEN----- Note: your browser may not like the self signed ssl certificate, ignore and continue for now To get the token yourself run: kubectl -n kubernetes-dashboard describe secret admin-user-token You can copy the <LONG TOKEN> and paste it into the dashboard token. You can also get the access token again by running: starling utils get-dashboard-token You can also monitor the system using starling status # or to continually watch starling status --watch And finally, you can inspect the system using the standard kubectl commands Running the simulator \u00b6 Once the simulator in kind has started, we can start the general simulator. IMPORTANT : The simulator can potentially be resource heavy to run. This is with respect to both CPU usage and RAM. Before going further, please ensure you have enough space on your primary drive (at least 30Gb to be safe, especially C drive on windows). This is especially true if running multiple vehicles. It is not recommended to run more than around 6. First we should load or download the required simulation containers locally. This can be done using the follwoing command. We need to run the load command as we want to load the local container images into the kind container. This avoids the need for the kind containers to download the containers themselves at each runtime. This command can take as long as 30 minutes depending on internet connection. It goes through the deployment files and downloads a couple of large containers e.g. the gazebo and sitl containers. If the container doesn't already exist locally, it will attempt to download it from docker hub starling simulator load Once containers are downloaded and loaded into Kind, we can start the simulator containers using the following: starling simulator start # or to do both steps at the same time: starling simulator start --load Once run, you can monitor the deployments on the kubernetes dashboard. In particular you can inspect the worloads -> pods . If they show green the systems should hopefully have started correctly. Again, see this illustrated guide to the dashboard Note, if the load was unsucessful, or had issues, some containers might take a while to download starling containers. Once started, the simulator should be available to inspect from https://localhost:8080 At this point, you can run your own deployments or some of the examples. If at any point, you want to restart the simulation, you can run the following. This will delete the starling simulator containers, and then start them again. starling simulator restart If you want to stop the simulation, simply run starling simulator stop Deploying containers \u00b6 The primary usage of starling is to test your local containers. To do so, please follow the following steps. First, if it is a local container - a container image which exists on your local machine either through having pulled it from docker hub, or built locally - you will need to load that image into kind. Each time you rebuild or change the image, you will need to load it into kind. This can be achieved using the following command: Note: If the container doesnt exist locally it will automatically try and download it from docker hub starling utils kind-load <container name> # e.g. starling utils kind-load uobflightlabstarling/starling-mavros:latest Secondly, you will need to write a kubernetes deployment file known as a kubeconfig. A kubeconfig is a yaml file which specifies what sort of deployment you want, along with many options (where to run your controller etc.). For now, see the deployment folder and the repositories in StarlingUAS for examples. In particular we mention the existence of two types of deployment: Deployment : This specfies the self-healing deployment of one pod (a.k.a a collection of containers) to a node. DaemonSet : This specifies the self-healing automatic deployment of a pod to all nodes which match a given specification. Use this for a deployment to all vehicles. Once a kubernetes configuration has been written, it can be deployed to kind. starling deploy -k <path to kubeconfig file> start Note if you have added starling to path (through standard installation or otherwise), you can run this from any directory, in particular the directory of the project you wish to deploy. If you haven't, you may need to give a full absolute path. Note if kind is not active or installed, starling deploy will interpret commands for docker-compose . To stop or restart a deployment you can use the following: starling deploy -k <path to kubeconfig file> stop starling deploy -k <path to kubeconfig file> restart To restart a deployment with an update to the container, you just need to ensure you have loaded it in before you restart: starling utils kind-load <container name> starling deploy -k <path to kubeconfig file> restart Again, use the dashboard to monitor the status of your deployment Docker-Compose Examples \u00b6 The docker-compose contains a number of preset examples which spin up at minimum a simulator, a software autopilot, mavros and a ros webridge, but also example ui's and basic controllers. Each example file has a linux and windows variant. - The linux variant runs network_mode=host which allows for ROS2 traffic to be shared between the container and your local network. Therefore the traffic is visible to your machine's local (non-container) ROS2 foxy instance, allowing you to run bare-metal application such as rviz2 or your own controllers (i.e. you do not need to wrap your own controllers in a docker container). Any exposed ports are automatically exposed to localhost . - The windows variant runs inside a docker-compose network named <folder-which the-docker-compose-file-is-in>_default , (e.g. running a px4 example will create a local network px4_default ). This network is segregated from your local network traffic except for the exposed ports in the docker-compose file which are now accessible from localhost (Windows has no support of net=host ). Any other ROS2 nodes will need to be wrapped in a docker container for running and run with --network px4_default or --network ardupilot_default . See the example controller repository for an example ROS2-in-docker setup. PX4 Examples \u00b6 The docker-compose/px4 folder contains a number of px4 based examples. See the README for more details. Core System \u00b6 To start the simulator, a px4 SITL, mavros and the web-bridge, use the following command: # Pull and Run the docker-containers starling deploy -f docker-compose/px4/docker-compose.core.linux.yml --pull # or for windows starling deploy -f docker-compose/px4/docker-compose.core.windows.yml --pull Simple Offboard with Trajectory Follower UI System \u00b6 To start the core with a simple-offboard controller , simple-allocator and Trajectory follower Web GUI , use the following command: # Run the docker-containers starling deploy -f docker-compose/px4/docker-compose.simple-offboard.linux.yml --pull # or for windows starling deploy -fdocker-compose/px4/docker-compose.simple-offboard.windows.yml --pull Ardupilot Examples \u00b6 The docker-compose/ardupilot folder contains a number of px4 based examples. See the README for more details. Core System \u00b6 To start the gazebo simulator, an arducopter SITL, mavros and the web-bridge, use the following command: # Run the docker-containers starling deploy -f docker-compose/ardupilot/docker-compose.ap-gazebo.linux.yml --pull # or for windows starling deploy -f docker-compose/ardupilot/docker-compose.ap-gazebo.windows.yml --pull Running External Examples \u00b6 An example offboard ROS2 controller can then be conncted to SITL by running the following in a separate terminal. For this we have to use docker 's built in command line interface. # Download the latest container docker pull uobflightlabstarling/example_controller_python docker run -it --rm --network px4_default uobflightlabstarling/example_controller_python # or for ardupilot docker run -it --rm --network ardupilot_default uobflightlabstarling/example_controller_python See the docs for further details Kubernetes Examples \u00b6 A folder of example deployments is provided in the repository. Currently there is only the one example, but hopefully more will follow! These can all be run using the deploy command with the name of the example (corresponds to name of the folder) starling deploy example <example-name> #e.g. starling deploy example simple-offboard simple-offboard - This example deploys 3 elements which together allow the running of simple waypoint following examples through a graphical user interface on one or more UAVs. Can be run using starling deploy example simple-offboard once the simulator has been initialised A daemonset is used to deploy the simple offboard controller to each vehicle. This provides a high level interface for controlling a vehicle An example Python based UI using the dash library . This provides a graphical user interface to upload and fly trajectories. An allocator module which takes a trajectory from the UI and distributes it to a particular vehicle to fly.","title":"2. The Starling CLI"},{"location":"guide/cli/#starling-cli","text":"This guide/tutorial provides a top to bottom set of instructions on how to use the Starling Command Line Interface present within the Murmuration repostiroy . This guide will be useful to those who have completed the first introduction and wish to start using starling more actively, especially with multiple vehicles. Starling CLI Background Installation Only local development (step 1) Multi-Vehicle Local testing (step 2) Multi-Vehicle Multi-Computer Setup Docker-Compose Usage Kind Usage Starting or stopping the kind cluster Monitoring: Running the simulator Deploying containers Docker-Compose Examples PX4 Examples Core System Simple Offboard with Trajectory Follower UI System Ardupilot Examples Core System Running External Examples Kubernetes Examples","title":"Starling CLI"},{"location":"guide/cli/#background","text":"The starling cli is comprised of a number of bash and python scripts to help streamline the usage of common aspects of the Starling vehicle controller simulation stack. It is envisioned as aiding the process of following the Starling application development workflow: Developing and testing the application locally in containers on a single simulated drone Developing and testing the application locally within a simulated network archietcture of the flight arena using single or multiple simulated drones Developing and testing the application on real vehicles within the flight arena. The only requirement of the starling cli is a terminal running a shell and git. However please make sure you are running on a suitable computer as some of the applications are quite resource intensive.","title":"Background"},{"location":"guide/cli/#installation","text":"There are no installation steps apart from cloneing this github repository into your local workspace. git clone https://github.com/StarlingUAS/Murmuration.git In the root of the repository, there is the core cli script named starling . starling includes a further installation script to help install further requirements. This installation script will need to be run using root. See the following guide on which arguments you should give. If running within Murmuration, swap starling for ./starling . However for convenience, you can put starling onto your path. This can be done by adding export PATH=<Path to murmuration>:$PATH into ~/.bashrc and source ~/.bashrc , or running the same command locally in the terminal. Then you can use the below commands verbatim.","title":"Installation"},{"location":"guide/cli/#only-local-development-step-1","text":"To install the bare minimum requirements of starling, run the following. sudo starling install This will Update the system Install python3, pip, curl, udmap and a python utility called click Add the Mumuration directory to your path so you can run starling from anywhere Install docker","title":"Only local development (step 1)"},{"location":"guide/cli/#multi-vehicle-local-testing-step-2","text":"For this we utilise kubernetes in docker, a.k.a kind link : sudo starling install kind This is an application which will allow us to test out kubernetes deployments within a docker container without needing to install kubernetes elements locally. It also allows windows and mac users to develop starling applications as well.","title":"Multi-Vehicle Local testing (step 2)"},{"location":"guide/cli/#multi-vehicle-multi-computer-setup","text":"The final step allows you to setup the setup used in the flight arena using multiple physical machines. For this, we utilise k3s. This is not recommended for most users. Most should be able to use kind for testing, to then deploy straight in the flight arena. sudo starling install k3s","title":"Multi-Vehicle Multi-Computer Setup"},{"location":"guide/cli/#docker-compose-usage","text":"The starling utility is made up of a number of useful scripts which can be used together to run the simulation and your own controllers. Using starling with docker-compose is very simple and essentially a drop in replacement. Once starling has installed, a docker compose configuration yaml file can be run with the following command: starling deploy -f <docker compose file> # e.g. starling deploy -f docker-compose.px4.yaml # or if the system needs to build a local container starling deploy -f docker-compose.px4.yaml --build The container can then be stopped by pressing CTRL+C . If other docker-compose commands are required, you can just use the docker-compose cli. If you want to run a container standalone, please refer to normal usage of docker run .... .","title":"Docker-Compose Usage"},{"location":"guide/cli/#kind-usage","text":"The starling utility is made up of a number of useful scripts which can be used together to run the simulation and your own controllers","title":"Kind Usage"},{"location":"guide/cli/#starting-or-stopping-the-kind-cluster","text":"Running the following will give you a single drone cluster starling start kind If you are looking to experiment with a multi-drone system, you can specify the number using the -n option. For example for a 3 node (drone) cluster starling start kind -n 3 To stop the cluster, simply run the following. Note that this will delete all changes within the kind cluster and maybe require re-setup. starling stop kind","title":"Starting or stopping the kind cluster"},{"location":"guide/cli/#monitoring","text":"For more details, see the following tutorial for an illustrated guide. Once started, you can start the monitoring dashboard using the following command: starling start dashboard This will start up the kubernetes dashboard . To access the dashboard, open up a browser and go to https://localhost:31771. Note the browser may not like the security, we promise it is safe to access! If you click 'advanced options' and 'continue to website' you should be able to access the website. To log on, the above command should show something like the following: The Dashboard is available at https://localhost:31771 You will need the dashboard token, to access it. Copy and paste the token from below -----BEGIN DASHBOARD TOKEN----- <LONG TOKEN> -----END DASHBOARD TOKEN----- Note: your browser may not like the self signed ssl certificate, ignore and continue for now To get the token yourself run: kubectl -n kubernetes-dashboard describe secret admin-user-token You can copy the <LONG TOKEN> and paste it into the dashboard token. You can also get the access token again by running: starling utils get-dashboard-token You can also monitor the system using starling status # or to continually watch starling status --watch And finally, you can inspect the system using the standard kubectl commands","title":"Monitoring:"},{"location":"guide/cli/#running-the-simulator","text":"Once the simulator in kind has started, we can start the general simulator. IMPORTANT : The simulator can potentially be resource heavy to run. This is with respect to both CPU usage and RAM. Before going further, please ensure you have enough space on your primary drive (at least 30Gb to be safe, especially C drive on windows). This is especially true if running multiple vehicles. It is not recommended to run more than around 6. First we should load or download the required simulation containers locally. This can be done using the follwoing command. We need to run the load command as we want to load the local container images into the kind container. This avoids the need for the kind containers to download the containers themselves at each runtime. This command can take as long as 30 minutes depending on internet connection. It goes through the deployment files and downloads a couple of large containers e.g. the gazebo and sitl containers. If the container doesn't already exist locally, it will attempt to download it from docker hub starling simulator load Once containers are downloaded and loaded into Kind, we can start the simulator containers using the following: starling simulator start # or to do both steps at the same time: starling simulator start --load Once run, you can monitor the deployments on the kubernetes dashboard. In particular you can inspect the worloads -> pods . If they show green the systems should hopefully have started correctly. Again, see this illustrated guide to the dashboard Note, if the load was unsucessful, or had issues, some containers might take a while to download starling containers. Once started, the simulator should be available to inspect from https://localhost:8080 At this point, you can run your own deployments or some of the examples. If at any point, you want to restart the simulation, you can run the following. This will delete the starling simulator containers, and then start them again. starling simulator restart If you want to stop the simulation, simply run starling simulator stop","title":"Running the simulator"},{"location":"guide/cli/#deploying-containers","text":"The primary usage of starling is to test your local containers. To do so, please follow the following steps. First, if it is a local container - a container image which exists on your local machine either through having pulled it from docker hub, or built locally - you will need to load that image into kind. Each time you rebuild or change the image, you will need to load it into kind. This can be achieved using the following command: Note: If the container doesnt exist locally it will automatically try and download it from docker hub starling utils kind-load <container name> # e.g. starling utils kind-load uobflightlabstarling/starling-mavros:latest Secondly, you will need to write a kubernetes deployment file known as a kubeconfig. A kubeconfig is a yaml file which specifies what sort of deployment you want, along with many options (where to run your controller etc.). For now, see the deployment folder and the repositories in StarlingUAS for examples. In particular we mention the existence of two types of deployment: Deployment : This specfies the self-healing deployment of one pod (a.k.a a collection of containers) to a node. DaemonSet : This specifies the self-healing automatic deployment of a pod to all nodes which match a given specification. Use this for a deployment to all vehicles. Once a kubernetes configuration has been written, it can be deployed to kind. starling deploy -k <path to kubeconfig file> start Note if you have added starling to path (through standard installation or otherwise), you can run this from any directory, in particular the directory of the project you wish to deploy. If you haven't, you may need to give a full absolute path. Note if kind is not active or installed, starling deploy will interpret commands for docker-compose . To stop or restart a deployment you can use the following: starling deploy -k <path to kubeconfig file> stop starling deploy -k <path to kubeconfig file> restart To restart a deployment with an update to the container, you just need to ensure you have loaded it in before you restart: starling utils kind-load <container name> starling deploy -k <path to kubeconfig file> restart Again, use the dashboard to monitor the status of your deployment","title":"Deploying containers"},{"location":"guide/cli/#docker-compose-examples","text":"The docker-compose contains a number of preset examples which spin up at minimum a simulator, a software autopilot, mavros and a ros webridge, but also example ui's and basic controllers. Each example file has a linux and windows variant. - The linux variant runs network_mode=host which allows for ROS2 traffic to be shared between the container and your local network. Therefore the traffic is visible to your machine's local (non-container) ROS2 foxy instance, allowing you to run bare-metal application such as rviz2 or your own controllers (i.e. you do not need to wrap your own controllers in a docker container). Any exposed ports are automatically exposed to localhost . - The windows variant runs inside a docker-compose network named <folder-which the-docker-compose-file-is-in>_default , (e.g. running a px4 example will create a local network px4_default ). This network is segregated from your local network traffic except for the exposed ports in the docker-compose file which are now accessible from localhost (Windows has no support of net=host ). Any other ROS2 nodes will need to be wrapped in a docker container for running and run with --network px4_default or --network ardupilot_default . See the example controller repository for an example ROS2-in-docker setup.","title":"Docker-Compose Examples"},{"location":"guide/cli/#px4-examples","text":"The docker-compose/px4 folder contains a number of px4 based examples. See the README for more details.","title":"PX4 Examples"},{"location":"guide/cli/#core-system","text":"To start the simulator, a px4 SITL, mavros and the web-bridge, use the following command: # Pull and Run the docker-containers starling deploy -f docker-compose/px4/docker-compose.core.linux.yml --pull # or for windows starling deploy -f docker-compose/px4/docker-compose.core.windows.yml --pull","title":"Core System"},{"location":"guide/cli/#simple-offboard-with-trajectory-follower-ui-system","text":"To start the core with a simple-offboard controller , simple-allocator and Trajectory follower Web GUI , use the following command: # Run the docker-containers starling deploy -f docker-compose/px4/docker-compose.simple-offboard.linux.yml --pull # or for windows starling deploy -fdocker-compose/px4/docker-compose.simple-offboard.windows.yml --pull","title":"Simple Offboard with Trajectory Follower UI System"},{"location":"guide/cli/#ardupilot-examples","text":"The docker-compose/ardupilot folder contains a number of px4 based examples. See the README for more details.","title":"Ardupilot Examples"},{"location":"guide/cli/#core-system_1","text":"To start the gazebo simulator, an arducopter SITL, mavros and the web-bridge, use the following command: # Run the docker-containers starling deploy -f docker-compose/ardupilot/docker-compose.ap-gazebo.linux.yml --pull # or for windows starling deploy -f docker-compose/ardupilot/docker-compose.ap-gazebo.windows.yml --pull","title":"Core System"},{"location":"guide/cli/#running-external-examples","text":"An example offboard ROS2 controller can then be conncted to SITL by running the following in a separate terminal. For this we have to use docker 's built in command line interface. # Download the latest container docker pull uobflightlabstarling/example_controller_python docker run -it --rm --network px4_default uobflightlabstarling/example_controller_python # or for ardupilot docker run -it --rm --network ardupilot_default uobflightlabstarling/example_controller_python See the docs for further details","title":"Running External Examples"},{"location":"guide/cli/#kubernetes-examples","text":"A folder of example deployments is provided in the repository. Currently there is only the one example, but hopefully more will follow! These can all be run using the deploy command with the name of the example (corresponds to name of the folder) starling deploy example <example-name> #e.g. starling deploy example simple-offboard simple-offboard - This example deploys 3 elements which together allow the running of simple waypoint following examples through a graphical user interface on one or more UAVs. Can be run using starling deploy example simple-offboard once the simulator has been initialised A daemonset is used to deploy the simple offboard controller to each vehicle. This provides a high level interface for controlling a vehicle An example Python based UI using the dash library . This provides a graphical user interface to upload and fly trajectories. An allocator module which takes a trajectory from the UI and distributes it to a particular vehicle to fly.","title":"Kubernetes Examples"},{"location":"guide/development/","text":"Developing your own containers \u00b6 This guide shows recommended methods for developing your own controllers and using Starling to develop your experiments. Contents \u00b6 Developing your own containers Contents Development Environment Developing a container VSCode / ROS2 Template Project Development Environment \u00b6 We recommend that the any user looking to do development uses Visual Studio Code . VScode is a free cross platform, general use text editor and development environment with a vast array of built in tools. In particular we recommend VScode for use with Starling as it has a number of easy to use extensions suited for remote development which includes Development over SSH (i.e. developing onboard a drone) Development inside a local docker container Development inside a remote docker container (e.g. a container running on a Raspberry Pi connected over SSH)! Developing a container \u00b6 There are actually a number of methods of developing an application within a containerised environment. Developing fully outside of the container and then building only using docker build. This is the simplest and most straight forward method, recommended for most applications. Iteration can prove to be a little slow as docker build attempts to rebuild the entire container. Creating/pulling the project outside of the container and bind mounting the project into the container and building within. This can be achieved by using the bind syntax: docker run -itd --name <container reference name> -v $(pwd)/<my local files>:/ros_ws/src/ <containername> . Ommitting the --rm will stop the container from being automatically removed, and therefore the container will always be available/save your working environment. This essentially places your local files into the docker container for the container to interact with (read/write/execute). The local files can then be viewed and edited by using the docker remote development tool in vscode, then run via the integrated terminal (remember to source ros using source /opt/ros/foxy/setup.bash ). Any local changes can be built using standard ROS2 procedures (i.e. cd /ros_ws && colon build packages-select <my package name> ) Iteration is very quick as changes can be made and built/tested from within the container. Any local changes can then be pushed to github/repositories in the usual manner. Note that files created inside the container may have permissions issues, these can be solved by running sudo chown -R <user>:<user> <my project> . Container can be restarted if the container enviornment is not quite right (e.g. need more ports or different volume mounted or need different network configuration) Recommended method for more complex or detailed applications. Running a Docker container (such as uobflightlabstarling/starling-controller-base ) and creating/pulling the project fully inside the container. Start the container without the --rm , and giving it a permanent name using --name <container name> . This will ensure the container is persistent. Use VScode to remote inside the container Using the integrated terminal (or by using docker exec ... ) install git and any other useful libraries or tools Pull or start your project within the ros workspace ( /ros_ws ) Any local changes can be built using standard ROS2 procedures (i.e. cd /ros_ws && colon build packages-select <my package name> ) Iteration is very quick and your local environment need not be modified in any way. Be aware that you may need to repeat some of these steps after restarting the container if the container environment (e.g. ports/networking) needs to be changed. Recommended method if quick changes need to be tested, but not for full development in case the environment needs to be changed. VSCode / ROS2 Template Project \u00b6 There is a template repository within the StarlingUAS organisation called vscode_ros2_workspace . This was developed by an active VSCode/ROS2 developer working in industry. This template most likely has many features that you are possibly unlikely to use, but will give you a flavour of what developing systems in ROS2 is like out there. Feel free to try and use this template, I would recommend thoroughly reading through the repository README for the full instructions. Also if you are on windows you may need to change line 11 of .devcontainer/devcontainer.json to the name of the current docker network (if using docker-compose - see the tutorial example single drone local).","title":"Development Guide"},{"location":"guide/development/#developing-your-own-containers","text":"This guide shows recommended methods for developing your own controllers and using Starling to develop your experiments.","title":"Developing your own containers"},{"location":"guide/development/#contents","text":"Developing your own containers Contents Development Environment Developing a container VSCode / ROS2 Template Project","title":"Contents"},{"location":"guide/development/#development-environment","text":"We recommend that the any user looking to do development uses Visual Studio Code . VScode is a free cross platform, general use text editor and development environment with a vast array of built in tools. In particular we recommend VScode for use with Starling as it has a number of easy to use extensions suited for remote development which includes Development over SSH (i.e. developing onboard a drone) Development inside a local docker container Development inside a remote docker container (e.g. a container running on a Raspberry Pi connected over SSH)!","title":"Development Environment"},{"location":"guide/development/#developing-a-container","text":"There are actually a number of methods of developing an application within a containerised environment. Developing fully outside of the container and then building only using docker build. This is the simplest and most straight forward method, recommended for most applications. Iteration can prove to be a little slow as docker build attempts to rebuild the entire container. Creating/pulling the project outside of the container and bind mounting the project into the container and building within. This can be achieved by using the bind syntax: docker run -itd --name <container reference name> -v $(pwd)/<my local files>:/ros_ws/src/ <containername> . Ommitting the --rm will stop the container from being automatically removed, and therefore the container will always be available/save your working environment. This essentially places your local files into the docker container for the container to interact with (read/write/execute). The local files can then be viewed and edited by using the docker remote development tool in vscode, then run via the integrated terminal (remember to source ros using source /opt/ros/foxy/setup.bash ). Any local changes can be built using standard ROS2 procedures (i.e. cd /ros_ws && colon build packages-select <my package name> ) Iteration is very quick as changes can be made and built/tested from within the container. Any local changes can then be pushed to github/repositories in the usual manner. Note that files created inside the container may have permissions issues, these can be solved by running sudo chown -R <user>:<user> <my project> . Container can be restarted if the container enviornment is not quite right (e.g. need more ports or different volume mounted or need different network configuration) Recommended method for more complex or detailed applications. Running a Docker container (such as uobflightlabstarling/starling-controller-base ) and creating/pulling the project fully inside the container. Start the container without the --rm , and giving it a permanent name using --name <container name> . This will ensure the container is persistent. Use VScode to remote inside the container Using the integrated terminal (or by using docker exec ... ) install git and any other useful libraries or tools Pull or start your project within the ros workspace ( /ros_ws ) Any local changes can be built using standard ROS2 procedures (i.e. cd /ros_ws && colon build packages-select <my package name> ) Iteration is very quick and your local environment need not be modified in any way. Be aware that you may need to repeat some of these steps after restarting the container if the container environment (e.g. ports/networking) needs to be changed. Recommended method if quick changes need to be tested, but not for full development in case the environment needs to be changed.","title":"Developing a container"},{"location":"guide/development/#vscode-ros2-template-project","text":"There is a template repository within the StarlingUAS organisation called vscode_ros2_workspace . This was developed by an active VSCode/ROS2 developer working in industry. This template most likely has many features that you are possibly unlikely to use, but will give you a flavour of what developing systems in ROS2 is like out there. Feel free to try and use this template, I would recommend thoroughly reading through the repository README for the full instructions. Also if you are on windows you may need to change line 11 of .devcontainer/devcontainer.json to the name of the current docker network (if using docker-compose - see the tutorial example single drone local).","title":"VSCode / ROS2 Template Project"},{"location":"guide/different_vehicles/","text":"Different Vehicles in Simulation \u00b6 With the way the system is setup, it can be quite simple to run another vehicle in simulation, this page shows two different methods of achieving this effect. Different Vehicles in Simulation Using Existing Models Example: Rover Using existing models which are not included in the container Adding your own Models into the container Background and /ros.env.d Setting up the project Custom Model Specifying the model Building the model (setup.bash) Custom World Specifying the world Building the model (setup.bash) Building and Running the models Docker-Compose DockerFile Using Existing Models \u00b6 To reduce the size of the containers, they only contain the minimal number of vehicle models For PX4 SITL container, there are only two models (1) iris (2) r1_rover. For Ardupilot SITL container, not many exist ... see the next section for how to add your own. For those which already exist it is simply a case of setting the PX4_SIM_MODEL envrionment variable when you run uobflightlabstarling/starling-sim-iris in order to specify the model which will be rendered. By default this is iris uobflightlabstarling/starling-sim-px4-sitl:latest in order to ensure that the correct SITL parameters have been loaded. By default this is also iris Example: Rover \u00b6 Example docker-compose to run the r1-rover version: '3' services: simhost: image: uobflightlabstarling/starling-sim-iris environment: - \"PX4_SIM_MODEL=r1-rover\" #new! ports: - \"8080:8080\" sitl: image: uobflightlabstarling/starling-sim-px4-sitl:latest environment: - \"PX4_SIM_HOST=simhost\" - \"PX4_OFFBOARD_HOST=mavros\" - \"PX4_SIM_MODEL=r1-rover\" # new! ports: - \"18570:18570/udp\" mavros: image: uobflightlabstarling/starling-mavros:latest command: ros2 launch launch/mavros_bridge.launch.xml environment: - \"MAVROS_TGT_SYSTEM=1\" - \"MAVROS_FCU_IP=0.0.0.0\" - \"MAVROS_GCS_URL=tcp-l://0.0.0.0:5760\" ports: - \"5760:5760\" Using existing models which are not included in the container \u00b6 As mentioned above, there are a number of models which should be included, but have been deleted by default to reduce the size of the container. For example for the PX4 SITL container, the following models should all exist: github link and here , but almost all of them are deleted. You can specify other vehicles to keep by rebuilding starling-sim-base-core with the build arg KEEP_PX4_VEHICLES set to the vehicles you wish to keep. To rebuild you will need to clone the ProjectStarling repository, navigate to simulation and locally rebuild the correct controller: docker build --build-args KEEP_PX4_VEHICLES=\"! -o -path iris_with_standoffs\" Adding your own Models into the container \u00b6 There will be many cases in which you will want to add extra models into your simulations. This might be to create an environment, or to add your own custom vehicles. For some exaples of projects which utilise custom models or worlds, please see the following: FenswoodScenario where a number of custom models (downloaded from the gazebo repository) and a custom world is defined. BRLFlightArenaGazebo which is comprised of a custom world and a custom gimbal-tripod model built from scratch. Background and /ros.env.d \u00b6 Both the base simulation container , and the base controller container both use a /ros.env.d mechanism. Both containers are set-up with a special folder in the root directory called /ros.env.d . When the container is started up, there is a special script that runs (called the entrypoint script) which looks at all of the folders within /ros.env.d and whether they have their own setup.bash script. If a setup.bash script exists, it will be source 'd , i.e. script is run with any environment variables defined being exported to be used in later processes. By mounting external folders into the Dockerfile into /ros.env.d , we can add external and extra resources into a given controller or simulation container. Setting up the project \u00b6 The general format for a starling project with extra assets is the following: project/ \u251c\u2500 custom_models/ \u2502 \u251c\u2500 model_1/ \u2502 \u2502 \u251c\u2500 setup.bash \u2502 \u2502 \u251c\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500 model_1/ \u2502 \u2502 \u2502 | \u251c\u2500 meshes/ # optionally populated \u2502 \u2502 \u2502 \u2502 \u251c\u2500 model.config \u2502 \u2502 \u251c\u2500 model.sdf.xacro \u2502 \u251c\u2500 model_2/ \u2502 \u2502 \u251c\u2500 setup.bash \u2502 \u2502 \u251c\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500 model_2/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500 model.config \u2502 \u2502 \u251c\u2500 model.sdf.xacro \u251c\u2500 custom_world/ \u2502 \u251c\u2500 world/ \u2502 \u2502 \u251c\u2500 my_custom_world.world \u2502 \u251c\u2500 models/ \u2502 \u2502 \u251c\u2500 some_downloaded_model_asset/ \u2502 \u251c\u2500 setup.bash \u251c\u2500 docker-compose.yaml \u251c\u2500 system.launch.xml \u251c\u2500 Makefile \u251c\u2500 Dockerfile This project seperates custom models and the custom worlds into two seperate folders. The system.launch.xml defines the roslaunch file being run by the container. The Dockerfile will then copy all of these elements into the container at build time. Custom Model \u00b6 Specifying the model \u00b6 As above, it is recommended that you have 3 items within the custom model folder. setup.bash which we will populate later model.sdf.xacro defines the model as an sdf file with xacro argument replacement, I will explain what this means next models/ folder which creates a template of the file structure required by gazebo for any model. Xacro is handy tool for specifying arguments to an sdf file in order to generate custom sdf files. This is needed as the sdf file which describes a model is usually hard coded to a particular vehicle instance. Xacro allows us to dynamically generate these files. An example model which uses the model.sdf.xacro is given here: <?xml version='1.0'?> <sdf version=\"1.6\" xmlns:xacro='http://ros.org/wiki/xacro'> <xacro:property name=\"ros_namespace\" value=\"$(arg ros_namespace)\"/> <model name=\"iris_demo\"> <link name=\"tripod\"> <collision name=\"geom_1\"> <geometry> <box><size>0.5 0.5 1.5 0 0 0</size></box> </geometry> </collision> <!-- Keep collision box, but dont show it --> <visual name=\"tripod\"> <geometry> <box><size>0.5 0.5 1.5 0 0 0</size></box> </geometry> <material> <uri>file://media/materials/scripts/gazebo.material</uri> <script>Gazebo/Grey</script> </material> </visual> </link> <include> <uri>model://gimbal_small_2d</uri> <pose>0 0 1.0 1.57 0 1.57</pose> </include> <joint name=\"${ros_namespace}_gimbal_mount\" type=\"revolute\"> <parent>tripod</parent> <child>gimbal_small_2d::base_link</child> <axis> <limit> <lower>0</lower> <upper>0</upper> </limit> <xyz>0 0 1</xyz> <use_parent_model_frame>true</use_parent_model_frame> </axis> </joint> </model> </sdf> This example describes the sdf file for a gimbal mounted on a simple tripod block. The xacro elements specify a 'property' of ros_namespace . When compiling with the xacro.py tool, it can generate a new sdf file with that property set to a value you want. You must then setup a model directory looking like the following (with optional meshes if using a model from the internet) \u2502 \u2502 \u251c\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500 model_1/ \u2502 \u2502 \u2502 | \u251c\u2500 meshes/ # optionally populated \u2502 \u2502 \u2502 \u2502 \u251c\u2500 model.config The exact contents of the model file follow gazebo guidelines. There is an overview here . There are a number online as well. Note that Xacro does not change the sdf or gazebo model syntax. It can be thought of as doing a copy paste to your existing file. Building the model ( setup.bash ) \u00b6 Now we have the model defined, we now want to ensure that the model can now be built/configured/compiled during the container runtime. This involves writing the setup.bash file. Usually there are 3 steps: Making sure the script is being run from the correct directory, and that the relevant paths are set. Apply xacro with environment variables (given by the user or the container itself) and saving it in the model folder path (next to model.config ) Adding the model to the gazebo model path Here is an example setup.bash for the gimbal example. #!/bin/bash SCRIPT_DIR=\"$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\" XACRO_PATH=${SCRIPT_DIR}/model.sdf.xacro MODEL_PATH=${SCRIPT_DIR}/models/gimbal_tripod/model.sdf xacro ${XACRO_PATH} \\ ros_namespace:=${GIMBAL_NAMESPACE} \\ -o ${MODEL_PATH} echo \"Written Gimbal model to ${MODEL_PATH}\" export GAZEBO_MODEL_PATH=\"${SCRIPT_DIR}/models:${GAZEBO_MODEL_PATH}\" In particular the xacro command takes the sdf.xacro we created previously and creates a new version with the parameters realised - here ros_namespace is set to the environment variable GIMBAL_NAMESPACE . Environment variables are often defined in the Dockerfile or by the user. Custom World \u00b6 Specifying the world \u00b6 A world can be defined by the worlds folder. Inside the worlds folder, there is the world file which describes the base world gazebo will generate. The world file itself is an sdf file with the world tag. Since the world is fixed, there is also no need for xacro (although you could use it if you wanted to). <?xml version=\"1.0\" ?> <sdf version=\"1.4\"> <world name=\"flightarena\"> <spherical_coordinates> <surface_model>EARTH_WGS84</surface_model> <latitude_deg>51.4233628</latitude_deg> <longitude_deg>-2.671671</longitude_deg> <elevation>100.0</elevation> </spherical_coordinates> <include> <uri>model://sun</uri> </include> ... All the contents of the world ... </world> </sdf> An example can be found in these repositories FenswoodScenario , BRLFlightArenaGazebo . The contents of the world file uses the same syntax as your normal model files. See the examples for adding primitive objects. For models, you will need to include the models inside this directory (and add that folder onto your path in the next step). Building the model ( setup.bash ) \u00b6 Similar to setup for the custom model, we need to define a setup for the custom world. For worlds, this usually just involves ensuring that the world and asscoiated models are on the gazebo paths: SRC_PATH=\"$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\" # Add flightarena.world to the Gazebo resource path export GAZEBO_RESOURCE_PATH=$SRC_PATH/worlds:${GAZEBO_RESOURCE_PATH} echo \"RESOURCE_PATH: $GAZEBO_RESOURCE_PATH\" # Add the 'grass_box' model to the Gazebo model path export GAZEBO_MODEL_PATH=$SRC_PATH/models:${GAZEBO_MODEL_PATH} echo \"GAZEBO_MODEL_PATH: $GAZEBO_MODEL_PATH\" # Make the default media files work cp -r /usr/share/gazebo-11/media $GZWEB_WS/http/client/assets/ (cd /root/gzweb/http/client/assets/media/materials/textures \\ && for f in *jpg; do convert $f ${f%.*}.png; done) The final line is to ensure that all of the textures are compatible with the Gazebo web interface. Building and Running the models \u00b6 Now that all of the models and worlds have been created, we then need to either bundle it up, or inject it into the container so it can be included. There are two methods. Both of which are applicable depending on eventual usecase. Docker-Compose: The models/worlds can be mounted into the simulation container at runtime Building a new Docker Container: If you know that you want to build upon the models or worlds into multiple differing applications, it's probably best to package it up and use as a dependency for the child projects. Docker-Compose \u00b6 As mentioned earlier, as long as the folders have a setup.bash script inside, it is compatible with automation within /ros.env.d . Therefore you can mount your folders into the container as volumes within the docker-compose file: simhost: image: uobflightlabstarling/starling-sim-iris:${STARLING_RELEASE:-latest} volumes: - ./flightarena:/ros.env.d/02_flightarena - ./systems/models/gimbal_small_2d:/ros.env.d/03_gimbal_small_2d - ./systems/models/gimbal_tripod:/ros.env.d/04_gimbal_tripod - ./system.launch.xml:launch/system.launch.xml env: - GIMBAL_NAMESPACE=gimbal_1 command: [\"ros2\", \"launch\", \"launch/system.launch.xml\"] ports: - \"8080:8080\" Where the syntax is <local file>:<container file> . Also note that inside ros.env.d we name the folders XX_<name> where XX is a two digit number describing the order of script loading from 00 being first to 99 being last. 00 to 02 are already used in setup, it is recommended you use numbers > 03. This is useful for dependencies. DockerFile \u00b6 The dockerfile is an alternative way where we explicitly copy the files into the newly built container. The following dockerfile does exactly this. ARG VERSION=latest ARG REGISTRY FROM ${REGISTRY}uobflightlabstarling/starling-sim-iris:latest # Copy in the xacro & bash file to setup the model # Using ros.env.d automatic sourcing on entrypoint (see /simulator/base/core/Dockerfile) COPY flightarena /ros.env.d/02_flightarena COPY systems/models/gimbal_small_2d /ros.env.d/03_gimbal_small_2d COPY systems/models/gimbal_tripod /ros.env.d/04_gimbal_tripod # Build gimbal plugin with ROS2 on path and add plugin to path COPY systems/gimbal_plugin /ros_ws/src/gimbal_plugin RUN cd /ros_ws \\ && . /opt/ros/foxy/setup.sh \\ && export CMAKE_PREFIX_PATH=$AMENT_PREFIX_PATH:$CMAKE_PREFIX_PATH \\ && colcon build --cmake-force-configure \\ && rm -r build \\ && echo 'export GAZEBO_PLUGIN_PATH=/ros_ws/install/gimbal_plugin/lib:${GAZEBO_PLUGIN_PATH}' >> /ros.env.d/03_gimbal_small_2d/setup.bash # Copy in the vehicle launch file COPY system.launch.xml /ros_ws/launch/ WORKDIR /ros_ws ENV GIMBAL_NAMESPACE gimbal_1 # Launch gazebo and spawn model CMD [ \"ros2\", \"launch\", \"launch/system.launch.xml\" ] Often you want to also setup the build process. See the examples for details!","title":"Spawning Different Vehicles and Objects"},{"location":"guide/different_vehicles/#different-vehicles-in-simulation","text":"With the way the system is setup, it can be quite simple to run another vehicle in simulation, this page shows two different methods of achieving this effect. Different Vehicles in Simulation Using Existing Models Example: Rover Using existing models which are not included in the container Adding your own Models into the container Background and /ros.env.d Setting up the project Custom Model Specifying the model Building the model (setup.bash) Custom World Specifying the world Building the model (setup.bash) Building and Running the models Docker-Compose DockerFile","title":"Different Vehicles in Simulation"},{"location":"guide/different_vehicles/#using-existing-models","text":"To reduce the size of the containers, they only contain the minimal number of vehicle models For PX4 SITL container, there are only two models (1) iris (2) r1_rover. For Ardupilot SITL container, not many exist ... see the next section for how to add your own. For those which already exist it is simply a case of setting the PX4_SIM_MODEL envrionment variable when you run uobflightlabstarling/starling-sim-iris in order to specify the model which will be rendered. By default this is iris uobflightlabstarling/starling-sim-px4-sitl:latest in order to ensure that the correct SITL parameters have been loaded. By default this is also iris","title":"Using Existing Models"},{"location":"guide/different_vehicles/#example-rover","text":"Example docker-compose to run the r1-rover version: '3' services: simhost: image: uobflightlabstarling/starling-sim-iris environment: - \"PX4_SIM_MODEL=r1-rover\" #new! ports: - \"8080:8080\" sitl: image: uobflightlabstarling/starling-sim-px4-sitl:latest environment: - \"PX4_SIM_HOST=simhost\" - \"PX4_OFFBOARD_HOST=mavros\" - \"PX4_SIM_MODEL=r1-rover\" # new! ports: - \"18570:18570/udp\" mavros: image: uobflightlabstarling/starling-mavros:latest command: ros2 launch launch/mavros_bridge.launch.xml environment: - \"MAVROS_TGT_SYSTEM=1\" - \"MAVROS_FCU_IP=0.0.0.0\" - \"MAVROS_GCS_URL=tcp-l://0.0.0.0:5760\" ports: - \"5760:5760\"","title":"Example: Rover"},{"location":"guide/different_vehicles/#using-existing-models-which-are-not-included-in-the-container","text":"As mentioned above, there are a number of models which should be included, but have been deleted by default to reduce the size of the container. For example for the PX4 SITL container, the following models should all exist: github link and here , but almost all of them are deleted. You can specify other vehicles to keep by rebuilding starling-sim-base-core with the build arg KEEP_PX4_VEHICLES set to the vehicles you wish to keep. To rebuild you will need to clone the ProjectStarling repository, navigate to simulation and locally rebuild the correct controller: docker build --build-args KEEP_PX4_VEHICLES=\"! -o -path iris_with_standoffs\"","title":"Using existing models which are not included in the container"},{"location":"guide/different_vehicles/#adding-your-own-models-into-the-container","text":"There will be many cases in which you will want to add extra models into your simulations. This might be to create an environment, or to add your own custom vehicles. For some exaples of projects which utilise custom models or worlds, please see the following: FenswoodScenario where a number of custom models (downloaded from the gazebo repository) and a custom world is defined. BRLFlightArenaGazebo which is comprised of a custom world and a custom gimbal-tripod model built from scratch.","title":"Adding your own Models into the container"},{"location":"guide/different_vehicles/#background-and-rosenvd","text":"Both the base simulation container , and the base controller container both use a /ros.env.d mechanism. Both containers are set-up with a special folder in the root directory called /ros.env.d . When the container is started up, there is a special script that runs (called the entrypoint script) which looks at all of the folders within /ros.env.d and whether they have their own setup.bash script. If a setup.bash script exists, it will be source 'd , i.e. script is run with any environment variables defined being exported to be used in later processes. By mounting external folders into the Dockerfile into /ros.env.d , we can add external and extra resources into a given controller or simulation container.","title":"Background and /ros.env.d"},{"location":"guide/different_vehicles/#setting-up-the-project","text":"The general format for a starling project with extra assets is the following: project/ \u251c\u2500 custom_models/ \u2502 \u251c\u2500 model_1/ \u2502 \u2502 \u251c\u2500 setup.bash \u2502 \u2502 \u251c\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500 model_1/ \u2502 \u2502 \u2502 | \u251c\u2500 meshes/ # optionally populated \u2502 \u2502 \u2502 \u2502 \u251c\u2500 model.config \u2502 \u2502 \u251c\u2500 model.sdf.xacro \u2502 \u251c\u2500 model_2/ \u2502 \u2502 \u251c\u2500 setup.bash \u2502 \u2502 \u251c\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500 model_2/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500 model.config \u2502 \u2502 \u251c\u2500 model.sdf.xacro \u251c\u2500 custom_world/ \u2502 \u251c\u2500 world/ \u2502 \u2502 \u251c\u2500 my_custom_world.world \u2502 \u251c\u2500 models/ \u2502 \u2502 \u251c\u2500 some_downloaded_model_asset/ \u2502 \u251c\u2500 setup.bash \u251c\u2500 docker-compose.yaml \u251c\u2500 system.launch.xml \u251c\u2500 Makefile \u251c\u2500 Dockerfile This project seperates custom models and the custom worlds into two seperate folders. The system.launch.xml defines the roslaunch file being run by the container. The Dockerfile will then copy all of these elements into the container at build time.","title":"Setting up the project"},{"location":"guide/different_vehicles/#custom-model","text":"","title":"Custom Model"},{"location":"guide/different_vehicles/#specifying-the-model","text":"As above, it is recommended that you have 3 items within the custom model folder. setup.bash which we will populate later model.sdf.xacro defines the model as an sdf file with xacro argument replacement, I will explain what this means next models/ folder which creates a template of the file structure required by gazebo for any model. Xacro is handy tool for specifying arguments to an sdf file in order to generate custom sdf files. This is needed as the sdf file which describes a model is usually hard coded to a particular vehicle instance. Xacro allows us to dynamically generate these files. An example model which uses the model.sdf.xacro is given here: <?xml version='1.0'?> <sdf version=\"1.6\" xmlns:xacro='http://ros.org/wiki/xacro'> <xacro:property name=\"ros_namespace\" value=\"$(arg ros_namespace)\"/> <model name=\"iris_demo\"> <link name=\"tripod\"> <collision name=\"geom_1\"> <geometry> <box><size>0.5 0.5 1.5 0 0 0</size></box> </geometry> </collision> <!-- Keep collision box, but dont show it --> <visual name=\"tripod\"> <geometry> <box><size>0.5 0.5 1.5 0 0 0</size></box> </geometry> <material> <uri>file://media/materials/scripts/gazebo.material</uri> <script>Gazebo/Grey</script> </material> </visual> </link> <include> <uri>model://gimbal_small_2d</uri> <pose>0 0 1.0 1.57 0 1.57</pose> </include> <joint name=\"${ros_namespace}_gimbal_mount\" type=\"revolute\"> <parent>tripod</parent> <child>gimbal_small_2d::base_link</child> <axis> <limit> <lower>0</lower> <upper>0</upper> </limit> <xyz>0 0 1</xyz> <use_parent_model_frame>true</use_parent_model_frame> </axis> </joint> </model> </sdf> This example describes the sdf file for a gimbal mounted on a simple tripod block. The xacro elements specify a 'property' of ros_namespace . When compiling with the xacro.py tool, it can generate a new sdf file with that property set to a value you want. You must then setup a model directory looking like the following (with optional meshes if using a model from the internet) \u2502 \u2502 \u251c\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500 model_1/ \u2502 \u2502 \u2502 | \u251c\u2500 meshes/ # optionally populated \u2502 \u2502 \u2502 \u2502 \u251c\u2500 model.config The exact contents of the model file follow gazebo guidelines. There is an overview here . There are a number online as well. Note that Xacro does not change the sdf or gazebo model syntax. It can be thought of as doing a copy paste to your existing file.","title":"Specifying the model"},{"location":"guide/different_vehicles/#building-the-model-setupbash","text":"Now we have the model defined, we now want to ensure that the model can now be built/configured/compiled during the container runtime. This involves writing the setup.bash file. Usually there are 3 steps: Making sure the script is being run from the correct directory, and that the relevant paths are set. Apply xacro with environment variables (given by the user or the container itself) and saving it in the model folder path (next to model.config ) Adding the model to the gazebo model path Here is an example setup.bash for the gimbal example. #!/bin/bash SCRIPT_DIR=\"$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\" XACRO_PATH=${SCRIPT_DIR}/model.sdf.xacro MODEL_PATH=${SCRIPT_DIR}/models/gimbal_tripod/model.sdf xacro ${XACRO_PATH} \\ ros_namespace:=${GIMBAL_NAMESPACE} \\ -o ${MODEL_PATH} echo \"Written Gimbal model to ${MODEL_PATH}\" export GAZEBO_MODEL_PATH=\"${SCRIPT_DIR}/models:${GAZEBO_MODEL_PATH}\" In particular the xacro command takes the sdf.xacro we created previously and creates a new version with the parameters realised - here ros_namespace is set to the environment variable GIMBAL_NAMESPACE . Environment variables are often defined in the Dockerfile or by the user.","title":"Building the model (setup.bash)"},{"location":"guide/different_vehicles/#custom-world","text":"","title":"Custom World"},{"location":"guide/different_vehicles/#specifying-the-world","text":"A world can be defined by the worlds folder. Inside the worlds folder, there is the world file which describes the base world gazebo will generate. The world file itself is an sdf file with the world tag. Since the world is fixed, there is also no need for xacro (although you could use it if you wanted to). <?xml version=\"1.0\" ?> <sdf version=\"1.4\"> <world name=\"flightarena\"> <spherical_coordinates> <surface_model>EARTH_WGS84</surface_model> <latitude_deg>51.4233628</latitude_deg> <longitude_deg>-2.671671</longitude_deg> <elevation>100.0</elevation> </spherical_coordinates> <include> <uri>model://sun</uri> </include> ... All the contents of the world ... </world> </sdf> An example can be found in these repositories FenswoodScenario , BRLFlightArenaGazebo . The contents of the world file uses the same syntax as your normal model files. See the examples for adding primitive objects. For models, you will need to include the models inside this directory (and add that folder onto your path in the next step).","title":"Specifying the world"},{"location":"guide/different_vehicles/#building-the-model-setupbash_1","text":"Similar to setup for the custom model, we need to define a setup for the custom world. For worlds, this usually just involves ensuring that the world and asscoiated models are on the gazebo paths: SRC_PATH=\"$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\" # Add flightarena.world to the Gazebo resource path export GAZEBO_RESOURCE_PATH=$SRC_PATH/worlds:${GAZEBO_RESOURCE_PATH} echo \"RESOURCE_PATH: $GAZEBO_RESOURCE_PATH\" # Add the 'grass_box' model to the Gazebo model path export GAZEBO_MODEL_PATH=$SRC_PATH/models:${GAZEBO_MODEL_PATH} echo \"GAZEBO_MODEL_PATH: $GAZEBO_MODEL_PATH\" # Make the default media files work cp -r /usr/share/gazebo-11/media $GZWEB_WS/http/client/assets/ (cd /root/gzweb/http/client/assets/media/materials/textures \\ && for f in *jpg; do convert $f ${f%.*}.png; done) The final line is to ensure that all of the textures are compatible with the Gazebo web interface.","title":"Building the model (setup.bash)"},{"location":"guide/different_vehicles/#building-and-running-the-models","text":"Now that all of the models and worlds have been created, we then need to either bundle it up, or inject it into the container so it can be included. There are two methods. Both of which are applicable depending on eventual usecase. Docker-Compose: The models/worlds can be mounted into the simulation container at runtime Building a new Docker Container: If you know that you want to build upon the models or worlds into multiple differing applications, it's probably best to package it up and use as a dependency for the child projects.","title":"Building and Running the models"},{"location":"guide/different_vehicles/#docker-compose","text":"As mentioned earlier, as long as the folders have a setup.bash script inside, it is compatible with automation within /ros.env.d . Therefore you can mount your folders into the container as volumes within the docker-compose file: simhost: image: uobflightlabstarling/starling-sim-iris:${STARLING_RELEASE:-latest} volumes: - ./flightarena:/ros.env.d/02_flightarena - ./systems/models/gimbal_small_2d:/ros.env.d/03_gimbal_small_2d - ./systems/models/gimbal_tripod:/ros.env.d/04_gimbal_tripod - ./system.launch.xml:launch/system.launch.xml env: - GIMBAL_NAMESPACE=gimbal_1 command: [\"ros2\", \"launch\", \"launch/system.launch.xml\"] ports: - \"8080:8080\" Where the syntax is <local file>:<container file> . Also note that inside ros.env.d we name the folders XX_<name> where XX is a two digit number describing the order of script loading from 00 being first to 99 being last. 00 to 02 are already used in setup, it is recommended you use numbers > 03. This is useful for dependencies.","title":"Docker-Compose"},{"location":"guide/different_vehicles/#dockerfile","text":"The dockerfile is an alternative way where we explicitly copy the files into the newly built container. The following dockerfile does exactly this. ARG VERSION=latest ARG REGISTRY FROM ${REGISTRY}uobflightlabstarling/starling-sim-iris:latest # Copy in the xacro & bash file to setup the model # Using ros.env.d automatic sourcing on entrypoint (see /simulator/base/core/Dockerfile) COPY flightarena /ros.env.d/02_flightarena COPY systems/models/gimbal_small_2d /ros.env.d/03_gimbal_small_2d COPY systems/models/gimbal_tripod /ros.env.d/04_gimbal_tripod # Build gimbal plugin with ROS2 on path and add plugin to path COPY systems/gimbal_plugin /ros_ws/src/gimbal_plugin RUN cd /ros_ws \\ && . /opt/ros/foxy/setup.sh \\ && export CMAKE_PREFIX_PATH=$AMENT_PREFIX_PATH:$CMAKE_PREFIX_PATH \\ && colcon build --cmake-force-configure \\ && rm -r build \\ && echo 'export GAZEBO_PLUGIN_PATH=/ros_ws/install/gimbal_plugin/lib:${GAZEBO_PLUGIN_PATH}' >> /ros.env.d/03_gimbal_small_2d/setup.bash # Copy in the vehicle launch file COPY system.launch.xml /ros_ws/launch/ WORKDIR /ros_ws ENV GIMBAL_NAMESPACE gimbal_1 # Launch gazebo and spawn model CMD [ \"ros2\", \"launch\", \"launch/system.launch.xml\" ] Often you want to also setup the build process. See the examples for details!","title":"DockerFile"},{"location":"guide/example-controller/","text":"Example Python Controller \u00b6 NEEDS UPDATING For now see the example_python_controller repository for exact commands. The theoretical content is still useful though. This is a very basic \"controller\" written in Python, using ROS2 and MAVROS to control a PX4-based vehicle. It commands all current vehicles to fly in a 1m radius circle at a height of 2.5m. It begins sending offboard setpoints, sets the mode to OFFBOARD , arms the vehicle, and finally begins moving the setpoint around the sky. Contents \u00b6 Example Python Controller Contents Usage Docker Kubernetes Details Controller Phases Initialisation Mode switching Arming Takeoff Flight Multiple Drones Coordinate Systems ArduPilot Compataility ROS Specifics Usage \u00b6 Docker \u00b6 To run the controller, it needs to be connected to the network that the other ROS nodes exist in. For example: docker run -it --network projectstarling_default example_controller_python where projectstarling_default is the name of the default network created by the example docker-compose.yaml in the root directory. Replace example_controller_python with uobflightlabstarling/example_controller_python if you have not locally built the controller. Note: Due to the way rclpy works, this container can not be killed gracefully unless using the -it flags for docker run. See this issue . Kubernetes \u00b6 To run the controller in kubernetes, k3s must be up and running, then simply apply the k8 deployment script kubectl apply -f example_controller_python/k8.example_controller_python.amd64.yaml This will firstly run the local uobflightlabstarling/example_controller_python if one exists, otherwise it will pull from docker hub. Details \u00b6 Controller Phases \u00b6 Initialisation \u00b6 In the initialisation phase, the controller sends a stream of setpoints to the vehicle while it waits for a valid position. This is needed as PX4 will not switch into OFFBOARD mode without a valid setpoint stream. In this phase, the controller also initialises the initial position of the vehicle. Mode switching \u00b6 PX4 follows various types of setpoints when in OFFBOARD mode. The controller uses a ROS service to command the vehicle to switch into this mode. It will retry the mode switch once per second until it succeeds. Once this is complete it will wait for a ros message from /mission_start before continuing on to ARM. Arming \u00b6 With the vehicle in the correct mode, the controller uses another ROS service to command the vehicle to arm. Once the vehicle is armed, the rotors begin to spin and the vehicle will begin trying to reach the setpoints it is provided with. Again the controller attempts this once per second until success. Takeoff \u00b6 Once the vehicle is armed and in OFFBOARD mode, the flight can begin. The controller uses the initial position it recorded earlier and begins to generate a setpoint that rises above this. Once the setpoint is at the desired takeoff altitude, the controller waits for the actual vehicle position to reach some predetermined value before it considers the takeoff to be complete. Flight \u00b6 With takeoff complete, the controller begins sending setpoints for the vehicle. In this case, these follow a circle around the local coordinate system origin. Note: If a message is sent on the /emergency_stop topic this controller will send the PX4 e-STOP command. This stops the motors of the drones. Multiple Drones \u00b6 If this controller is run with multiple SITL or real drone instances, each broadcasting their topics in the form <drone_name>/mavros/... , then one controller is deployed for each drone instance. By default, launch/example_fly_all.launch.py is the launch file which produces this behaviour. It provides an example for how to write an offboard multi-drone controller. Coordinate Systems \u00b6 ROS and MAVROS use Front-Left-Up (FLU) coordinate systems while PX4 uses a Front-Right-Down (FRD) coordinate system. Coordinates and vectors transferred between the two systems are automatically transformed to the appropriate convention by MAVROS. You should provide MAVROS with setpoints in FLU and expect position data from it in FLU. Another aspect to be aware of is PX4's treatment of the local coordinate system. Generally, this coordinate system will be centred where the vehicle was powered up. However, internal estimates are often poor during ground handling, so there may be a signficant offset from your expectation. Coordinating the local coordinate systems of multiple vehicles can become complex. If external local positioning information is provided, for example from Vicon, PX4's coordinate system will share the same origin. This makes dealing with multiple vehicles muc easier. PX4's global coordinate system as exposed by MAVROS comes in two forms. The first is a PoseStamped message, which is relative to either a provided map origin, the home position, or the first GNSS fix received. The other is a full geographic coordinate formed of latitude, longitude and altitude as part of a NavSatFix message. ArduPilot Compataility \u00b6 The controller should be mostly compatible with ArduPilot. The main difference is the name that ArduPilot gives to the mode needed to obey setpoints. The closest equivalent is ArduPilot's \"GUIDED\" mode. The controller has not been tested with ArduPilot yet. ROS Specifics \u00b6 There are a few ROS specifics that may look a little strange to the uninitiated. The first is the blank file under resource/ . This file is in fact quite critical. It is used by the ROS index to let it know that this package has been installed. The empty __init__.py file is there for similar mysterious reasons. The second oddity is the setup.cfg file. This is again a ROS specific configuration ensuring that things get installed into the correct places. package.xml provides ROS with information about the package. You should list any dependencies your controller has here. For almost all of the controllers, you will want to list mavros_msgs as a dependency. However, note that this package is not yet available from the package managers for ROS2. Luckily we prepared an image for you earlier with it already installed. A lot of information from here then has to be duplicated into the setup.py script. If only there was a way to automate the building of build scripts...","title":"Example Python Controller"},{"location":"guide/example-controller/#example-python-controller","text":"NEEDS UPDATING For now see the example_python_controller repository for exact commands. The theoretical content is still useful though. This is a very basic \"controller\" written in Python, using ROS2 and MAVROS to control a PX4-based vehicle. It commands all current vehicles to fly in a 1m radius circle at a height of 2.5m. It begins sending offboard setpoints, sets the mode to OFFBOARD , arms the vehicle, and finally begins moving the setpoint around the sky.","title":"Example Python Controller"},{"location":"guide/example-controller/#contents","text":"Example Python Controller Contents Usage Docker Kubernetes Details Controller Phases Initialisation Mode switching Arming Takeoff Flight Multiple Drones Coordinate Systems ArduPilot Compataility ROS Specifics","title":"Contents"},{"location":"guide/example-controller/#usage","text":"","title":"Usage"},{"location":"guide/example-controller/#docker","text":"To run the controller, it needs to be connected to the network that the other ROS nodes exist in. For example: docker run -it --network projectstarling_default example_controller_python where projectstarling_default is the name of the default network created by the example docker-compose.yaml in the root directory. Replace example_controller_python with uobflightlabstarling/example_controller_python if you have not locally built the controller. Note: Due to the way rclpy works, this container can not be killed gracefully unless using the -it flags for docker run. See this issue .","title":"Docker"},{"location":"guide/example-controller/#kubernetes","text":"To run the controller in kubernetes, k3s must be up and running, then simply apply the k8 deployment script kubectl apply -f example_controller_python/k8.example_controller_python.amd64.yaml This will firstly run the local uobflightlabstarling/example_controller_python if one exists, otherwise it will pull from docker hub.","title":"Kubernetes"},{"location":"guide/example-controller/#details","text":"","title":"Details"},{"location":"guide/example-controller/#controller-phases","text":"","title":"Controller Phases"},{"location":"guide/example-controller/#initialisation","text":"In the initialisation phase, the controller sends a stream of setpoints to the vehicle while it waits for a valid position. This is needed as PX4 will not switch into OFFBOARD mode without a valid setpoint stream. In this phase, the controller also initialises the initial position of the vehicle.","title":"Initialisation"},{"location":"guide/example-controller/#mode-switching","text":"PX4 follows various types of setpoints when in OFFBOARD mode. The controller uses a ROS service to command the vehicle to switch into this mode. It will retry the mode switch once per second until it succeeds. Once this is complete it will wait for a ros message from /mission_start before continuing on to ARM.","title":"Mode switching"},{"location":"guide/example-controller/#arming","text":"With the vehicle in the correct mode, the controller uses another ROS service to command the vehicle to arm. Once the vehicle is armed, the rotors begin to spin and the vehicle will begin trying to reach the setpoints it is provided with. Again the controller attempts this once per second until success.","title":"Arming"},{"location":"guide/example-controller/#takeoff","text":"Once the vehicle is armed and in OFFBOARD mode, the flight can begin. The controller uses the initial position it recorded earlier and begins to generate a setpoint that rises above this. Once the setpoint is at the desired takeoff altitude, the controller waits for the actual vehicle position to reach some predetermined value before it considers the takeoff to be complete.","title":"Takeoff"},{"location":"guide/example-controller/#flight","text":"With takeoff complete, the controller begins sending setpoints for the vehicle. In this case, these follow a circle around the local coordinate system origin. Note: If a message is sent on the /emergency_stop topic this controller will send the PX4 e-STOP command. This stops the motors of the drones.","title":"Flight"},{"location":"guide/example-controller/#multiple-drones","text":"If this controller is run with multiple SITL or real drone instances, each broadcasting their topics in the form <drone_name>/mavros/... , then one controller is deployed for each drone instance. By default, launch/example_fly_all.launch.py is the launch file which produces this behaviour. It provides an example for how to write an offboard multi-drone controller.","title":"Multiple Drones"},{"location":"guide/example-controller/#coordinate-systems","text":"ROS and MAVROS use Front-Left-Up (FLU) coordinate systems while PX4 uses a Front-Right-Down (FRD) coordinate system. Coordinates and vectors transferred between the two systems are automatically transformed to the appropriate convention by MAVROS. You should provide MAVROS with setpoints in FLU and expect position data from it in FLU. Another aspect to be aware of is PX4's treatment of the local coordinate system. Generally, this coordinate system will be centred where the vehicle was powered up. However, internal estimates are often poor during ground handling, so there may be a signficant offset from your expectation. Coordinating the local coordinate systems of multiple vehicles can become complex. If external local positioning information is provided, for example from Vicon, PX4's coordinate system will share the same origin. This makes dealing with multiple vehicles muc easier. PX4's global coordinate system as exposed by MAVROS comes in two forms. The first is a PoseStamped message, which is relative to either a provided map origin, the home position, or the first GNSS fix received. The other is a full geographic coordinate formed of latitude, longitude and altitude as part of a NavSatFix message.","title":"Coordinate Systems"},{"location":"guide/example-controller/#ardupilot-compataility","text":"The controller should be mostly compatible with ArduPilot. The main difference is the name that ArduPilot gives to the mode needed to obey setpoints. The closest equivalent is ArduPilot's \"GUIDED\" mode. The controller has not been tested with ArduPilot yet.","title":"ArduPilot Compataility"},{"location":"guide/example-controller/#ros-specifics","text":"There are a few ROS specifics that may look a little strange to the uninitiated. The first is the blank file under resource/ . This file is in fact quite critical. It is used by the ROS index to let it know that this package has been installed. The empty __init__.py file is there for similar mysterious reasons. The second oddity is the setup.cfg file. This is again a ROS specific configuration ensuring that things get installed into the correct places. package.xml provides ROS with information about the package. You should list any dependencies your controller has here. For almost all of the controllers, you will want to list mavros_msgs as a dependency. However, note that this package is not yet available from the package managers for ROS2. Luckily we prepared an image for you earlier with it already installed. A lot of information from here then has to be duplicated into the setup.py script. If only there was a way to automate the building of build scripts...","title":"ROS Specifics"},{"location":"guide/getting-started/","text":"Getting Started with Starling \u00b6 Contents \u00b6 Getting Started with Starling Contents Install Docker Linux/ Ubuntu Install Script Manual Installation Windows Mac OS Get Starling Using Starling Single Drone Applications Multiple Drone Applications FAQs Install Docker \u00b6 The only dependency of running and using Starling locally is the [Docker] containerisation software. Please visit docker's installation page for further details. We recommend using Starling on linux based systems, but it should work on Windows and possible Mac depending on the state of the Docker. Linux/ Ubuntu \u00b6 Install Script \u00b6 For linux, we have provided a helpful install script. First you will need to clone the Murmuration github repository: git clone https://github.com/StarlingUAS/Murmuration.git Then run: sudo ./starling install Once that completes you should be all good to go for the next step. Manual Installation \u00b6 For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (be aware of issues with this ) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes (may need to restart) Verify that it was successful: docker run hello-world That is Docker on Linux installed. See the original page for any further details. Windows \u00b6 For Windows systems, please see the windows installation instructions for full details . You will need to install Docker Desktop for Windows. Follow the instructions and tutorial below, but refer to our Windows Support page if you have issues. Starling has been fully tested on Windows, and both single and multi-agent cluster testing is available through the use of the Starling CLI. However it is recommended that you use WSL rather than powershell. Mac OS \u00b6 For Mav systems, please see the Mac Os installation instructions for full details. You will need to install Docker Desktop for MAC OS. Starling has not be tested on MAC OS, so there is no guarantee that it is functional. However in theory the single-agent non-cluster (docker-only) application should be compatible. Get Starling \u00b6 For simple usage, or for usage as a simulator, you can use the Murmuration repository which gives a number of starling examples and a command line interface. git clone https://github.com/StarlingUAS/Murmuration.git For more advanced usage, or to see examples of the core elements, clone the Starling repository: git clone https://github.com/StarlingUAS/ProjectStarling.git Using Starling \u00b6 Starling provides a framework for users to implement almost any drone related experiment or application. However, we have identified that most applications fall into the following categories. Therefore we have provided detailed user guides for these use cases. It is recommended that the user follow these guides to run starling before starting your own implementation as these guides will also slowly introduce you to the concepts and details Starling uses. These user guides are all located on the side navigation bar for easy access. A key idea is the fact that the controller you develop, whether onboard or offboard, will be identical regardless of which step you are in. This therefore designs a workflow for the development of controllers. Single Drone Applications \u00b6 For single drone applications, there are 3 steps to the development of a controller running onboard (on the 'drone' itself) or offboard (on a separate machine): Step 1: Single Drone testing on local machine using Docker only Follow the following user guide Simplest deployment method and the quickest way to get a simulation (gazebo) and drone (px4, mavros) instance running. Quick and easy way to test the basic functionality of your controller without worrying about too may other factors. Step 2: Single Drone testing on local cluster Follow the following user guide Once you have your controller mostly working and debugged, the next step is to test it within the network and system architecture used when flying real drones. This will require learning a little bit about the cluster architecture and how Starling works under the hood. Step 3: Single Drone flying at the Robotics Laboratory Follow the following user guide Detailing how to run your controller on real drones at the Bristol Robotics Laboratory Multiple Drone Applications \u00b6 For multiple drone applications, there are also 3 steps to the development of a centralised or decentralised, onboard or offboard controller. Step 1: Single Drone testing on local machine using Docker only Follow the following user guide Simplest deployment method and the quickest way to get a single simulation (gazebo) and drone (px4, mavros) instance running. Quick and easy way to test the basic functionality of your controller without worrying about too may other factors. Can be used to quickly protype controllers before testing with multiple vehicles. Step 2: Multiple Drone testing on local cluster Follow the following user guide The cluster facilitates the multi-agent capability of starling. The guide takes you through the basic concepts required for running multi-agent applications Step 3: Multiple Drones flying at the Robotics Laboratory Follow the following user guide Detailing how to run your controller on real drones at the Bristol Robotics Laboratory FAQs \u00b6","title":"Getting Started"},{"location":"guide/getting-started/#getting-started-with-starling","text":"","title":"Getting Started with Starling"},{"location":"guide/getting-started/#contents","text":"Getting Started with Starling Contents Install Docker Linux/ Ubuntu Install Script Manual Installation Windows Mac OS Get Starling Using Starling Single Drone Applications Multiple Drone Applications FAQs","title":"Contents"},{"location":"guide/getting-started/#install-docker","text":"The only dependency of running and using Starling locally is the [Docker] containerisation software. Please visit docker's installation page for further details. We recommend using Starling on linux based systems, but it should work on Windows and possible Mac depending on the state of the Docker.","title":"Install Docker"},{"location":"guide/getting-started/#linux-ubuntu","text":"","title":"Linux/ Ubuntu"},{"location":"guide/getting-started/#install-script","text":"For linux, we have provided a helpful install script. First you will need to clone the Murmuration github repository: git clone https://github.com/StarlingUAS/Murmuration.git Then run: sudo ./starling install Once that completes you should be all good to go for the next step.","title":"Install Script"},{"location":"guide/getting-started/#manual-installation","text":"For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (be aware of issues with this ) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes (may need to restart) Verify that it was successful: docker run hello-world That is Docker on Linux installed. See the original page for any further details.","title":"Manual Installation"},{"location":"guide/getting-started/#windows","text":"For Windows systems, please see the windows installation instructions for full details . You will need to install Docker Desktop for Windows. Follow the instructions and tutorial below, but refer to our Windows Support page if you have issues. Starling has been fully tested on Windows, and both single and multi-agent cluster testing is available through the use of the Starling CLI. However it is recommended that you use WSL rather than powershell.","title":"Windows"},{"location":"guide/getting-started/#mac-os","text":"For Mav systems, please see the Mac Os installation instructions for full details. You will need to install Docker Desktop for MAC OS. Starling has not be tested on MAC OS, so there is no guarantee that it is functional. However in theory the single-agent non-cluster (docker-only) application should be compatible.","title":"Mac OS"},{"location":"guide/getting-started/#get-starling","text":"For simple usage, or for usage as a simulator, you can use the Murmuration repository which gives a number of starling examples and a command line interface. git clone https://github.com/StarlingUAS/Murmuration.git For more advanced usage, or to see examples of the core elements, clone the Starling repository: git clone https://github.com/StarlingUAS/ProjectStarling.git","title":"Get Starling"},{"location":"guide/getting-started/#using-starling","text":"Starling provides a framework for users to implement almost any drone related experiment or application. However, we have identified that most applications fall into the following categories. Therefore we have provided detailed user guides for these use cases. It is recommended that the user follow these guides to run starling before starting your own implementation as these guides will also slowly introduce you to the concepts and details Starling uses. These user guides are all located on the side navigation bar for easy access. A key idea is the fact that the controller you develop, whether onboard or offboard, will be identical regardless of which step you are in. This therefore designs a workflow for the development of controllers.","title":"Using Starling"},{"location":"guide/getting-started/#single-drone-applications","text":"For single drone applications, there are 3 steps to the development of a controller running onboard (on the 'drone' itself) or offboard (on a separate machine): Step 1: Single Drone testing on local machine using Docker only Follow the following user guide Simplest deployment method and the quickest way to get a simulation (gazebo) and drone (px4, mavros) instance running. Quick and easy way to test the basic functionality of your controller without worrying about too may other factors. Step 2: Single Drone testing on local cluster Follow the following user guide Once you have your controller mostly working and debugged, the next step is to test it within the network and system architecture used when flying real drones. This will require learning a little bit about the cluster architecture and how Starling works under the hood. Step 3: Single Drone flying at the Robotics Laboratory Follow the following user guide Detailing how to run your controller on real drones at the Bristol Robotics Laboratory","title":"Single Drone Applications"},{"location":"guide/getting-started/#multiple-drone-applications","text":"For multiple drone applications, there are also 3 steps to the development of a centralised or decentralised, onboard or offboard controller. Step 1: Single Drone testing on local machine using Docker only Follow the following user guide Simplest deployment method and the quickest way to get a single simulation (gazebo) and drone (px4, mavros) instance running. Quick and easy way to test the basic functionality of your controller without worrying about too may other factors. Can be used to quickly protype controllers before testing with multiple vehicles. Step 2: Multiple Drone testing on local cluster Follow the following user guide The cluster facilitates the multi-agent capability of starling. The guide takes you through the basic concepts required for running multi-agent applications Step 3: Multiple Drones flying at the Robotics Laboratory Follow the following user guide Detailing how to run your controller on real drones at the Bristol Robotics Laboratory","title":"Multiple Drone Applications"},{"location":"guide/getting-started/#faqs","text":"","title":"FAQs"},{"location":"guide/kube-single-drone-local-machine/","text":"Single Drone on Local Machine Using Cluster Architecture \u00b6 Follow these instructions for quick and easy testing of controllers on a single drone on a single local machine. Use in the following scenarios : Local development of single (or multiple) drone applications in simulation (gazebo) Local development of offboard (drone control software on pc not on drone) Local development of distributed onboard (drone control software running on drone itself) Testing controllers/software on the real drone software/ communications architecture that would be used in the BRL. This is considered to be step 2a for the Starling development process. Note: Reading the background may be useful but not necessary. Contents \u00b6 Single Drone on Local Machine Using Cluster Architecture Contents Drone and Simulator on a local cluster Starting the cluster Restarting or deleting the drone or simulator in the cluster Accessing logs on the dashboard Controlling the Drones Offboard Control 1. Connecting a Ground Control Station via Mavlink 2. Running Example ROS2 Offboard Controller node Onboard Control Development on the Drone and Simulator Useful Scripts: Modifying the example controller Creating your own from scratch Troubleshooting/ FAQs Drone and Simulator on a local cluster \u00b6 First check that you have installed the single prerequisit of docker , see Getting Started Starting the cluster \u00b6 In the root directory run one of the following in a terminal ./run_k3s.sh ./run_k3s.sh -ow # Will automatically open the UI webpages for you. This will start the following: Install the cluster root node which governs the running of all the other parts. A node running the Gazebo simulation environment A node running the following: An initialisation routine spawning 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A MAVROS node connected to the SITL instance A simple UI with a go and estop button. The cluster control dashboard (printing out the access key string) Note: this might take a while on first run as downloads are required. Note: this installation will ask for root (sudo) permission when necessary. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Go to http://localhost:3000 in a browser to see the starling user interface containing go/stop buttons. Go to http://localhost:31771 in a browser to see the cluster dashboard. There is a lot here, and this guide will point you to the key functions. Please see this page for further details. Your browser of choice may not like the web-page and complain of certificate errors. Please ignore this and continue onwards. You may have to click 'advanced' or a similar option for the browser to let you in. To log in to the site, you will need the long login Token which is hopefully displayed by run_k3s.sh . This token should also be automatically placed onto your clipboard for pasting. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane. Restarting or deleting the drone or simulator in the cluster \u00b6 There may be cases where you wish to restart or refresh either the software running on the drones or the simulator itself (e.g. removing old models): ./run_k3s.sh -d # or --delete will remove the gazebo and drone instances ./run_k3s.sh -r # or --restart will restart the gazebo and drone instances Note: you can also add the -sk command which will skip the k3s re-download step and the dashboard check, i.e. ./run_k3s.sh -sk -r If you wish to remove the cluster and all associated software from the machine, you will need to uninstall: ./run_k3s.sh --uninstall Note: This will remove everything to do with the starling cluster. The dashboard access token will be deleted. The container images will remain on your machine, but to remove those as well run docker system prune --volumes . Accessing logs on the dashboard \u00b6 Please see the instructions here Controlling the Drones \u00b6 Offboard Control \u00b6 There are two supported methods for offboard control of either the SITL or real drones. Control drone directly via Mavlink, by Ground Control Station (GCS) or other Mavlink compatible method (e.g. Dronekit). Control drone via ROS2 node 1. Connecting a Ground Control Station via Mavlink \u00b6 If a mavros or sitl instance is running, there will be a GCS link on udp://localhost:14553 (hopefully). This means that you can run a GCS such as QGroundControl or Mission Planner: Create a comms link to localhost:14553 on UDP The GCS should auto detect the drone(s) You should be able to control and monitor any sitl drone through the standard mavlink interface. This is a quick an easy way to control the SITL instance via Mavlink. 2. Running Example ROS2 Offboard Controller node \u00b6 An example offboard ROS2 controller can then be connected to SITL by running the following in a terminal: ./scripts/start_example_controller.sh This will first build the example controller so it is available locally. Then deploy the example controller to the cluster. It will take a few minutes to startup. When run, the example will confirm in the terminal that it has connected and that it is waiting for mission start. To start the mission, press the green go button in the starling user interface which will send a message over /mission_start topic. A confirmation message should appear in the terminal, and the drone will arm (drone propellors spinning up) and takeoff. It will fly in circles for 10 seconds before landing and disarming. Once the controller has completed, the process will exit and the controller will restart, allowing you to repeat the controller. If used with multiple vehicles, it will automatically find all drones broadcasting mavros topics, and start a controller for each one. To remove or restart the controller, use the -d or -r options respectively with the script. ./scripts/start_example_controller.sh -d # Delete or Remove controller ./scripts/start_example_controller.sh -r # Restart controller Onboard Control \u00b6 TODO: Implement example onboard controller and associated scripts Development on the Drone and Simulator \u00b6 Useful Scripts: \u00b6 There are a number of useful scripts in the /scripts directory of this repository. Scripts can be run from any location, but for this tutorial we assume the user is in the root directory. ./scripts/start_k3s.sh - This starts the cluster -u will uninstall the cluster from the machine (and remove any running processes) -sk will skip the installation check for k3s ./scripts/start_single_px4sitl_gazebo.sh - This starts the starling core functions (Dashboard and UI). It also starts the gazebo simulator and a 'drone' process running the PX4 SITL and a connected Mavros ROS2 node. Assumes the cluster has been started. -d will stop the gazebo and all connected 'drone' processes only (use if reloading the controller). -r will restart the gazebo and all connected 'drone' processes. labels: app: nginx -sk will skip the starting/check that the starling core functions are running. -ow will automatically open the UI webpages. ./scripts/start_starling_base.sh - This starts the starling user interface and dashboard. Automatically run by start_single_px4sitl_gazebo.sh ./scripts/start_dashboard.sh - This starts the dashboard process on the cluster. Assumes the cluster has already been started. Automatically run by start_single_px4sitl_gazebo.sh and start_starling_base.sh -ow will automatically open the UI webpages. Note: The ./run_k3s script internally runs start_k3s.sh and start_single_px4sitl_gazebo.sh . Any options passed will be forwarded to the relevant script. Modifying the example controller \u00b6 In the controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package can be built by running make in the controllers directory. This will use colcon build to build the node and output a local image named example_controller_python . Inside the controllers folder, there is an annotated kubernetes config file k8.example_controller_python.amd64.yaml . This specifies the deployment of a pod which contains your local example_controller_python image ( this line ). Similar to before you can start up the local example controller by using the script: ./scripts/start_example_controller.sh But if you have made a copy, or wish to run your own version of the configuration, you can manually deploy your controller by running the following: k3s apply -f k8.example_controller_python.amd64.yaml k3s delete -f k8.example_controller_python.amd64.yaml # To delete See kubernetes configuration for more details. For debugging, you can both see the logs, and execute on your controller container through the dashboard. See instructions here Inside you can source install/setup.bash and run ROS2 commands like normal. Creating your own from scratch \u00b6 Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. 3. A Kubernetes YAML config file specifying either a Pod or a deployment. Use the example k8.example_controller_python.amd64.yaml as a template. There are annoated comments. Also see here for more details. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> . Once built, the configuration file must have a container config specifying your own image name. Your container can then be deployed manually using kubectl -f <your config>.yaml as above. Troubleshooting/ FAQs \u00b6","title":"Single Drone Simulation on Local Cluster"},{"location":"guide/kube-single-drone-local-machine/#single-drone-on-local-machine-using-cluster-architecture","text":"Follow these instructions for quick and easy testing of controllers on a single drone on a single local machine. Use in the following scenarios : Local development of single (or multiple) drone applications in simulation (gazebo) Local development of offboard (drone control software on pc not on drone) Local development of distributed onboard (drone control software running on drone itself) Testing controllers/software on the real drone software/ communications architecture that would be used in the BRL. This is considered to be step 2a for the Starling development process. Note: Reading the background may be useful but not necessary.","title":"Single Drone on Local Machine Using Cluster Architecture"},{"location":"guide/kube-single-drone-local-machine/#contents","text":"Single Drone on Local Machine Using Cluster Architecture Contents Drone and Simulator on a local cluster Starting the cluster Restarting or deleting the drone or simulator in the cluster Accessing logs on the dashboard Controlling the Drones Offboard Control 1. Connecting a Ground Control Station via Mavlink 2. Running Example ROS2 Offboard Controller node Onboard Control Development on the Drone and Simulator Useful Scripts: Modifying the example controller Creating your own from scratch Troubleshooting/ FAQs","title":"Contents"},{"location":"guide/kube-single-drone-local-machine/#drone-and-simulator-on-a-local-cluster","text":"First check that you have installed the single prerequisit of docker , see Getting Started","title":"Drone and Simulator on a local cluster"},{"location":"guide/kube-single-drone-local-machine/#starting-the-cluster","text":"In the root directory run one of the following in a terminal ./run_k3s.sh ./run_k3s.sh -ow # Will automatically open the UI webpages for you. This will start the following: Install the cluster root node which governs the running of all the other parts. A node running the Gazebo simulation environment A node running the following: An initialisation routine spawning 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A MAVROS node connected to the SITL instance A simple UI with a go and estop button. The cluster control dashboard (printing out the access key string) Note: this might take a while on first run as downloads are required. Note: this installation will ask for root (sudo) permission when necessary. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Go to http://localhost:3000 in a browser to see the starling user interface containing go/stop buttons. Go to http://localhost:31771 in a browser to see the cluster dashboard. There is a lot here, and this guide will point you to the key functions. Please see this page for further details. Your browser of choice may not like the web-page and complain of certificate errors. Please ignore this and continue onwards. You may have to click 'advanced' or a similar option for the browser to let you in. To log in to the site, you will need the long login Token which is hopefully displayed by run_k3s.sh . This token should also be automatically placed onto your clipboard for pasting. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane.","title":"Starting the cluster"},{"location":"guide/kube-single-drone-local-machine/#restarting-or-deleting-the-drone-or-simulator-in-the-cluster","text":"There may be cases where you wish to restart or refresh either the software running on the drones or the simulator itself (e.g. removing old models): ./run_k3s.sh -d # or --delete will remove the gazebo and drone instances ./run_k3s.sh -r # or --restart will restart the gazebo and drone instances Note: you can also add the -sk command which will skip the k3s re-download step and the dashboard check, i.e. ./run_k3s.sh -sk -r If you wish to remove the cluster and all associated software from the machine, you will need to uninstall: ./run_k3s.sh --uninstall Note: This will remove everything to do with the starling cluster. The dashboard access token will be deleted. The container images will remain on your machine, but to remove those as well run docker system prune --volumes .","title":"Restarting or deleting the drone or simulator in the cluster"},{"location":"guide/kube-single-drone-local-machine/#accessing-logs-on-the-dashboard","text":"Please see the instructions here","title":"Accessing logs on the dashboard"},{"location":"guide/kube-single-drone-local-machine/#controlling-the-drones","text":"","title":"Controlling the Drones"},{"location":"guide/kube-single-drone-local-machine/#offboard-control","text":"There are two supported methods for offboard control of either the SITL or real drones. Control drone directly via Mavlink, by Ground Control Station (GCS) or other Mavlink compatible method (e.g. Dronekit). Control drone via ROS2 node","title":"Offboard Control"},{"location":"guide/kube-single-drone-local-machine/#1-connecting-a-ground-control-station-via-mavlink","text":"If a mavros or sitl instance is running, there will be a GCS link on udp://localhost:14553 (hopefully). This means that you can run a GCS such as QGroundControl or Mission Planner: Create a comms link to localhost:14553 on UDP The GCS should auto detect the drone(s) You should be able to control and monitor any sitl drone through the standard mavlink interface. This is a quick an easy way to control the SITL instance via Mavlink.","title":"1. Connecting a Ground Control Station via Mavlink"},{"location":"guide/kube-single-drone-local-machine/#2-running-example-ros2-offboard-controller-node","text":"An example offboard ROS2 controller can then be connected to SITL by running the following in a terminal: ./scripts/start_example_controller.sh This will first build the example controller so it is available locally. Then deploy the example controller to the cluster. It will take a few minutes to startup. When run, the example will confirm in the terminal that it has connected and that it is waiting for mission start. To start the mission, press the green go button in the starling user interface which will send a message over /mission_start topic. A confirmation message should appear in the terminal, and the drone will arm (drone propellors spinning up) and takeoff. It will fly in circles for 10 seconds before landing and disarming. Once the controller has completed, the process will exit and the controller will restart, allowing you to repeat the controller. If used with multiple vehicles, it will automatically find all drones broadcasting mavros topics, and start a controller for each one. To remove or restart the controller, use the -d or -r options respectively with the script. ./scripts/start_example_controller.sh -d # Delete or Remove controller ./scripts/start_example_controller.sh -r # Restart controller","title":"2. Running Example ROS2 Offboard Controller node"},{"location":"guide/kube-single-drone-local-machine/#onboard-control","text":"TODO: Implement example onboard controller and associated scripts","title":"Onboard Control"},{"location":"guide/kube-single-drone-local-machine/#development-on-the-drone-and-simulator","text":"","title":"Development on the Drone and Simulator"},{"location":"guide/kube-single-drone-local-machine/#useful-scripts","text":"There are a number of useful scripts in the /scripts directory of this repository. Scripts can be run from any location, but for this tutorial we assume the user is in the root directory. ./scripts/start_k3s.sh - This starts the cluster -u will uninstall the cluster from the machine (and remove any running processes) -sk will skip the installation check for k3s ./scripts/start_single_px4sitl_gazebo.sh - This starts the starling core functions (Dashboard and UI). It also starts the gazebo simulator and a 'drone' process running the PX4 SITL and a connected Mavros ROS2 node. Assumes the cluster has been started. -d will stop the gazebo and all connected 'drone' processes only (use if reloading the controller). -r will restart the gazebo and all connected 'drone' processes. labels: app: nginx -sk will skip the starting/check that the starling core functions are running. -ow will automatically open the UI webpages. ./scripts/start_starling_base.sh - This starts the starling user interface and dashboard. Automatically run by start_single_px4sitl_gazebo.sh ./scripts/start_dashboard.sh - This starts the dashboard process on the cluster. Assumes the cluster has already been started. Automatically run by start_single_px4sitl_gazebo.sh and start_starling_base.sh -ow will automatically open the UI webpages. Note: The ./run_k3s script internally runs start_k3s.sh and start_single_px4sitl_gazebo.sh . Any options passed will be forwarded to the relevant script.","title":"Useful Scripts:"},{"location":"guide/kube-single-drone-local-machine/#modifying-the-example-controller","text":"In the controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package can be built by running make in the controllers directory. This will use colcon build to build the node and output a local image named example_controller_python . Inside the controllers folder, there is an annotated kubernetes config file k8.example_controller_python.amd64.yaml . This specifies the deployment of a pod which contains your local example_controller_python image ( this line ). Similar to before you can start up the local example controller by using the script: ./scripts/start_example_controller.sh But if you have made a copy, or wish to run your own version of the configuration, you can manually deploy your controller by running the following: k3s apply -f k8.example_controller_python.amd64.yaml k3s delete -f k8.example_controller_python.amd64.yaml # To delete See kubernetes configuration for more details. For debugging, you can both see the logs, and execute on your controller container through the dashboard. See instructions here Inside you can source install/setup.bash and run ROS2 commands like normal.","title":"Modifying the example controller"},{"location":"guide/kube-single-drone-local-machine/#creating-your-own-from-scratch","text":"Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. 3. A Kubernetes YAML config file specifying either a Pod or a deployment. Use the example k8.example_controller_python.amd64.yaml as a template. There are annoated comments. Also see here for more details. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> . Once built, the configuration file must have a container config specifying your own image name. Your container can then be deployed manually using kubectl -f <your config>.yaml as above.","title":"Creating your own from scratch"},{"location":"guide/kube-single-drone-local-machine/#troubleshooting-faqs","text":"","title":"Troubleshooting/ FAQs"},{"location":"guide/multiple-drone-drones/","text":"Multiple Drones Deployed onto Real Drones in the Flight Arena \u00b6 Multiple Drones Deployed onto Real Drones in the Flight Arena TODO TODO \u00b6","title":"Multiple Drones on Hardware"},{"location":"guide/multiple-drone-drones/#multiple-drones-deployed-onto-real-drones-in-the-flight-arena","text":"Multiple Drones Deployed onto Real Drones in the Flight Arena TODO","title":"Multiple Drones Deployed onto Real Drones in the Flight Arena"},{"location":"guide/multiple-drone-drones/#todo","text":"","title":"TODO"},{"location":"guide/multiple-drone-local-machine/","text":"Multiple Drones Local Machine \u00b6 Follow these instructions for quick and easy testing of controllers on multiple simulated drones on a single local machine. Use in the following scenarios : Local development of multiple drone applications in simulation (gazebo) Local development of offboard control (drone control software on pc not on drone) Local development of distributed onboard control (drone control software running on drone itself) Testing controllers/software on the real drone software/ communications architecture that would be used in the BRL. This is considered to be step 2b for the Starling development process. Note: Reading the background may be useful but not necessary. Contents \u00b6 Multiple Drones Local Machine Contents Drone and Simulator on a local cluster Starting the cluster Restarting or deleting the drone or simulator in the cluster Accessing logs on the dashboard Scaling the number of drones Dashboard Configuration file Controlling the Drones Offboard Control 1. Connecting a Ground Control Station via Mavlink 2. Running Example ROS2 Offboard Controller node Onboard Control Development on the Drone and Simulator Useful Scripts: Modifying the example controller Creating your own from scratch Troubleshooting/ FAQs Drone and Simulator on a local cluster \u00b6 First check that you have installed the single prerequisit of docker , see Getting Started Starting the cluster \u00b6 In the root directory run one of the following in a terminal ./run_k3s.sh ./run_k3s.sh -ow # Will automatically open the UI webpages for you. This will start the following: Install the cluster root node which governs the running of all the other parts. A node running the Gazebo simulation environment A node running the following: An initialisation routine spawning 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A MAVROS node connected to the SITL instance A simple UI with a go and estop button. The cluster control dashboard (printing out the access key string) Note: this might take a while on first run as downloads are required. Note: this installation will ask for root (sudo) permission when necessary. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Go to http://localhost:3000 in a browser to see the starling user interface containing go/stop buttons. Go to http://localhost:31771 in a browser to see the cluster dashboard. There is a lot here, and this guide will point you to the key functions. Please see this page for further details. Your browser of choice may not like the web-page and complain of certificate errors. Please ignore this and continue onwards. You may have to click 'advanced' or a similar option for the browser to let you in. To log in to the site, you will need the long login Token which is hopefully displayed by run_k3s.sh . This token should also be automatically placed onto your clipboard for pasting. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane. Restarting or deleting the drone or simulator in the cluster \u00b6 There may be cases where you wish to restart or refresh either the software running on the drones or the simulator itself (e.g. removing old models): ./run_k3s.sh -d # or --delete will remove the gazebo and drone instances ./run_k3s.sh -r # or --restart will restart the gazebo and drone instances Note: you can also add the -sk command which will skip the k3s re-download step and the dashboard check, i.e. ./run_k3s.sh -sk -r If you wish to remove the cluster and all associated software from the machine, you will need to uninstall: ./run_k3s.sh --uninstall Note: This will remove everything to do with the starling cluster. The dashboard access token will be deleted. The container images will remain on your machine, but to remove those as well run docker system prune --volumes . Accessing logs on the dashboard \u00b6 Please see the instructions here Scaling the number of drones \u00b6 Dashboard \u00b6 The simplest way to scale the number of drones is via the dashboard as shown below: The steps are as follows: Select the 'Stateful Sets' page on the sidebar There should be an entry called 'starling-px4-sitl' representing the drone deployment. On the right hand side, there is an option button: \u2630. Press the option button and select Scale In the Desired replicas field, input the number of drones you wish to deploy. Then click Save to deploy. Going to gazebo, the drones should start appearing. The default drone arrangement is in a discrete spiral from (0, 0). Note: Right now, drones cannot be removed or despawned from the simulator. (Gazebo ROS2 despawning issues). If you have spawned too many, it is currently recommended that you restart gazebo and all the drones using ./script/start_px4sitl_gazebo.sh -r . Configuration file \u00b6 A more permanent way is to specify in the cluster deployment configuration file. Please see this guide to kubernetes configuraiton files for more information. Within a kubernetes configuration of kind Deployment or StatefulSet specifcation you can set the 'replicas' option. The example configurations are all in the deployment folder of the repository. apiVersion: apps/v1 kind: Deployment metadata: ... spec: replicas: 3 ... Controlling the Drones \u00b6 Offboard Control \u00b6 There are two supported methods for offboard control of either the SITL or real drones. Control drone directly via Mavlink, by Ground Control Station (GCS) or other Mavlink compatible method (e.g. Dronekit). Control drone via ROS2 node 1. Connecting a Ground Control Station via Mavlink \u00b6 If a mavros or sitl instance is running, there will be a GCS link on udp://localhost:14553 (hopefully). This means that you can run a GCS such as QGroundControl or Mission Planner: Create a comms link to localhost:14553 on UDP The GCS should auto detect the drone(s) You should be able to control and monitor any sitl drone through the standard mavlink interface. This is a quick an easy way to control the SITL instance via Mavlink. 2. Running Example ROS2 Offboard Controller node \u00b6 An example offboard ROS2 controller can then be connected to SITL by running the following in a terminal: ./scripts/start_example_controller.sh This will first build the example controller so it is available locally. Then deploy the example controller to the cluster. It will take a few minutes to startup. When run, the example will confirm in the terminal that it has connected and that it is waiting for mission start. To start the mission, press the green go button in the starling user interface which will send a message over /mission_start topic. A confirmation message should appear in the terminal, and the drone will arm (drone propellors spinning up) and takeoff. It will fly in circles for 10 seconds before landing and disarming. Once the controller has completed, the process will exit and the controller will restart, allowing you to repeat the controller. If used with multiple vehicles, it will automatically find all drones broadcasting mavros topics, and start a controller for each one. To remove or restart the controller, use the -d or -r options respectively with the script. ./scripts/start_example_controller.sh -d # Delete or Remove controller ./scripts/start_example_controller.sh -r # Restart controller See example python controller for more details. Onboard Control \u00b6 TODO: Implement example onboard controller and associated scripts Development on the Drone and Simulator \u00b6 Useful Scripts: \u00b6 There are a number of useful scripts in the /scripts directory of this repository. Scripts can be run from any location, but for this tutorial we assume the user is in the root directory. ./scripts/start_k3s.sh - This starts the cluster -u will uninstall the cluster from the machine (and remove any running processes) -sk will skip the installation check for k3s ./scripts/start_single_px4sitl_gazebo.sh - This starts the starling core functions (Dashboard and UI). It also starts the gazebo simulator and a 'drone' process running the PX4 SITL and a connected Mavros ROS2 node. Assumes the cluster has been started. -d will stop the gazebo and all connected 'drone' processes only (use if reloading the controller). -r will restart the gazebo and all connected 'drone' processes. labels: app: nginx -sk will skip the starting/check that the starling core functions are running. -ow will automatically open the UI webpages. ./scripts/start_starling_base.sh - This starts the starling user interface and dashboard. Automatically run by start_single_px4sitl_gazebo.sh ./scripts/start_dashboard.sh - This starts the dashboard process on the cluster. Assumes the cluster has already been started. Automatically run by start_single_px4sitl_gazebo.sh and start_starling_base.sh -ow will automatically open the UI webpages. Note: The ./run_k3s script internally runs start_k3s.sh and start_single_px4sitl_gazebo.sh . Any options passed will be forwarded to the relevant script. Modifying the example controller \u00b6 In the controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package can be built by running make in the controllers directory. This will use colcon build to build the node and output a local image named example_controller_python . Inside the controllers folder, there is an annotated kubernetes config file k8.example_controller_python.amd64.yaml . This specifies the deployment of a pod which contains your local example_controller_python image ( this line ). Similar to before you can start up the local example controller by using the script: ./scripts/start_example_controller.sh But if you have made a copy, or wish to run your own version of the configuration, you can manually deploy your controller by running the following: k3s apply -f k8.example_controller_python.amd64.yaml k3s delete -f k8.example_controller_python.amd64.yaml # To delete See kubernetes configuration for more details. For debugging, you can both see the logs, and execute on your controller container through the dashboard. See instructions here Inside you can source install/setup.bash and run ROS2 commands like normal. Creating your own from scratch \u00b6 Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. 3. A Kubernetes YAML config file specifying either a Pod or a deployment. Use the example k8.example_controller_python.amd64.yaml as a template. There are annoated comments. Also see here for more details. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> . Once built, the configuration file must have a container config specifying your own image name. Your container can then be deployed manually using kubectl -f <your config>.yaml as above. Troubleshooting/ FAQs \u00b6","title":"Multiple Drones Simulation on Local Cluster"},{"location":"guide/multiple-drone-local-machine/#multiple-drones-local-machine","text":"Follow these instructions for quick and easy testing of controllers on multiple simulated drones on a single local machine. Use in the following scenarios : Local development of multiple drone applications in simulation (gazebo) Local development of offboard control (drone control software on pc not on drone) Local development of distributed onboard control (drone control software running on drone itself) Testing controllers/software on the real drone software/ communications architecture that would be used in the BRL. This is considered to be step 2b for the Starling development process. Note: Reading the background may be useful but not necessary.","title":"Multiple Drones Local Machine"},{"location":"guide/multiple-drone-local-machine/#contents","text":"Multiple Drones Local Machine Contents Drone and Simulator on a local cluster Starting the cluster Restarting or deleting the drone or simulator in the cluster Accessing logs on the dashboard Scaling the number of drones Dashboard Configuration file Controlling the Drones Offboard Control 1. Connecting a Ground Control Station via Mavlink 2. Running Example ROS2 Offboard Controller node Onboard Control Development on the Drone and Simulator Useful Scripts: Modifying the example controller Creating your own from scratch Troubleshooting/ FAQs","title":"Contents"},{"location":"guide/multiple-drone-local-machine/#drone-and-simulator-on-a-local-cluster","text":"First check that you have installed the single prerequisit of docker , see Getting Started","title":"Drone and Simulator on a local cluster"},{"location":"guide/multiple-drone-local-machine/#starting-the-cluster","text":"In the root directory run one of the following in a terminal ./run_k3s.sh ./run_k3s.sh -ow # Will automatically open the UI webpages for you. This will start the following: Install the cluster root node which governs the running of all the other parts. A node running the Gazebo simulation environment A node running the following: An initialisation routine spawning 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A MAVROS node connected to the SITL instance A simple UI with a go and estop button. The cluster control dashboard (printing out the access key string) Note: this might take a while on first run as downloads are required. Note: this installation will ask for root (sudo) permission when necessary. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Go to http://localhost:3000 in a browser to see the starling user interface containing go/stop buttons. Go to http://localhost:31771 in a browser to see the cluster dashboard. There is a lot here, and this guide will point you to the key functions. Please see this page for further details. Your browser of choice may not like the web-page and complain of certificate errors. Please ignore this and continue onwards. You may have to click 'advanced' or a similar option for the browser to let you in. To log in to the site, you will need the long login Token which is hopefully displayed by run_k3s.sh . This token should also be automatically placed onto your clipboard for pasting. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane.","title":"Starting the cluster"},{"location":"guide/multiple-drone-local-machine/#restarting-or-deleting-the-drone-or-simulator-in-the-cluster","text":"There may be cases where you wish to restart or refresh either the software running on the drones or the simulator itself (e.g. removing old models): ./run_k3s.sh -d # or --delete will remove the gazebo and drone instances ./run_k3s.sh -r # or --restart will restart the gazebo and drone instances Note: you can also add the -sk command which will skip the k3s re-download step and the dashboard check, i.e. ./run_k3s.sh -sk -r If you wish to remove the cluster and all associated software from the machine, you will need to uninstall: ./run_k3s.sh --uninstall Note: This will remove everything to do with the starling cluster. The dashboard access token will be deleted. The container images will remain on your machine, but to remove those as well run docker system prune --volumes .","title":"Restarting or deleting the drone or simulator in the cluster"},{"location":"guide/multiple-drone-local-machine/#accessing-logs-on-the-dashboard","text":"Please see the instructions here","title":"Accessing logs on the dashboard"},{"location":"guide/multiple-drone-local-machine/#scaling-the-number-of-drones","text":"","title":"Scaling the number of drones"},{"location":"guide/multiple-drone-local-machine/#dashboard","text":"The simplest way to scale the number of drones is via the dashboard as shown below: The steps are as follows: Select the 'Stateful Sets' page on the sidebar There should be an entry called 'starling-px4-sitl' representing the drone deployment. On the right hand side, there is an option button: \u2630. Press the option button and select Scale In the Desired replicas field, input the number of drones you wish to deploy. Then click Save to deploy. Going to gazebo, the drones should start appearing. The default drone arrangement is in a discrete spiral from (0, 0). Note: Right now, drones cannot be removed or despawned from the simulator. (Gazebo ROS2 despawning issues). If you have spawned too many, it is currently recommended that you restart gazebo and all the drones using ./script/start_px4sitl_gazebo.sh -r .","title":"Dashboard"},{"location":"guide/multiple-drone-local-machine/#configuration-file","text":"A more permanent way is to specify in the cluster deployment configuration file. Please see this guide to kubernetes configuraiton files for more information. Within a kubernetes configuration of kind Deployment or StatefulSet specifcation you can set the 'replicas' option. The example configurations are all in the deployment folder of the repository. apiVersion: apps/v1 kind: Deployment metadata: ... spec: replicas: 3 ...","title":"Configuration file"},{"location":"guide/multiple-drone-local-machine/#controlling-the-drones","text":"","title":"Controlling the Drones"},{"location":"guide/multiple-drone-local-machine/#offboard-control","text":"There are two supported methods for offboard control of either the SITL or real drones. Control drone directly via Mavlink, by Ground Control Station (GCS) or other Mavlink compatible method (e.g. Dronekit). Control drone via ROS2 node","title":"Offboard Control"},{"location":"guide/multiple-drone-local-machine/#1-connecting-a-ground-control-station-via-mavlink","text":"If a mavros or sitl instance is running, there will be a GCS link on udp://localhost:14553 (hopefully). This means that you can run a GCS such as QGroundControl or Mission Planner: Create a comms link to localhost:14553 on UDP The GCS should auto detect the drone(s) You should be able to control and monitor any sitl drone through the standard mavlink interface. This is a quick an easy way to control the SITL instance via Mavlink.","title":"1. Connecting a Ground Control Station via Mavlink"},{"location":"guide/multiple-drone-local-machine/#2-running-example-ros2-offboard-controller-node","text":"An example offboard ROS2 controller can then be connected to SITL by running the following in a terminal: ./scripts/start_example_controller.sh This will first build the example controller so it is available locally. Then deploy the example controller to the cluster. It will take a few minutes to startup. When run, the example will confirm in the terminal that it has connected and that it is waiting for mission start. To start the mission, press the green go button in the starling user interface which will send a message over /mission_start topic. A confirmation message should appear in the terminal, and the drone will arm (drone propellors spinning up) and takeoff. It will fly in circles for 10 seconds before landing and disarming. Once the controller has completed, the process will exit and the controller will restart, allowing you to repeat the controller. If used with multiple vehicles, it will automatically find all drones broadcasting mavros topics, and start a controller for each one. To remove or restart the controller, use the -d or -r options respectively with the script. ./scripts/start_example_controller.sh -d # Delete or Remove controller ./scripts/start_example_controller.sh -r # Restart controller See example python controller for more details.","title":"2. Running Example ROS2 Offboard Controller node"},{"location":"guide/multiple-drone-local-machine/#onboard-control","text":"TODO: Implement example onboard controller and associated scripts","title":"Onboard Control"},{"location":"guide/multiple-drone-local-machine/#development-on-the-drone-and-simulator","text":"","title":"Development on the Drone and Simulator"},{"location":"guide/multiple-drone-local-machine/#useful-scripts","text":"There are a number of useful scripts in the /scripts directory of this repository. Scripts can be run from any location, but for this tutorial we assume the user is in the root directory. ./scripts/start_k3s.sh - This starts the cluster -u will uninstall the cluster from the machine (and remove any running processes) -sk will skip the installation check for k3s ./scripts/start_single_px4sitl_gazebo.sh - This starts the starling core functions (Dashboard and UI). It also starts the gazebo simulator and a 'drone' process running the PX4 SITL and a connected Mavros ROS2 node. Assumes the cluster has been started. -d will stop the gazebo and all connected 'drone' processes only (use if reloading the controller). -r will restart the gazebo and all connected 'drone' processes. labels: app: nginx -sk will skip the starting/check that the starling core functions are running. -ow will automatically open the UI webpages. ./scripts/start_starling_base.sh - This starts the starling user interface and dashboard. Automatically run by start_single_px4sitl_gazebo.sh ./scripts/start_dashboard.sh - This starts the dashboard process on the cluster. Assumes the cluster has already been started. Automatically run by start_single_px4sitl_gazebo.sh and start_starling_base.sh -ow will automatically open the UI webpages. Note: The ./run_k3s script internally runs start_k3s.sh and start_single_px4sitl_gazebo.sh . Any options passed will be forwarded to the relevant script.","title":"Useful Scripts:"},{"location":"guide/multiple-drone-local-machine/#modifying-the-example-controller","text":"In the controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package can be built by running make in the controllers directory. This will use colcon build to build the node and output a local image named example_controller_python . Inside the controllers folder, there is an annotated kubernetes config file k8.example_controller_python.amd64.yaml . This specifies the deployment of a pod which contains your local example_controller_python image ( this line ). Similar to before you can start up the local example controller by using the script: ./scripts/start_example_controller.sh But if you have made a copy, or wish to run your own version of the configuration, you can manually deploy your controller by running the following: k3s apply -f k8.example_controller_python.amd64.yaml k3s delete -f k8.example_controller_python.amd64.yaml # To delete See kubernetes configuration for more details. For debugging, you can both see the logs, and execute on your controller container through the dashboard. See instructions here Inside you can source install/setup.bash and run ROS2 commands like normal.","title":"Modifying the example controller"},{"location":"guide/multiple-drone-local-machine/#creating-your-own-from-scratch","text":"Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. 3. A Kubernetes YAML config file specifying either a Pod or a deployment. Use the example k8.example_controller_python.amd64.yaml as a template. There are annoated comments. Also see here for more details. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> . Once built, the configuration file must have a container config specifying your own image name. Your container can then be deployed manually using kubectl -f <your config>.yaml as above.","title":"Creating your own from scratch"},{"location":"guide/multiple-drone-local-machine/#troubleshooting-faqs","text":"","title":"Troubleshooting/ FAQs"},{"location":"guide/single-drone-drones/","text":"Single Drone Deployment onto Real Drones in the Flight Arena \u00b6 Follow these instructions to deploy developed controllers on a single drone in the flight arena. This is considered to be setp 3 for the Starling development process. Note: Reading the flight arena setup may be useful but should not be necessary. Contents \u00b6 Single Drone Deployment onto Real Drones in the Flight Arena Contents Setup Steps Controller development Offboard Controllers Onboard Controllers Execution Troubleshooting/ FAQs Setup Steps \u00b6 Controller development \u00b6 It has been assumed that you have tested your controller with the docker-compose set up and with the previous local cluster setup. When running on real drones, there are a number of points to be aware of depending on if you are running offboard or onboard. Offboard Controllers \u00b6 Onboard Controllers \u00b6 Execution \u00b6 Troubleshooting/ FAQs \u00b6","title":"Single Drone on Hardware"},{"location":"guide/single-drone-drones/#single-drone-deployment-onto-real-drones-in-the-flight-arena","text":"Follow these instructions to deploy developed controllers on a single drone in the flight arena. This is considered to be setp 3 for the Starling development process. Note: Reading the flight arena setup may be useful but should not be necessary.","title":"Single Drone Deployment onto Real Drones in the Flight Arena"},{"location":"guide/single-drone-drones/#contents","text":"Single Drone Deployment onto Real Drones in the Flight Arena Contents Setup Steps Controller development Offboard Controllers Onboard Controllers Execution Troubleshooting/ FAQs","title":"Contents"},{"location":"guide/single-drone-drones/#setup-steps","text":"","title":"Setup Steps"},{"location":"guide/single-drone-drones/#controller-development","text":"It has been assumed that you have tested your controller with the docker-compose set up and with the previous local cluster setup. When running on real drones, there are a number of points to be aware of depending on if you are running offboard or onboard.","title":"Controller development"},{"location":"guide/single-drone-drones/#offboard-controllers","text":"","title":"Offboard Controllers"},{"location":"guide/single-drone-drones/#onboard-controllers","text":"","title":"Onboard Controllers"},{"location":"guide/single-drone-drones/#execution","text":"","title":"Execution"},{"location":"guide/single-drone-drones/#troubleshooting-faqs","text":"","title":"Troubleshooting/ FAQs"},{"location":"guide/single-drone-local-machine/","text":"Single Drone Local Machine \u00b6 Follow these instructions for quick and easy testing of controllers on a single drone on a single local machine. Use in the following scenarios : Local development of single drone applications in simulation (gazebo) Local development of offboard (drone control software on pc not on drone) Local development of onboard (drone control software running on drone itself) No requirement of imitating the real drone software/ communication architecture This is considered to be step 1 for the Starling development process. Contents \u00b6 Single Drone Local Machine Contents Pre-Setup Starting the Drone and Simulator (Core for use with GCS) Starting the Drone and Simulator (Simple-Offboard through ROS2) Offboard Control 1. Connecting a Ground Control Station via Mavlink 2. Running Example ROS2 Offboard Controller node 3. Using ROS2 Simple Offboard Controller Node 4. Uploading a Trajectory Onboard Control Implementing a Controller Modifying the example controller Creating your own from scratch Troubleshooting/ FAQs Pre-Setup \u00b6 There are two basic example starting confgurations, this 'Simple' version starts the bare minimum with the expectation of it being used through mavlink. There is also a 'Full' version with instructions below this section which starts a system capable of flying higher level paths and trajectories submitted through a GUI or ROS. First check that you have installed the single prerequisit of docker , see Getting Started . You may also need to install docker-compose via sudo apt-get install docker-compose . Secondly you will need to clone the examples repository. If you are only running examples, clone the Murmuration repository: git clone https://github.com/StarlingUAS/Murmuration.git cd Murmuration If you are preparing to run multi-drone systems or require more complex setup, recursively clone the ProjectStarling repository: git clone --recurse-submodules https://github.com/StarlingUAS/ProjectStarling.git cd ProjectStarling/Mumurations # Go into Murmuration directory Note if ProjectStarling is already cloned, the submodules can be accessed by running git submodule init && git submodule update Starting the Drone and Simulator (Core for use with GCS) \u00b6 In the Mumuration root directory first download the most up-to-date versions of the system, then start up the system: # First Pull the relevant docker containers docker-compose -f docker-compose/px4/docker-compose.core.linux.yml pull # or for windows # docker-compose -f docker-compose/px4/docker-compose.core.windows.yml pull # Run the docker-containers docker-compose -f docker-compose/px4/docker-compose.core.linux.yml up # or for windows # docker-compose -f docker-compose/px4/docker-compose.core.windows.yml up This will start the following: Gazebo simulation enviornment running 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A Mavros node connected to the SITL instance A ros-web-bridge allowing for ros connections through the web (this is used later) Note: the downloads might take a while (can be up to 30 min or an hour depending on internet) on first run as downloads are required. Note: Use ctrl+c to stop the process when you have finished. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane. Separately, a Ground Control Station such as QGroundControl for Linux or Mission Planner can be downloaded and started. The simulated drone should automatically connect through the default ports. See below Starting the Drone and Simulator (Simple-Offboard through ROS2) \u00b6 This confugration starts a slightly more complex system which is capable of automatic higher level navigation and trajectory following. Ensure that the 'simple' version has been stopped (Ctrl+C) before using this one. In the Mumuration root directory first download the most up-to-date versions of the system, then start up the system: # First Pull the relevant docker containers docker-compose -f docker-compose/px4/docker-compose.simple-offboard.linux.yml pull # or for windows # docker-compose -f docker-compose/px4/docker-compose.simple-offboard.windows.yml pull # Run the docker-containers docker-compose -f docker-compose/px4/docker-compose.simple-offboard.linux.yml up # or for windows # docker-compose -f docker-compose/px4/docker-compose.simple-offboard.windows.yml up This will start the following: Gazebo simulation enviornment running 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A Mavros node connected to the SITL instance A Simple Offboard PX4 controller which provides higher level navigation actions A UI with a go, abort and estop button, as well as a trajectory uploading utility A Allocation handler which distibutes uploaded trajectories to detected vehicles Note: this might take a while (can be up to 30 min or an hour depending on internet) on first run as downloads are required. Note: Use ctrl+c to stop the process when you have finished. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Go to http://localhost:3000 in a browser to see the starling user interface containing go/stop buttons. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane. Offboard Control \u00b6 There are four supported methods for offboard control of either the SITL or real drones. Control drone directly via Mavlink, by Ground Control Station (GCS) or other Mavlink compatible method (e.g. Dronekit). Control drone via Custom ROS2 node Control drone via Simple Offboard ROS2 node Uploading a trajectory via the GUI on http://localhost:3000 1. Connecting a Ground Control Station via Mavlink \u00b6 If a mavros or sitl instance is running, there will be a GCS link on udp://localhost:14553 (hopefully). This means that you can run a GCS such as QGroundControl or Mission Planner: Create a comms link to localhost:14553 on UDP The GCS should auto detect the drone(s) You should be able to control and monitor any sitl drone through the standard mavlink interface. This is a quick an easy way to control the SITL instance via Mavlink. 2. Running Example ROS2 Offboard Controller node \u00b6 An example offboard ROS2 controller can then be conncted to SITL by running the following in a separate terminal: docker pull uobflightlabstarling/example_controller_python # Download the latest container docker run -it --rm --network px4_default uobflightlabstarling/example_controller_python This will download and run the example_controller_python image from uobflightlabstarling on docker hub. -it opens an interactive terminal. --rm removes the container when completed. --network attaches the container to the default network created by docker-compose . The default network name is \" <folder-containing-docker-compose-file>_default \". Note: The controller may complain that it cannot find the drone. Double check that the name of the root folder matches the one passed to --network . When run, the example will confirm in the terminal that it has connected and that it is waiting for mission start. To start the mission, press the green go button in the starling user interface which will send a message over /mission_start topic. A confirmation message should appear in the terminal, and the drone will arm (drone propellors spinning up) and takeoff. It will fly in circles for 10 seconds before landing and disarming. 3. Using ROS2 Simple Offboard Controller Node \u00b6 If running the 'Full' version instructions above, this node will already be running. If not runing, the container can be run by running the following in a separate terminal: docker pull uobflightlabstarling/starling_simple_offboard # Download the latest container docker run -it --rm --network projectstarling_default -e VEHICLE_MAVLINK_SYSID=1 uobflightlabstarling/starling_simple_offboard This will download and run the simple offboard controller node. This node provides a number of ROS2 Services which can be called to interact with the connected drone. The easiest way to interact with this controller is by uploading a trajectory for the integrated trajectory follower to handle (see next point). See the repository for further details. 4. Uploading a Trajectory \u00b6 If running the 'Full' version instructions above, the GUI should be available on http://localhost:3000 . Navigating to http://localhost:3000/load_trajectories will give you a page where you can upload trajectories in csv or excel format. The trajectories can be position, velocity, attitude or attitude rates. Click on the 'Help' button for more information on trajectory format. Onboard Control \u00b6 In this instance there is only an abstract difference between onboard and offboard control since all controllers are running on the same machine as the simulated vehicle. Implementing a Controller \u00b6 Modifying the example controller \u00b6 In the ProjectStarling controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package must first be built: cd controllers make example_controller_python Note Alternatively you can run the build manually through docker build -t example_controller_python example_controller_python/ (i.e. point to the folder containing the example_controller_python dockerfile) This will use colcon build to build the node and output a local image named example_controller_python . This local image can be run as follows: docker run -it --name example_controller --rm --network projectstarling_default example_controller_python Note that because uobflightlabstarling is not referenced, it will look locally for a docker image. --name gives this instance a name which we can refer to. Each container essentially runs its own operating system (see wiki for more details). Just as you could ssh into another machine, you can also inspect a running container: docker exec -it example_controller bash Where example_controller is the name we gave the running instance. We essentially tell the container to execute bash for us to get a command line Inside you can source install/setup.bash and run ROS2 commands like normal. Creating your own from scratch \u00b6 See the development docs for more detailed information on container development. Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> <folder containing the dockerfile> # e.g. docker build -t my_new_controller . Your container can then be run as above using `docker run. Troubleshooting/ FAQs \u00b6","title":"Single Drone Simulation on Local Machine"},{"location":"guide/single-drone-local-machine/#single-drone-local-machine","text":"Follow these instructions for quick and easy testing of controllers on a single drone on a single local machine. Use in the following scenarios : Local development of single drone applications in simulation (gazebo) Local development of offboard (drone control software on pc not on drone) Local development of onboard (drone control software running on drone itself) No requirement of imitating the real drone software/ communication architecture This is considered to be step 1 for the Starling development process.","title":"Single Drone Local Machine"},{"location":"guide/single-drone-local-machine/#contents","text":"Single Drone Local Machine Contents Pre-Setup Starting the Drone and Simulator (Core for use with GCS) Starting the Drone and Simulator (Simple-Offboard through ROS2) Offboard Control 1. Connecting a Ground Control Station via Mavlink 2. Running Example ROS2 Offboard Controller node 3. Using ROS2 Simple Offboard Controller Node 4. Uploading a Trajectory Onboard Control Implementing a Controller Modifying the example controller Creating your own from scratch Troubleshooting/ FAQs","title":"Contents"},{"location":"guide/single-drone-local-machine/#pre-setup","text":"There are two basic example starting confgurations, this 'Simple' version starts the bare minimum with the expectation of it being used through mavlink. There is also a 'Full' version with instructions below this section which starts a system capable of flying higher level paths and trajectories submitted through a GUI or ROS. First check that you have installed the single prerequisit of docker , see Getting Started . You may also need to install docker-compose via sudo apt-get install docker-compose . Secondly you will need to clone the examples repository. If you are only running examples, clone the Murmuration repository: git clone https://github.com/StarlingUAS/Murmuration.git cd Murmuration If you are preparing to run multi-drone systems or require more complex setup, recursively clone the ProjectStarling repository: git clone --recurse-submodules https://github.com/StarlingUAS/ProjectStarling.git cd ProjectStarling/Mumurations # Go into Murmuration directory Note if ProjectStarling is already cloned, the submodules can be accessed by running git submodule init && git submodule update","title":"Pre-Setup"},{"location":"guide/single-drone-local-machine/#starting-the-drone-and-simulator-core-for-use-with-gcs","text":"In the Mumuration root directory first download the most up-to-date versions of the system, then start up the system: # First Pull the relevant docker containers docker-compose -f docker-compose/px4/docker-compose.core.linux.yml pull # or for windows # docker-compose -f docker-compose/px4/docker-compose.core.windows.yml pull # Run the docker-containers docker-compose -f docker-compose/px4/docker-compose.core.linux.yml up # or for windows # docker-compose -f docker-compose/px4/docker-compose.core.windows.yml up This will start the following: Gazebo simulation enviornment running 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A Mavros node connected to the SITL instance A ros-web-bridge allowing for ros connections through the web (this is used later) Note: the downloads might take a while (can be up to 30 min or an hour depending on internet) on first run as downloads are required. Note: Use ctrl+c to stop the process when you have finished. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane. Separately, a Ground Control Station such as QGroundControl for Linux or Mission Planner can be downloaded and started. The simulated drone should automatically connect through the default ports. See below","title":"Starting the Drone and Simulator (Core for use with GCS)"},{"location":"guide/single-drone-local-machine/#starting-the-drone-and-simulator-simple-offboard-through-ros2","text":"This confugration starts a slightly more complex system which is capable of automatic higher level navigation and trajectory following. Ensure that the 'simple' version has been stopped (Ctrl+C) before using this one. In the Mumuration root directory first download the most up-to-date versions of the system, then start up the system: # First Pull the relevant docker containers docker-compose -f docker-compose/px4/docker-compose.simple-offboard.linux.yml pull # or for windows # docker-compose -f docker-compose/px4/docker-compose.simple-offboard.windows.yml pull # Run the docker-containers docker-compose -f docker-compose/px4/docker-compose.simple-offboard.linux.yml up # or for windows # docker-compose -f docker-compose/px4/docker-compose.simple-offboard.windows.yml up This will start the following: Gazebo simulation enviornment running 1 Iris quadcopter model A SITL (Software In The Loop) instance running the PX4 autopilot. A Mavros node connected to the SITL instance A Simple Offboard PX4 controller which provides higher level navigation actions A UI with a go, abort and estop button, as well as a trajectory uploading utility A Allocation handler which distibutes uploaded trajectories to detected vehicles Note: this might take a while (can be up to 30 min or an hour depending on internet) on first run as downloads are required. Note: Use ctrl+c to stop the process when you have finished. The User Interfaces are available in the following locations: Go to http://localhost:8080 in a browser to (hopefully) see the gazebo simulator. Go to http://localhost:3000 in a browser to see the starling user interface containing go/stop buttons. Note: All specified sites can be accessed from other machines by replacing localhost with your computer's IP address. Note: Sometimes it might take a bit of time for the UIs to become available, give it a minute and refresh the page. With Gazebo you may accidentally be too zoomed in, or the grid may not show up. Use the mouse wheel to zoom in and out. The grid can be toggled on the left hand pane.","title":"Starting the Drone and Simulator (Simple-Offboard through ROS2)"},{"location":"guide/single-drone-local-machine/#offboard-control","text":"There are four supported methods for offboard control of either the SITL or real drones. Control drone directly via Mavlink, by Ground Control Station (GCS) or other Mavlink compatible method (e.g. Dronekit). Control drone via Custom ROS2 node Control drone via Simple Offboard ROS2 node Uploading a trajectory via the GUI on http://localhost:3000","title":"Offboard Control"},{"location":"guide/single-drone-local-machine/#1-connecting-a-ground-control-station-via-mavlink","text":"If a mavros or sitl instance is running, there will be a GCS link on udp://localhost:14553 (hopefully). This means that you can run a GCS such as QGroundControl or Mission Planner: Create a comms link to localhost:14553 on UDP The GCS should auto detect the drone(s) You should be able to control and monitor any sitl drone through the standard mavlink interface. This is a quick an easy way to control the SITL instance via Mavlink.","title":"1. Connecting a Ground Control Station via Mavlink"},{"location":"guide/single-drone-local-machine/#2-running-example-ros2-offboard-controller-node","text":"An example offboard ROS2 controller can then be conncted to SITL by running the following in a separate terminal: docker pull uobflightlabstarling/example_controller_python # Download the latest container docker run -it --rm --network px4_default uobflightlabstarling/example_controller_python This will download and run the example_controller_python image from uobflightlabstarling on docker hub. -it opens an interactive terminal. --rm removes the container when completed. --network attaches the container to the default network created by docker-compose . The default network name is \" <folder-containing-docker-compose-file>_default \". Note: The controller may complain that it cannot find the drone. Double check that the name of the root folder matches the one passed to --network . When run, the example will confirm in the terminal that it has connected and that it is waiting for mission start. To start the mission, press the green go button in the starling user interface which will send a message over /mission_start topic. A confirmation message should appear in the terminal, and the drone will arm (drone propellors spinning up) and takeoff. It will fly in circles for 10 seconds before landing and disarming.","title":"2. Running Example ROS2 Offboard Controller node"},{"location":"guide/single-drone-local-machine/#3-using-ros2-simple-offboard-controller-node","text":"If running the 'Full' version instructions above, this node will already be running. If not runing, the container can be run by running the following in a separate terminal: docker pull uobflightlabstarling/starling_simple_offboard # Download the latest container docker run -it --rm --network projectstarling_default -e VEHICLE_MAVLINK_SYSID=1 uobflightlabstarling/starling_simple_offboard This will download and run the simple offboard controller node. This node provides a number of ROS2 Services which can be called to interact with the connected drone. The easiest way to interact with this controller is by uploading a trajectory for the integrated trajectory follower to handle (see next point). See the repository for further details.","title":"3. Using ROS2 Simple Offboard Controller Node"},{"location":"guide/single-drone-local-machine/#4-uploading-a-trajectory","text":"If running the 'Full' version instructions above, the GUI should be available on http://localhost:3000 . Navigating to http://localhost:3000/load_trajectories will give you a page where you can upload trajectories in csv or excel format. The trajectories can be position, velocity, attitude or attitude rates. Click on the 'Help' button for more information on trajectory format.","title":"4. Uploading a Trajectory"},{"location":"guide/single-drone-local-machine/#onboard-control","text":"In this instance there is only an abstract difference between onboard and offboard control since all controllers are running on the same machine as the simulated vehicle.","title":"Onboard Control"},{"location":"guide/single-drone-local-machine/#implementing-a-controller","text":"","title":"Implementing a Controller"},{"location":"guide/single-drone-local-machine/#modifying-the-example-controller","text":"In the ProjectStarling controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package must first be built: cd controllers make example_controller_python Note Alternatively you can run the build manually through docker build -t example_controller_python example_controller_python/ (i.e. point to the folder containing the example_controller_python dockerfile) This will use colcon build to build the node and output a local image named example_controller_python . This local image can be run as follows: docker run -it --name example_controller --rm --network projectstarling_default example_controller_python Note that because uobflightlabstarling is not referenced, it will look locally for a docker image. --name gives this instance a name which we can refer to. Each container essentially runs its own operating system (see wiki for more details). Just as you could ssh into another machine, you can also inspect a running container: docker exec -it example_controller bash Where example_controller is the name we gave the running instance. We essentially tell the container to execute bash for us to get a command line Inside you can source install/setup.bash and run ROS2 commands like normal.","title":"Modifying the example controller"},{"location":"guide/single-drone-local-machine/#creating-your-own-from-scratch","text":"See the development docs for more detailed information on container development. Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> <folder containing the dockerfile> # e.g. docker build -t my_new_controller . Your container can then be run as above using `docker run.","title":"Creating your own from scratch"},{"location":"guide/single-drone-local-machine/#troubleshooting-faqs","text":"","title":"Troubleshooting/ FAQs"},{"location":"guide/windows-support/","text":"Starling on Windows \u00b6 Running Starling on Windows needs a few things to be installed: Installing Docker \u00b6 Starling relies on a technology called Docker. This has a Windows installer on the docker site: docs.docker.com/docker-for-windows/install/ Download and run the installer, to install Docker on Windows. Note that if you have Windows 10 Home (rather than Pro), the instructions differ slightly. They can be found here: docs.docker.com/docker-for-windows/install-windows-home/ . Installing Git \u00b6 If you don't already have Git installed, it is highly recommended. Git manages source code versioning and is used for distributing the core Starling files. It can be downloaded from here: git-scm.com/download/win . You could also install GitHub Desktop (other vendors are available). Installing WSL \u00b6 Windows Subsystem for Linux allows a windows machine to run a linux environment without needing a Virtual Machine or having to restart the computer. While it is not strictly necessary (most things can be run with Powershell), it is hightly recommended as it reduces most issues caused by windows itself. To install, you should follow the instructions from https://docs.microsoft.com/en-us/windows/wsl/install . Getting Starling \u00b6 For simple usage of the system, clone the Mumuration repo to a working folder on your computer. If you are doing slightly more complex work, clone the ProjectStarling repo to a working folder on your computer. You probably want to avoid any cloud-backed folders as these tend to interfere with Git. With Git installed as above, you can open PowerShell, navigate to the folder where you want to clone the project (using cd ) and run: # Mumurations git clone https://github.com/StarlingUAS/Mumuration # ProjectStarling git clone --recurse-subdmoules https://github.com/StarlingUAS/ProjectStarling Running \u00b6 Using the Starling CLI should reduce many of the problems. With Docker installed and the repo downloaded, navigate into the project folder in a terminal. Once there you should see a file called docker-compose.tcp.yml . This contains instructions for a tool called Docker Compose to setup a set of containers for you. Note that this version has been modified slightly to use TCP ports which makes things easier on Windows/WSL. To launch the containers, open PowerShell, make sure you're in the root of the Mumuration repo (if using ProjectStarling it will be 'ProjectStarling/Mumuration') and run the windows version of any launch: docker-compose -f docker-compose/px4/docker-compose.simple-offboard.windows.yml up You should see the tool begin to \"pull\" (download) the files needed to run the project. These have been built from files in the Starling repo and uploaded to DockerHub to save you time. Some of these files are quite big, so be prepared to wait a while for downloading to finish. After the first run, the system will use the already-downloaded files which will speed up the process. One the downloading process is complete, the tool will begin to setup the containers. At this point, Windows may prompt you to allow \"Docker Container Backend\" to connect to networks With everything running, and docker allowed to access the network, open a web browser and go to localhost:8080 where you should see the web-based Gazebo interface. You can also launch Mission Planner (other GCS software is available) and connect to TCP port 5760. Using the IP address of 127.0.0.1 should work as docker will ensure that the connection gets routed to the right place. Things to be aware of \u00b6 Docker works slightly differently in Windows compared to Linux which can cause problems, especially with regards to networking. If you're webpages for gazebo or others are not connecting on local ports, it may be because you have run the linux docker-compose file by accident!","title":"Windows Support and FAQ"},{"location":"guide/windows-support/#starling-on-windows","text":"Running Starling on Windows needs a few things to be installed:","title":"Starling on Windows"},{"location":"guide/windows-support/#installing-docker","text":"Starling relies on a technology called Docker. This has a Windows installer on the docker site: docs.docker.com/docker-for-windows/install/ Download and run the installer, to install Docker on Windows. Note that if you have Windows 10 Home (rather than Pro), the instructions differ slightly. They can be found here: docs.docker.com/docker-for-windows/install-windows-home/ .","title":"Installing Docker"},{"location":"guide/windows-support/#installing-git","text":"If you don't already have Git installed, it is highly recommended. Git manages source code versioning and is used for distributing the core Starling files. It can be downloaded from here: git-scm.com/download/win . You could also install GitHub Desktop (other vendors are available).","title":"Installing Git"},{"location":"guide/windows-support/#installing-wsl","text":"Windows Subsystem for Linux allows a windows machine to run a linux environment without needing a Virtual Machine or having to restart the computer. While it is not strictly necessary (most things can be run with Powershell), it is hightly recommended as it reduces most issues caused by windows itself. To install, you should follow the instructions from https://docs.microsoft.com/en-us/windows/wsl/install .","title":"Installing WSL"},{"location":"guide/windows-support/#getting-starling","text":"For simple usage of the system, clone the Mumuration repo to a working folder on your computer. If you are doing slightly more complex work, clone the ProjectStarling repo to a working folder on your computer. You probably want to avoid any cloud-backed folders as these tend to interfere with Git. With Git installed as above, you can open PowerShell, navigate to the folder where you want to clone the project (using cd ) and run: # Mumurations git clone https://github.com/StarlingUAS/Mumuration # ProjectStarling git clone --recurse-subdmoules https://github.com/StarlingUAS/ProjectStarling","title":"Getting Starling"},{"location":"guide/windows-support/#running","text":"Using the Starling CLI should reduce many of the problems. With Docker installed and the repo downloaded, navigate into the project folder in a terminal. Once there you should see a file called docker-compose.tcp.yml . This contains instructions for a tool called Docker Compose to setup a set of containers for you. Note that this version has been modified slightly to use TCP ports which makes things easier on Windows/WSL. To launch the containers, open PowerShell, make sure you're in the root of the Mumuration repo (if using ProjectStarling it will be 'ProjectStarling/Mumuration') and run the windows version of any launch: docker-compose -f docker-compose/px4/docker-compose.simple-offboard.windows.yml up You should see the tool begin to \"pull\" (download) the files needed to run the project. These have been built from files in the Starling repo and uploaded to DockerHub to save you time. Some of these files are quite big, so be prepared to wait a while for downloading to finish. After the first run, the system will use the already-downloaded files which will speed up the process. One the downloading process is complete, the tool will begin to setup the containers. At this point, Windows may prompt you to allow \"Docker Container Backend\" to connect to networks With everything running, and docker allowed to access the network, open a web browser and go to localhost:8080 where you should see the web-based Gazebo interface. You can also launch Mission Planner (other GCS software is available) and connect to TCP port 5760. Using the IP address of 127.0.0.1 should work as docker will ensure that the connection gets routed to the right place.","title":"Running"},{"location":"guide/windows-support/#things-to-be-aware-of","text":"Docker works slightly differently in Windows compared to Linux which can cause problems, especially with regards to networking. If you're webpages for gazebo or others are not connecting on local ports, it may be because you have run the linux docker-compose file by accident!","title":"Things to be aware of"},{"location":"guide/writing-a-controller/","text":"Creating your own controller \u00b6 This documentation describes creating your own controller which is compatible with the Starling ecosystem. TODO: Complete this docs page Creating your own controller Overview Implementation Launch Files Basic launch file: Enabling running In simulation Build and Bake Files Basic Bake file: Make Files Overview \u00b6 Implementation \u00b6 Launch Files \u00b6 The controller itself can be run using ros2 run , but if you might want to run multiple nodes together, or provide some more complex functionality, it is recommended that ros2 nodes be run using launch files. ROS2 launch files can be written as xml or python programmes. Here we will explain using xml which imo is clearer, but the python is much more flexible. Basic launch file: \u00b6 Here is an example of a launch file which should be saved as controller.launch.xml in a launch folder in the root of the repository: <launch> <arg name=\"vehicle_namespace\" default=\"$(env VEHICLE_NAMESPACE vehicle_$(env VEHICLE_MAVLINK_SYSID))\" /> <arg name=\"demonstration\" default=\"true\"/> <group> <push-ros-namespace namespace=\"$(var vehicle_namespace)\"/> <!-- <node name=\"primitive_4pl_controller\" pkg=\"primitive_4pl\" exec=\"controller\" output=\"screen\" respawn=\"true\"> --> <node name=\"primitive_4pl_controller\" pkg=\"primitive_4pl\" exec=\"controller\" output=\"screen\"> <param name=\"use_sim_time\" value=\"$(env USE_SIMULATED_TIME false)\"/> <param name=\"frame_id\" value=\"map\"/> <param name=\"setpoint_frame_id\" value=\"$(var vehicle_namespace)/setpoint\"/> <param name=\"vehicle_frame_id\" value=\"$(var vehicle_namespace)/body\"/> </node> <node name=\"primitive_4pl_controller\" pkg=\"primitive_4pl\" exec=\"demonstration\" output=\"screen\" if=\"$(var demonstration)\"/> </group> </launch> This launch file takes two arguments which can be set by the user. Importantly the vehicle_namespace is set by the environment varible VEHICLE_NAMESPACE which is populated by an initialsation script in the controller base. Then to ensure that the controller is an onboard controller, so we have one controller running for one vehicle, we create a group and set the namespace to vehicle namespace. As long as the topic names used inside of the controller are not absolute (i.e. no leading / - mytopic vs /mytopic ), the nodes topics will get mapped to vehicle_1/mytopic . Two nodes are started inside this node group. The first one requires a number of parameters which can need to use the vehicle name. The second one doesnt need any. Enabling running In simulation \u00b6 The base controller has a USE_SIMULATED_TIME environment variable. Any node in the controller launchfile should include the following line if the controller requires use of time: <node pkg=\"mypkg\" exec=\"controller\" ...> <param name=\"use_sim_time\" value=\"$(env USE_SIMULATED_TIME false)\"/> ... </node> So when you want to run the controller in simulation, you can simply set the USE_SIMULATED_TIME to true. For example docker run --it --rm -e USE_SIMULATED_TIME mycontroller . Build and Bake Files \u00b6 A bake file is a configuration file used to specify how to build a particular dockerfile. We use it in particular because it allows us to build cross-platfrom executables in the case we want to run our containers on a drone. A drone running a raspberry pi runs the arm64 architecture, vs your own machine which most likely runs the amd64 architecture. Basic Bake file: \u00b6 TODO: Explain what this means variable \"BAKE_VERSION\" { default = \"latest\" } variable \"BAKE_REGISTRY\" { default = \"\" } variable \"BAKE_RELEASENAME\" { default = \"\" } variable \"BAKE_CACHEFROM_REGISTRY\" { default = \"\" } variable \"BAKE_CACHETO_REGISTRY\" { default = \"\" } variable \"BAKE_CACHEFROM_NAME\" { default = \"\" } variable \"BAKE_CACHETO_NAME\" { default = \"\" } /* * Groups for target ordering */ group \"stage1\" { targets = [\"4pl\"] } // This target depends on starling-controller-base target \"4pl-controller\" { context = \".\" args = { \"VERSION\": \"${BAKE_VERSION}\", \"REGISTRY\": \"${BAKE_REGISTRY}\" } tags = [ \"${BAKE_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_VERSION}\", notequal(\"\",BAKE_RELEASENAME) ? \"${BAKE_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_RELEASENAME}\": \"\", ] platforms = [\"linux/amd64\", \"linux/arm64\"] cache-to = [ notequal(\"\",BAKE_CACHETO_NAME) ? \"${BAKE_CACHETO_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_CACHETO_NAME}\" : \"\" ] cache-from = [ notequal(\"\",BAKE_CACHEFROM_NAME) ? \"${BAKE_CACHEFROM_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_CACHEFROM_NAME}\" : \"\" ] } Make Files \u00b6 It is recommended that make (or nmake for windows) be used as a tool for building your controller. Make is a useful tool as it allows the running of any number of specific commands (similar to a bash file but with arguments dealt with for you). It is recommended that the makefile look something like the following, and be put in the root of the project folder: MAKEFILE_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST)))) BAKE_SCRIPT:=$(MAKEFILE_DIR)/docker-bake.hcl BUILDX_HOST_PLATFORM:=$(shell docker buildx inspect default | sed -nE 's/^Platforms: ([^,]*),.*$$/\\1/p') BAKE:=docker buildx bake --builder default --load --set *.platform=$(BUILDX_HOST_PLATFORM) -f $(BAKE_SCRIPT) CONTROLLER_NAME=controller NETWORK?=4pl-ros2-controller_default ENV?= BUILD_ARGS?= all: build build: $(BAKE) $(BUILD_ARGS) $(CONTROLLER) # This mybuilder needs the following lines to be run: # docker run --rm --privileged multiarch/qemu-user-static --reset -p yes # docker buildx create --name mybuilder # docker buildx use mybuilder # docker buildx inspect --bootstrap local-build-push: docker buildx bake --builder mybuilder -f $(BAKE_SCRIPT) --push $(CONTROLLER_NAME) run: build docker run -it --rm --net=$(NETWORK) $(ENV) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest run_bash: build docker run -it --rm --net=$(NETWORK) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest bash .PHONY: all build local-build-push run run_bash Breaking down the make file, we have a number of commands. MAKEFILE_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST)))) BAKE_SCRIPT:=$(MAKEFILE_DIR)/docker-bake.hcl BUILDX_HOST_PLATFORM:=$(shell docker buildx inspect default | sed -nE 's/^Platforms: ([^,]*),.*$$/\\1/p') BAKE:=docker buildx bake --builder default --load --set *.platform=$(BUILDX_HOST_PLATFORM) -f $(BAKE_SCRIPT) The first 4 lines define a number of useful variables including where the current directory is, the location of your bake script, your system's curent platform to build for and the final buildx bake command for building your controller CONTROLLER_NAME=controller NETWORK?=controller_default ENV?= BUILD_ARGS?= Then we define some useful variables to us, including the name of the controller and the local network (set to the default of foldername_default). Note that ENV and BUILD_ARGS have been set to a default of empty string on purporse. Then when running the file these can be specified e.g. make ENV=-e HELLO=mynewvariable . all: build build: $(BAKE) $(BUILD_ARGS) $(CONTROLLER) # This mybuilder needs the following lines to be run: # docker run --rm --privileged multiarch/qemu-user-static --reset -p yes # docker buildx create --name mybuilder # docker buildx use mybuilder # docker buildx inspect --bootstrap local-build-push: docker buildx bake --builder mybuilder -f $(BAKE_SCRIPT) --push $(CONTROLLER_NAME) run: build docker run -it --rm --net=$(NETWORK) $(ENV) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest run_bash: build docker run -it --rm --net=$(NETWORK) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest bash Then we specify the commands. Here we have specified 5 commands: all , build , local-build-push , run and run_bash . The important one is build which can be run using make build which builds the container. A useful is run which will both build and run the newly built docker container. run_bash is the same as run except it puts you into the bash shell of the container instead of directly running. local-build-push is a helper which will help you push your container to dockerhub if you so need to. .PHONY: all build local-build-push run run_bash This final line is makefile syntax just in case you have any local files which are accidentally named exactly the same as one of the named commands.","title":"Developing a controller"},{"location":"guide/writing-a-controller/#creating-your-own-controller","text":"This documentation describes creating your own controller which is compatible with the Starling ecosystem. TODO: Complete this docs page Creating your own controller Overview Implementation Launch Files Basic launch file: Enabling running In simulation Build and Bake Files Basic Bake file: Make Files","title":"Creating your own controller"},{"location":"guide/writing-a-controller/#overview","text":"","title":"Overview"},{"location":"guide/writing-a-controller/#implementation","text":"","title":"Implementation"},{"location":"guide/writing-a-controller/#launch-files","text":"The controller itself can be run using ros2 run , but if you might want to run multiple nodes together, or provide some more complex functionality, it is recommended that ros2 nodes be run using launch files. ROS2 launch files can be written as xml or python programmes. Here we will explain using xml which imo is clearer, but the python is much more flexible.","title":"Launch Files"},{"location":"guide/writing-a-controller/#basic-launch-file","text":"Here is an example of a launch file which should be saved as controller.launch.xml in a launch folder in the root of the repository: <launch> <arg name=\"vehicle_namespace\" default=\"$(env VEHICLE_NAMESPACE vehicle_$(env VEHICLE_MAVLINK_SYSID))\" /> <arg name=\"demonstration\" default=\"true\"/> <group> <push-ros-namespace namespace=\"$(var vehicle_namespace)\"/> <!-- <node name=\"primitive_4pl_controller\" pkg=\"primitive_4pl\" exec=\"controller\" output=\"screen\" respawn=\"true\"> --> <node name=\"primitive_4pl_controller\" pkg=\"primitive_4pl\" exec=\"controller\" output=\"screen\"> <param name=\"use_sim_time\" value=\"$(env USE_SIMULATED_TIME false)\"/> <param name=\"frame_id\" value=\"map\"/> <param name=\"setpoint_frame_id\" value=\"$(var vehicle_namespace)/setpoint\"/> <param name=\"vehicle_frame_id\" value=\"$(var vehicle_namespace)/body\"/> </node> <node name=\"primitive_4pl_controller\" pkg=\"primitive_4pl\" exec=\"demonstration\" output=\"screen\" if=\"$(var demonstration)\"/> </group> </launch> This launch file takes two arguments which can be set by the user. Importantly the vehicle_namespace is set by the environment varible VEHICLE_NAMESPACE which is populated by an initialsation script in the controller base. Then to ensure that the controller is an onboard controller, so we have one controller running for one vehicle, we create a group and set the namespace to vehicle namespace. As long as the topic names used inside of the controller are not absolute (i.e. no leading / - mytopic vs /mytopic ), the nodes topics will get mapped to vehicle_1/mytopic . Two nodes are started inside this node group. The first one requires a number of parameters which can need to use the vehicle name. The second one doesnt need any.","title":"Basic launch file:"},{"location":"guide/writing-a-controller/#enabling-running-in-simulation","text":"The base controller has a USE_SIMULATED_TIME environment variable. Any node in the controller launchfile should include the following line if the controller requires use of time: <node pkg=\"mypkg\" exec=\"controller\" ...> <param name=\"use_sim_time\" value=\"$(env USE_SIMULATED_TIME false)\"/> ... </node> So when you want to run the controller in simulation, you can simply set the USE_SIMULATED_TIME to true. For example docker run --it --rm -e USE_SIMULATED_TIME mycontroller .","title":"Enabling running In simulation"},{"location":"guide/writing-a-controller/#build-and-bake-files","text":"A bake file is a configuration file used to specify how to build a particular dockerfile. We use it in particular because it allows us to build cross-platfrom executables in the case we want to run our containers on a drone. A drone running a raspberry pi runs the arm64 architecture, vs your own machine which most likely runs the amd64 architecture.","title":"Build and Bake Files"},{"location":"guide/writing-a-controller/#basic-bake-file","text":"TODO: Explain what this means variable \"BAKE_VERSION\" { default = \"latest\" } variable \"BAKE_REGISTRY\" { default = \"\" } variable \"BAKE_RELEASENAME\" { default = \"\" } variable \"BAKE_CACHEFROM_REGISTRY\" { default = \"\" } variable \"BAKE_CACHETO_REGISTRY\" { default = \"\" } variable \"BAKE_CACHEFROM_NAME\" { default = \"\" } variable \"BAKE_CACHETO_NAME\" { default = \"\" } /* * Groups for target ordering */ group \"stage1\" { targets = [\"4pl\"] } // This target depends on starling-controller-base target \"4pl-controller\" { context = \".\" args = { \"VERSION\": \"${BAKE_VERSION}\", \"REGISTRY\": \"${BAKE_REGISTRY}\" } tags = [ \"${BAKE_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_VERSION}\", notequal(\"\",BAKE_RELEASENAME) ? \"${BAKE_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_RELEASENAME}\": \"\", ] platforms = [\"linux/amd64\", \"linux/arm64\"] cache-to = [ notequal(\"\",BAKE_CACHETO_NAME) ? \"${BAKE_CACHETO_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_CACHETO_NAME}\" : \"\" ] cache-from = [ notequal(\"\",BAKE_CACHEFROM_NAME) ? \"${BAKE_CACHEFROM_REGISTRY}uobflightlabstarling/4pl-controller:${BAKE_CACHEFROM_NAME}\" : \"\" ] }","title":"Basic Bake file:"},{"location":"guide/writing-a-controller/#make-files","text":"It is recommended that make (or nmake for windows) be used as a tool for building your controller. Make is a useful tool as it allows the running of any number of specific commands (similar to a bash file but with arguments dealt with for you). It is recommended that the makefile look something like the following, and be put in the root of the project folder: MAKEFILE_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST)))) BAKE_SCRIPT:=$(MAKEFILE_DIR)/docker-bake.hcl BUILDX_HOST_PLATFORM:=$(shell docker buildx inspect default | sed -nE 's/^Platforms: ([^,]*),.*$$/\\1/p') BAKE:=docker buildx bake --builder default --load --set *.platform=$(BUILDX_HOST_PLATFORM) -f $(BAKE_SCRIPT) CONTROLLER_NAME=controller NETWORK?=4pl-ros2-controller_default ENV?= BUILD_ARGS?= all: build build: $(BAKE) $(BUILD_ARGS) $(CONTROLLER) # This mybuilder needs the following lines to be run: # docker run --rm --privileged multiarch/qemu-user-static --reset -p yes # docker buildx create --name mybuilder # docker buildx use mybuilder # docker buildx inspect --bootstrap local-build-push: docker buildx bake --builder mybuilder -f $(BAKE_SCRIPT) --push $(CONTROLLER_NAME) run: build docker run -it --rm --net=$(NETWORK) $(ENV) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest run_bash: build docker run -it --rm --net=$(NETWORK) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest bash .PHONY: all build local-build-push run run_bash Breaking down the make file, we have a number of commands. MAKEFILE_DIR:=$(shell dirname $(realpath $(firstword $(MAKEFILE_LIST)))) BAKE_SCRIPT:=$(MAKEFILE_DIR)/docker-bake.hcl BUILDX_HOST_PLATFORM:=$(shell docker buildx inspect default | sed -nE 's/^Platforms: ([^,]*),.*$$/\\1/p') BAKE:=docker buildx bake --builder default --load --set *.platform=$(BUILDX_HOST_PLATFORM) -f $(BAKE_SCRIPT) The first 4 lines define a number of useful variables including where the current directory is, the location of your bake script, your system's curent platform to build for and the final buildx bake command for building your controller CONTROLLER_NAME=controller NETWORK?=controller_default ENV?= BUILD_ARGS?= Then we define some useful variables to us, including the name of the controller and the local network (set to the default of foldername_default). Note that ENV and BUILD_ARGS have been set to a default of empty string on purporse. Then when running the file these can be specified e.g. make ENV=-e HELLO=mynewvariable . all: build build: $(BAKE) $(BUILD_ARGS) $(CONTROLLER) # This mybuilder needs the following lines to be run: # docker run --rm --privileged multiarch/qemu-user-static --reset -p yes # docker buildx create --name mybuilder # docker buildx use mybuilder # docker buildx inspect --bootstrap local-build-push: docker buildx bake --builder mybuilder -f $(BAKE_SCRIPT) --push $(CONTROLLER_NAME) run: build docker run -it --rm --net=$(NETWORK) $(ENV) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest run_bash: build docker run -it --rm --net=$(NETWORK) -e USE_SIMULATED_TIME=true $(CONTROLLER_NAME):latest bash Then we specify the commands. Here we have specified 5 commands: all , build , local-build-push , run and run_bash . The important one is build which can be run using make build which builds the container. A useful is run which will both build and run the newly built docker container. run_bash is the same as run except it puts you into the bash shell of the container instead of directly running. local-build-push is a helper which will help you push your container to dockerhub if you so need to. .PHONY: all build local-build-push run run_bash This final line is makefile syntax just in case you have any local files which are accidentally named exactly the same as one of the named commands.","title":"Make Files"},{"location":"other/MATLAB/","text":"Custom Message Support \u00b6 MATLAB HAS NO CUSTOM ROS2 SERVICE SUPPORT Starling uses ROS2 and MAVROS to communicate with the vehicles. MAVROS includes some custom message types that need to be built for MATLAB before it can use them. The main MATLAB documentation is here . The specific steps for Starling are outlined below. Check that your MATLAB installation is setup to be able to work with ROS2. The MATLAB documentation is here . Firstly you need MATLAB's ROS Toolbox. For ROS2, you need to ensure that you are using Python 3. Use pyenv in the MATLAB prompt to check the current setup. If it is not setup to use Python 3, restart MATLAB and use pyenv('Version','3.7') to set the version on Windows. On Linux, use pyenv('Version','/path/to/python') Additionally, you also need a C+ compiler and CMake. Download the MAVROS repo with ROS2 message support from Github. Note that the official repo does not yet include ROS2 support. A modified version is available here: https://github.com/rob-clarke/mavros/tree/mavros2 You can download the code as a ZIP from that site or use git to clone it: sh git clone -b mavros2 https://github.com/rob-clarke/mavros Download the geographic_info repo. Again make sure you have the ROS2 branch: sh git clone -b ros2 https://github.com/ros-geographic-info/geographic_info Copy mavros_msgs from mavros and geographic_msgs from geographic_info to a new folder called matlab_msgs : bash mkdir matlab_msgs cp -r mavros/mavros_msgs matlab_msgs/mavros_msgs cp -r geographic_info/geographic_msgs matlab_msgs/geographic_msgs Clone the unique_identifier_msgs repo into your matlab_msgs folder: git clone https://github.com/ros2/unique_identifier_msgs matlab_msgs/unique_identifier_msgs Use MATLAB's ros2genmsg on the matlab_msgs folder source: matlab ros2genmsg('matlab_msgs') On Ubuntu 18 \u00b6 ros2genmsg failed for me and I had to modify the venv it generates: echo \"export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libexpat.so\" >> ~/.matlab/R2019b/ros2/python_venv/bin/activate It then worked the second time round... See here for an answer from MATLAB support. sudo apt install python3.7 libpython3.7 python3.7-venv sudo ln -s /usr/lib/x86_64-linux-gnu/libpython3.7m.so.1 /usr/lib/libpython3.7m.so.1 sudo ln -s /usr/lib/x86_64-linux-gnu/libpython3.7m.so.1.0 /usr/lib/libpython3.7m.so.1.0 Windows \u00b6 ros2genmsg apparently needs a very specfic version of Python (namely 3.7). The last available Windows installer for this is Python 3.7.9. See the Python downloads page The ROS Toolbox needs a very specific C++ compiler too: Microsoft Visual C++ 2017. MATLAB documentation on compiler support is here . Visual Studio 2017 can be downloaded from here . You should only need the community edition. Once Visual Studio is installed, run mex -setup cpp in the MATLAB prompt. You should see something like: MEX configured to use 'Microsoft Visual C++ 2017' for C++ language compilation. You also need to have CMake, installed. A binary installer can be found here: cmake.org/download/ I'm considering giving up on Windows at this point File paths might be too long! There may be a registry fix, but we'll just move things to a shorter path! Also MATLAB installs outdated versions of things which break some of the mavros_msgs files. Need to edit mavros_msgs/msg/BatteryStatus.msg to add any random characters next to the unit comments for voltage and current . macOS \u00b6 You will need xcode tools installed. Press Cmd-space and search for \"Terminal\". Then type the command below and press enter. xcode-select --install Deployment \u00b6 ~Once built, the Matlab script calls ros.internal.custommsgs.updatePreferences(msgFullName,{},{},{},'ros2',genDir) , where genDir = fullfile(folderPath, 'matlab_msg_gen', computer('arch')); (e.g. matlab_msgs/matlab_msg_gen/win64 ), and msgFullName is a cell array containing ${PKG_NAME}/${MSG_NAME} strings. How to do this without the build process? I don't know...~ Maybe solved, see MATLAB answers: uk.mathworks.com/matlabcentral/answers/616223","title":"MATLAB (unstable)"},{"location":"other/MATLAB/#custom-message-support","text":"MATLAB HAS NO CUSTOM ROS2 SERVICE SUPPORT Starling uses ROS2 and MAVROS to communicate with the vehicles. MAVROS includes some custom message types that need to be built for MATLAB before it can use them. The main MATLAB documentation is here . The specific steps for Starling are outlined below. Check that your MATLAB installation is setup to be able to work with ROS2. The MATLAB documentation is here . Firstly you need MATLAB's ROS Toolbox. For ROS2, you need to ensure that you are using Python 3. Use pyenv in the MATLAB prompt to check the current setup. If it is not setup to use Python 3, restart MATLAB and use pyenv('Version','3.7') to set the version on Windows. On Linux, use pyenv('Version','/path/to/python') Additionally, you also need a C+ compiler and CMake. Download the MAVROS repo with ROS2 message support from Github. Note that the official repo does not yet include ROS2 support. A modified version is available here: https://github.com/rob-clarke/mavros/tree/mavros2 You can download the code as a ZIP from that site or use git to clone it: sh git clone -b mavros2 https://github.com/rob-clarke/mavros Download the geographic_info repo. Again make sure you have the ROS2 branch: sh git clone -b ros2 https://github.com/ros-geographic-info/geographic_info Copy mavros_msgs from mavros and geographic_msgs from geographic_info to a new folder called matlab_msgs : bash mkdir matlab_msgs cp -r mavros/mavros_msgs matlab_msgs/mavros_msgs cp -r geographic_info/geographic_msgs matlab_msgs/geographic_msgs Clone the unique_identifier_msgs repo into your matlab_msgs folder: git clone https://github.com/ros2/unique_identifier_msgs matlab_msgs/unique_identifier_msgs Use MATLAB's ros2genmsg on the matlab_msgs folder source: matlab ros2genmsg('matlab_msgs')","title":"Custom Message Support"},{"location":"other/MATLAB/#on-ubuntu-18","text":"ros2genmsg failed for me and I had to modify the venv it generates: echo \"export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libexpat.so\" >> ~/.matlab/R2019b/ros2/python_venv/bin/activate It then worked the second time round... See here for an answer from MATLAB support. sudo apt install python3.7 libpython3.7 python3.7-venv sudo ln -s /usr/lib/x86_64-linux-gnu/libpython3.7m.so.1 /usr/lib/libpython3.7m.so.1 sudo ln -s /usr/lib/x86_64-linux-gnu/libpython3.7m.so.1.0 /usr/lib/libpython3.7m.so.1.0","title":"On Ubuntu 18"},{"location":"other/MATLAB/#windows","text":"ros2genmsg apparently needs a very specfic version of Python (namely 3.7). The last available Windows installer for this is Python 3.7.9. See the Python downloads page The ROS Toolbox needs a very specific C++ compiler too: Microsoft Visual C++ 2017. MATLAB documentation on compiler support is here . Visual Studio 2017 can be downloaded from here . You should only need the community edition. Once Visual Studio is installed, run mex -setup cpp in the MATLAB prompt. You should see something like: MEX configured to use 'Microsoft Visual C++ 2017' for C++ language compilation. You also need to have CMake, installed. A binary installer can be found here: cmake.org/download/ I'm considering giving up on Windows at this point File paths might be too long! There may be a registry fix, but we'll just move things to a shorter path! Also MATLAB installs outdated versions of things which break some of the mavros_msgs files. Need to edit mavros_msgs/msg/BatteryStatus.msg to add any random characters next to the unit comments for voltage and current .","title":"Windows"},{"location":"other/MATLAB/#macos","text":"You will need xcode tools installed. Press Cmd-space and search for \"Terminal\". Then type the command below and press enter. xcode-select --install","title":"macOS"},{"location":"other/MATLAB/#deployment","text":"~Once built, the Matlab script calls ros.internal.custommsgs.updatePreferences(msgFullName,{},{},{},'ros2',genDir) , where genDir = fullfile(folderPath, 'matlab_msg_gen', computer('arch')); (e.g. matlab_msgs/matlab_msg_gen/win64 ), and msgFullName is a cell array containing ${PKG_NAME}/${MSG_NAME} strings. How to do this without the build process? I don't know...~ Maybe solved, see MATLAB answers: uk.mathworks.com/matlabcentral/answers/616223","title":"Deployment"},{"location":"other/ideas/","text":"ros.env.d \u00b6 Adding a folder to a /ros.env.d could provide an extendable way to add to the ros environment. e.g. including additional model/plugin paths for Gazebo.","title":"Ideas"},{"location":"other/ideas/#rosenvd","text":"Adding a folder to a /ros.env.d could provide an extendable way to add to the ros environment. e.g. including additional model/plugin paths for Gazebo.","title":"ros.env.d"},{"location":"other/related_projects_and_links/","text":"Project Starling Ecosystem \u00b6 ProjectStarling can be considered an umbrella term for a large ecosystem of projects which are connected by the plug and play modularity of the systems. This page lists some of the key github projects and containers which are used throughout this project. In the future this page can hopefully also link to extension modules which have been developed by users. Contents \u00b6 Project Starling Ecosystem Contents Core Starling Projects and Containers Project Starling Murmuration Starling Simple Offboard Starling Allocator Starling UI/ GUI/ User Interfaces Starling Ui Dashly Starling UI Example Deprecated Starling UI Specific and Applied Projects Coex Clover (clover_ros2_pkgs) Fenswood Farm Core Starling Projects and Containers \u00b6 Project Starling \u00b6 link - https://github.com/UoBFlightLab/ProjectStarling This repository contains the core elements of the system. It contains the source for the following docker containers: uobflightlabstarling/starling-mavros - Core mavros container uobflightlabstarling/rosbridge-suite - Ros websocket bridge (rosbridge-suite) Container uobflightlabstarling/starling-controller-base - Controller Base Container uobflightlabstarling/starling-vicon - Container for vicon In addition it also contains the source for the following containers for simulation uobflightlabstarling/starling-sim-base-core - Base Gazebo simulator uobflightlabstarling/starling-sim-base-px4 - Base container with px4 uobflightlabstarling/starling-sim-px4-sitl - Gazebo container with px4 sitl installed uobflightlabstarling/starling-sim-iris - Base gazebo container with px4 sitl installed and spwawns the iris quadcopter model uobflightlabstarling/starling-sim-ardupilot-copter - Base Container with ArduCopter uobflightlabstarling/starling-sim-ardupilot-gazebo - Gazebo Simulator for use with Ardupilot uobflightlabstarling/starling-sim-iris-ap - Base gazebo container with ardupulot sitl installed and spawns the iris quadcopter model with camera. It also contains some example usage: uobflightlabstarling/example_controller_python Murmuration \u00b6 link - https://github.com/StarlingUAS/Murmuration This repository contains the main Command Line Interface and all of the docker-compose and kubernetes deployment files. These deployment files rely on a number of the containers in this file. Starling Simple Offboard \u00b6 link - https://github.com/mhl787156/starling_simple_offboard This repository contains a basic PX4 simple offboard controller, abstracting away mavros. uobflightlabstarling/starling-simple-offboard Starling Allocator \u00b6 link - https://github.com/mhl787156/starling_allocator This repository contains a trajectory allocator project allocating trajectories to visible mavros vehicles on the network (running starling simple offboard) uobflightlabstarling/starling-allocator Starling UI/ GUI/ User Interfaces \u00b6 Starling Ui Dashly \u00b6 link - https://github.com/mhl787156/starling_ui_dashly This respository contains a more extensible replacement GUI for thie basic starling ui. It can be used to upload trajectories for use with the allocator and simple offboard. uobflightlabstarling/starling-ui-dashly Starling UI Example \u00b6 link - https://github.com/StarlingUAS/starling_ui_example This repository contains an example UI application which uses the ros web-bridge/rosbridge-suite * uobflightlabstarling/starling-example-ui Deprecated Starling UI \u00b6 uobflightlabstarling/starling-ui - Basic UI (deprecated) Specific and Applied Projects \u00b6 Coex Clover (clover_ros2_pkgs) \u00b6 link - https://github.com/UoBFlightLab/clover_ros2_pkgs This repository contains libraries and dockerfile for building the software layer which controls hardware on the Clover drone uobflightlabstarling/starling-clover Fenswood Farm \u00b6 link - https://github.com/StarlingUAS/FenswoodScenario This repository contains the model and docker-compose file for the Fenswood Farm scenario for the University of Bristol Aerial MSc Group Project.","title":"Ecosystem"},{"location":"other/related_projects_and_links/#project-starling-ecosystem","text":"ProjectStarling can be considered an umbrella term for a large ecosystem of projects which are connected by the plug and play modularity of the systems. This page lists some of the key github projects and containers which are used throughout this project. In the future this page can hopefully also link to extension modules which have been developed by users.","title":"Project Starling Ecosystem"},{"location":"other/related_projects_and_links/#contents","text":"Project Starling Ecosystem Contents Core Starling Projects and Containers Project Starling Murmuration Starling Simple Offboard Starling Allocator Starling UI/ GUI/ User Interfaces Starling Ui Dashly Starling UI Example Deprecated Starling UI Specific and Applied Projects Coex Clover (clover_ros2_pkgs) Fenswood Farm","title":"Contents"},{"location":"other/related_projects_and_links/#core-starling-projects-and-containers","text":"","title":"Core Starling Projects and Containers"},{"location":"other/related_projects_and_links/#project-starling","text":"link - https://github.com/UoBFlightLab/ProjectStarling This repository contains the core elements of the system. It contains the source for the following docker containers: uobflightlabstarling/starling-mavros - Core mavros container uobflightlabstarling/rosbridge-suite - Ros websocket bridge (rosbridge-suite) Container uobflightlabstarling/starling-controller-base - Controller Base Container uobflightlabstarling/starling-vicon - Container for vicon In addition it also contains the source for the following containers for simulation uobflightlabstarling/starling-sim-base-core - Base Gazebo simulator uobflightlabstarling/starling-sim-base-px4 - Base container with px4 uobflightlabstarling/starling-sim-px4-sitl - Gazebo container with px4 sitl installed uobflightlabstarling/starling-sim-iris - Base gazebo container with px4 sitl installed and spwawns the iris quadcopter model uobflightlabstarling/starling-sim-ardupilot-copter - Base Container with ArduCopter uobflightlabstarling/starling-sim-ardupilot-gazebo - Gazebo Simulator for use with Ardupilot uobflightlabstarling/starling-sim-iris-ap - Base gazebo container with ardupulot sitl installed and spawns the iris quadcopter model with camera. It also contains some example usage: uobflightlabstarling/example_controller_python","title":"Project Starling"},{"location":"other/related_projects_and_links/#murmuration","text":"link - https://github.com/StarlingUAS/Murmuration This repository contains the main Command Line Interface and all of the docker-compose and kubernetes deployment files. These deployment files rely on a number of the containers in this file.","title":"Murmuration"},{"location":"other/related_projects_and_links/#starling-simple-offboard","text":"link - https://github.com/mhl787156/starling_simple_offboard This repository contains a basic PX4 simple offboard controller, abstracting away mavros. uobflightlabstarling/starling-simple-offboard","title":"Starling Simple Offboard"},{"location":"other/related_projects_and_links/#starling-allocator","text":"link - https://github.com/mhl787156/starling_allocator This repository contains a trajectory allocator project allocating trajectories to visible mavros vehicles on the network (running starling simple offboard) uobflightlabstarling/starling-allocator","title":"Starling Allocator"},{"location":"other/related_projects_and_links/#starling-ui-gui-user-interfaces","text":"","title":"Starling UI/ GUI/ User Interfaces"},{"location":"other/related_projects_and_links/#starling-ui-dashly","text":"link - https://github.com/mhl787156/starling_ui_dashly This respository contains a more extensible replacement GUI for thie basic starling ui. It can be used to upload trajectories for use with the allocator and simple offboard. uobflightlabstarling/starling-ui-dashly","title":"Starling Ui Dashly"},{"location":"other/related_projects_and_links/#starling-ui-example","text":"link - https://github.com/StarlingUAS/starling_ui_example This repository contains an example UI application which uses the ros web-bridge/rosbridge-suite * uobflightlabstarling/starling-example-ui","title":"Starling UI Example"},{"location":"other/related_projects_and_links/#deprecated-starling-ui","text":"uobflightlabstarling/starling-ui - Basic UI (deprecated)","title":"Deprecated Starling UI"},{"location":"other/related_projects_and_links/#specific-and-applied-projects","text":"","title":"Specific and Applied Projects"},{"location":"other/related_projects_and_links/#coex-clover-clover_ros2_pkgs","text":"link - https://github.com/UoBFlightLab/clover_ros2_pkgs This repository contains libraries and dockerfile for building the software layer which controls hardware on the Clover drone uobflightlabstarling/starling-clover","title":"Coex Clover (clover_ros2_pkgs)"},{"location":"other/related_projects_and_links/#fenswood-farm","text":"link - https://github.com/StarlingUAS/FenswoodScenario This repository contains the model and docker-compose file for the Fenswood Farm scenario for the University of Bristol Aerial MSc Group Project.","title":"Fenswood Farm"},{"location":"snippets/drone-dev/","text":"Development on the Drone and Simulator \u00b6 Useful Scripts: \u00b6 There are a number of useful scripts in the /scripts directory of this repository. Scripts can be run from any location, but for this tutorial we assume the user is in the root directory. ./scripts/start_k3s.sh - This starts the cluster -u will uninstall the cluster from the machine (and remove any running processes) -sk will skip the installation check for k3s ./scripts/start_single_px4sitl_gazebo.sh - This starts the starling core functions (Dashboard and UI). It also starts the gazebo simulator and a 'drone' process running the PX4 SITL and a connected Mavros ROS2 node. Assumes the cluster has been started. -d will stop the gazebo and all connected 'drone' processes only (use if reloading the controller). -r will restart the gazebo and all connected 'drone' processes. labels: app: nginx -sk will skip the starting/check that the starling core functions are running. -ow will automatically open the UI webpages. ./scripts/start_starling_base.sh - This starts the starling user interface and dashboard. Automatically run by start_single_px4sitl_gazebo.sh ./scripts/start_dashboard.sh - This starts the dashboard process on the cluster. Assumes the cluster has already been started. Automatically run by start_single_px4sitl_gazebo.sh and start_starling_base.sh -ow will automatically open the UI webpages. Note: The ./run_k3s script internally runs start_k3s.sh and start_single_px4sitl_gazebo.sh . Any options passed will be forwarded to the relevant script. Modifying the example controller \u00b6 In the controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package can be built by running make in the controllers directory. This will use colcon build to build the node and output a local image named example_controller_python . Inside the controllers folder, there is an annotated kubernetes config file k8.example_controller_python.amd64.yaml . This specifies the deployment of a pod which contains your local example_controller_python image ( this line ). Similar to before you can start up the local example controller by using the script: ./scripts/start_example_controller.sh But if you have made a copy, or wish to run your own version of the configuration, you can manually deploy your controller by running the following: k3s apply -f k8.example_controller_python.amd64.yaml k3s delete -f k8.example_controller_python.amd64.yaml # To delete See kubernetes configuration for more details. For debugging, you can both see the logs, and execute on your controller container through the dashboard. See instructions here Inside you can source install/setup.bash and run ROS2 commands like normal. Creating your own from scratch \u00b6 Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. 3. A Kubernetes YAML config file specifying either a Pod or a deployment. Use the example k8.example_controller_python.amd64.yaml as a template. There are annoated comments. Also see here for more details. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> . Once built, the configuration file must have a container config specifying your own image name. Your container can then be deployed manually using kubectl -f <your config>.yaml as above.","title":"Drone dev"},{"location":"snippets/drone-dev/#development-on-the-drone-and-simulator","text":"","title":"Development on the Drone and Simulator"},{"location":"snippets/drone-dev/#useful-scripts","text":"There are a number of useful scripts in the /scripts directory of this repository. Scripts can be run from any location, but for this tutorial we assume the user is in the root directory. ./scripts/start_k3s.sh - This starts the cluster -u will uninstall the cluster from the machine (and remove any running processes) -sk will skip the installation check for k3s ./scripts/start_single_px4sitl_gazebo.sh - This starts the starling core functions (Dashboard and UI). It also starts the gazebo simulator and a 'drone' process running the PX4 SITL and a connected Mavros ROS2 node. Assumes the cluster has been started. -d will stop the gazebo and all connected 'drone' processes only (use if reloading the controller). -r will restart the gazebo and all connected 'drone' processes. labels: app: nginx -sk will skip the starting/check that the starling core functions are running. -ow will automatically open the UI webpages. ./scripts/start_starling_base.sh - This starts the starling user interface and dashboard. Automatically run by start_single_px4sitl_gazebo.sh ./scripts/start_dashboard.sh - This starts the dashboard process on the cluster. Assumes the cluster has already been started. Automatically run by start_single_px4sitl_gazebo.sh and start_starling_base.sh -ow will automatically open the UI webpages. Note: The ./run_k3s script internally runs start_k3s.sh and start_single_px4sitl_gazebo.sh . Any options passed will be forwarded to the relevant script.","title":"Useful Scripts:"},{"location":"snippets/drone-dev/#modifying-the-example-controller","text":"In the controllers folder there is an example_controller_python which you should have seen in action in the example above. The ROS2 package is in example_controller_python . Any edits made to the ROS2 package can be built by running make in the controllers directory. This will use colcon build to build the node and output a local image named example_controller_python . Inside the controllers folder, there is an annotated kubernetes config file k8.example_controller_python.amd64.yaml . This specifies the deployment of a pod which contains your local example_controller_python image ( this line ). Similar to before you can start up the local example controller by using the script: ./scripts/start_example_controller.sh But if you have made a copy, or wish to run your own version of the configuration, you can manually deploy your controller by running the following: k3s apply -f k8.example_controller_python.amd64.yaml k3s delete -f k8.example_controller_python.amd64.yaml # To delete See kubernetes configuration for more details. For debugging, you can both see the logs, and execute on your controller container through the dashboard. See instructions here Inside you can source install/setup.bash and run ROS2 commands like normal.","title":"Modifying the example controller"},{"location":"snippets/drone-dev/#creating-your-own-from-scratch","text":"Of course you can create your own controller from scratch. Inside your controller repository, the following is required 1. Your ROS2 package folder (what would usually go inside the dev_ws/src directory) 2. A Dockerfile (named Dockerfile ) which is dervied FROM uobflightlabstarling/starling-controller-base , use the example Dockerfile as a template. 3. A Kubernetes YAML config file specifying either a Pod or a deployment. Use the example k8.example_controller_python.amd64.yaml as a template. There are annoated comments. Also see here for more details. Your Dockerfile can be built by running the following in the directory with the Dockerfile. docker build -t <name of your controller> . Once built, the configuration file must have a container config specifying your own image name. Your container can then be deployed manually using kubectl -f <your config>.yaml as above.","title":"Creating your own from scratch"},{"location":"snippets/onboard-control/","text":"","title":"Onboard control"},{"location":"tutorials/introduction/","text":"Introduction to Starling with the Fenswood Scenario \u00b6 This Tutorial aims to give an introductory overview of the Starling system through the use of the Fenswood Scenario example. It will take a student from the basics of Linux, ROS and Docker to hopefully being able to run the Fenswood Scenario simulation environment with a simple controller flying the drone. This tutorial was written for the University of Bristol Aerial MSc Group Project 2021. On this page: Introduction to Starling with the Fenswood Scenario A Brief Introduction to Linux What is Linux The Terminal Navigating the file system Working with files Installing Dependencies and Useful Programs Installing Git and VSCode sudo Installing Docker Starling and Docker What is the purpose of Starling What 'magic' is docker doing Running the an Example Configuration Getting the Example Scenario Running the Example Scenario A Brief Introduction to ROS Why does ROS exist? What is ROS ROS concepts through an example ROS2 for Starling Running the Example UAV Controller Getting the Example Controller Running the Example Controller What is the Example Controller Inspecting and Debugging Starling with Docker and ROS Inspecting Docker Inspecting ROS2 Summary and Conclusion A Brief Introduction to Linux \u00b6 Adapted from this digital ocean tutorial What is Linux \u00b6 Linux is a family of free and open-source operating systems based on the Linux kernel (core operating system). Operating systems based on Linux are known as Linux distributions or distros. Examples include Debian, Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. The Linux kernel has been under active development since 1991, and has proven to be extremely versatile and adaptable. You can find computers that run Linux in a wide variety of contexts all over the world, from web servers to cell phones. Today, 90% of all cloud infrastructure and 74% of the world\u2019s smartphones are powered by Linux. However, newcomers to Linux may find it somewhat difficult to approach, as Linux filesystems have a different structure than those found on Windows or MacOS. Additionally, Linux-based operating systems depend heavily on working with the command line interface, while most personal computers rely on graphical interfaces. The Terminal \u00b6 The terms \u201cterminal,\u201d \u201cshell,\u201d and \u201ccommand line interface\u201d are often used interchangeably, but there are subtle differences between them: A terminal is an input and output environment that presents a text-only window running a shell. A shell is a program that exposes the computer\u2019s operating system to a user or program. In Linux systems, the shell presented in a terminal is a command line interpreter. The default shell in Ubuntu Linux is known as bash . A command line interface is a user interface (managed by a command line interpreter program) which processes commands to a computer program and outputs the results. When someone refers to one of these three terms in the context of Linux, they generally mean a terminal environment where you can run commands and see the results printed out to the terminal, such as this: There are two ways to open a terminal: Pressing the Win or Cmd key to open the program menu and typing terminal , then pressing Enter Pressing Ctrl + Alt + T This default terminal is known as the 'gnome-terminal'. Other terminals exist such as 'terminator' Becoming a Linux user requires you to be comfortable with using a terminal. Any administrative task, including file manipulation, package installation, and user management, can be accomplished through the terminal. The terminal is interactive: you specify commands to run (after the $ sign) and the terminal outputs the results of those commands. To execute any command, you type it into the prompt and press Enter . When using the Starling system, interacting with Docker and ROS2, you'll most often be doing so through a terminal shell. Although personal computers that run Linux often come with the kind of graphical desktop environment familiar to most computer users, it is often more efficient or practical to perform certain tasks through commands entered into the terminal. As of writing, a GUI (Graphical User Interface) has not been developed for Starling, and so almost all tasks have to be achieved through the terminal shell. A basic command to try out is echo , which will print things to the terminal. For example echo hello-world will print hello-world into the terminal. You can also use it to observe the value of Environment Variables which record and keep useful variables to the operation of the Operating System. For example, when you run a command in bash , bash will look for the command executable in the locations provided by the environment variable PATH . You can print the contents of this env-var using echo $PATH . The $ before the name of the variable tells bash that the following word represents an environment variable, and that it should be looked up. Navigating the file system \u00b6 Like Windows and Mac, the Linux filesystems are based on a directory tree. This means that you can create directories (which are functionally identical to folders found in other operating systems) inside other directories, and files can exist in any directory. The forward slash ( / ) is used to indicate the root directory in the filesystem hierarchy. When a user logs in to the shell, they are brought to their own user directory, stored within /home/<username> . This is referred to as the user\u2019s home directory. Often you may see the tilde ( ~ ) character when specifying a file location (e.g. ~/Documents/hello.txt = /home/<username>/Documents/hello.txt ). This is shorthand for the user's home directory and gets substituted in when used. To see what directory you are currently active in you can run the pwd command, which stands for \u201cprint working directory\u201d myuser@my-machine:~$ pwd /home/myuser To see a list of files and directories that exist in your current working directory, run the ls command: myuser@my-machine:~$ ls Desktop Documents Downloads Pictures Public Wallpapers You can get more details if you run ls -al command: myuser@my-machine:~$ ls -al drwxr-xr-x 2 myuser myuser 4096 Apr 30 2021 Desktop drwxrwxr-x 8 myuser myuser 4096 Oct 29 09:27 Documents drwxrwxr-x 8 myuser myuser 4096 Dec 10 14:41 Downloads drwxrwxr-x 8 myuser myuser 4096 May 23 10:43 Pictures drwxrwxr-x 8 myuser myuser 4096 Jan 19 2017 Public drwxrwxr-x 8 myuser myuser 4096 Oct 15 09:43 Wallpapers You can create one or more new directories within your current working directory with the mkdir command, which stands for \u201cmake directory\u201d. For example, to create two new directories named testdir1 and testdir2, you might run the first command. You can create nested directories by using the -p option: myuser@my-machine:~$ mkdir testdir1 testdir2 myuser@my-machine:~$ mkdir -p testdir3/testsubdir To navigate into one of these new directories, run the cd command (which stands for \u201cchange directory\u201d) and specify the directory\u2019s name: myuser@my-machine:~$ cd testdir1 myuser@my-machine:~/testdir1$ Note that you can navigate from anywhere to anywhere. cd only requires a valid filepath. Note also that . represents the current folder and .. represents the parent folder. Note also how is shows the current working directory in the shell as well. cd # This will bring you back to home directory cd testdir3/testsubdir # Brings you into testsubdir cd ../ # Brings you back out one level into testdir3 cd ../testdir1 # Brings you back out one level and back into testdir1 cd /home/<username>/testdir2 # Absolute reference to testdir2 cd ~/testdir2 # Absolute reference using tilde to testdir2 Working with files \u00b6 You cannot use cd to interact with files; cd stands for \u201cchange directory\u201d, and only allows you to navigate directories. You can, however, create, edit, and view the contents of files. One way to create a file is with the touch command. This creates an empty file in your current working directory. To create a new file called file.txt: touch file.txt If you decide to rename file.txt later on, you can do so with the mv command. mv stands for \u201cmove\u201d and it can move a file or directory from one place to another. By specifying the original file, file.txt, you can \u201cmove\u201d it to a new location in the current working directory, thereby renaming it. mv file.txt newfile.txt It is also possible to copy a file to a new location with the cp command. If we want to copy newfile.txt, you can make a copy of newfile.txt named newfile_copy.txt like this: cp newfile.txt newfile_copy.txt However, files are not of much use if they don\u2019t contain anything. To edit files, a file editor is necessary. There are many options for file editors, all created by professionals for daily use. Such editors include vim, emacs, nano, and pico. nano is a suitable option for beginners: it is relatively user-friendly and doesn\u2019t overload you with cryptic options or commands. nano file.txt This will open a space where you can start typing to edit the file. In nano specifically you can save your written text by pressing Ctrl + X , Y , and then Enter . This returns you to the shell with a newly saved file.txt . Now that file.txt has some text within it, you can view it using cat or less . The cat command prints the contents of a specified file to your system\u2019s output. Try running cat and pass the file.txt file you just edited as an argument: cat file.txt Using cat to view file contents can be unwieldy and difficult to read if the file is particularly long. As an alternative, you can use the less command which will allow you to paginate the output. Use less to view the contents of the file.txt file, like this: less file.txt This will also print the contents of file.txt, but one terminal page at a time beginning at the start of the file. You can use the spacebar to advance a page, or the arrow keys to go up and down one line at a time. Press Q to quit out of less . Finally, to delete the file.txt file, pass the name of the file as an argument to rm : rm file.txt rm -d directory rmidr directory rm -r directory # If the directory you are deleting is not empty NOTE : If your question has to do with a specific Linux command, the manual pages offer detailed and insightful documentation for nearly every command. To see the man page for any command, pass the command\u2019s name as an argument to the man command - man command . For instance, man rm displays the purpose of rm , how to use it, what options are available, examples of use, and more useful information. NOTE : If a command fails or is hanging or you just want to stop it, most of the time you can stop the running process by pressing Ctrl + C . This will send a Keyboard Interrupt message to the program and hopefully stop it. Installing Dependencies and Useful Programs \u00b6 Like windows and mac, individual programs can be manually downloaded (usually as a tar.gz file instead of exe ) and manually installed into your operating system (using dpkg ). However, the linux project offers a much more straight forward method through the apt (Advanced Packaging Tool) utility. apt is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian operating systems like Ubuntu. (For other distributions you may come across equivalents like yum ). This is the primary method for installing software onto your system. To use apt , and more specifically apt-get which 'gets' programs for you, you must first run the update command to get the current list of all available software. Note that because sudo is used, you will most likely need to input your password. sudo will be explained below. sudo apt-get update Note that it will hang (stop responding) or fail if you are not connected to the internet. Installing Git and VSCode \u00b6 You can then install your programs using apt-get install . For Starling, you will need to use the git version control software to both download Starling and eventually build your own controllers. To install git , run the following: sudo apt-get install git We also recommend the use of Visual Studio Code as your development environment or text editor, but you are free to use whatever you want (atom, notepad++ etc etc). We heavily make use of it during development and recommend a number of extensions. VScode can be installed using the snap utility. snap is a slightly more modern successor to apt for more general programs. snap comes ready installed on your linux distrubtion. sudo snap install code --classic sudo \u00b6 Now in these commands, we have prefixed all of them with sudo . sudo these days usually stands for superuser do and allows a command to be run with the privileges of the superuser (aka the root user), if the user has been given permissions to do so. Any command which installs or modifies directories outside of the users home directory will often need superuser privileges to avoid non-superusers from changing things they shouldn't. As the above commands all seek to install programs to the system, they need superuser permissions to do so. Running without sudo will give you a permission error. Running a command with sudo will ask you for your own accounts password. Installing Docker \u00b6 For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker's repository: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (there are security issues with this, but it doesn't matter for running Starling locally) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes Verify that it was successful (we will come back to this command): docker run hello-world That is Docker on Linux installed. See the original page for any further details. Starling and Docker \u00b6 So now we have installed this thing called Docker - but what is it, and why does it matter to you as a user of Starling. For now, we recommend you treat docker as a special program that runs programs which we have written for you to use. For the purposes of this session we will not yet go into the details - we'll leave that for a future session, but we can at least give you a flavour of what Docker has allowed us to do. What is the purpose of Starling \u00b6 The purpose of Starling, for the use of this course, is to allow you to quickly and easily install and run a UAV simulation within a simulated environment, so that you can test your developed controllers against a semi-realistic scenario. Therefore Starling is a set of pre-built programs/executables, some of which are pre-configured for the following: Running a Physics Simulation with Visualisation Running the Drone autopilot control software locally (a.k.a Software In The Loop or SITL) Running the interface between Mavlink and other protocols such as the Robot Operating System (ROS) And many others... These pre-built programs, which we refer to as containers , are all available in the StarlingUAS repository on github . Together these containers form a modular ecosystem of drone systems which can be composed together to develop software for real drones. Any controllers developed via the simulator can be directly ported to run on a real drone. What 'magic' is docker doing \u00b6 Perhaps it is easiest to explain what we (and you) would have to do if docker did not exist! Each of the elements described above uses its own programs. For example the physics simulator is a program called gazebo . On each of your machines you would have to manually install gazebo and configure it to work. Then you would have to do the same for SITL, Mavlink, ROS and so on, and then make sure all of them work together - configuration files, scripts etc etc. And still this is without considering how to write software which will run on a drone on a different computer altogther (we don't want the 'but sir, it worked on my machine' syndrome!). With docker, we have essentially packaged up the software with all of it's dependencies and configurations to provide only the specific service we want it to provide. We have already figured out how to install and configure, say, gazebo, and how to get it running with everything else, put those instructions together and released it to you as a container . Almost as importantly, because the installation and configuration is baked into the container, the system itself runs in a consistent and replicatable environment , hopefully allowing reproducable results each time. This can be different from running 'bare metal' on your local machine where you may have accidentally changed a configuration without knowing - leading to things going wrong. All of this means that it's less work for you as the user, as you don't need to go through the difficulties of set-up and installation of all the many individual components. And it's better for us as we can get down to the interesting stuff quicker making life better for everyone! Running the an Example Configuration \u00b6 A separate respository has been created that provides a configuration to simulate flight at the Univeristy of Bristol's Fenswood Facility for Aerial Research Measurement (FARM). We will use this as an example. The Fenswood scenario repository is here: https://github.com/StarlingUAS/FenswoodScenario This section takes you through step by step as to how to download and run the example, and a bit of information as to what the example contains. This information is also in the example repository README. Getting the Example Scenario \u00b6 The example is packaged as github repository. For information on what is github see this article . In order to run the example scenario, you will first need to 'clone' the repository locally. Therefore, navigate to a location in the file system where you want to store the repository (e.g. cd ~/Documents ) and run the following: git clone https://github.com/StarlingUAS/FenswoodScenario.git Note : this is why we needed to install git earlier using apt This will download the repository folder into your local directory. Navigate into the folder using cd : myuser@my-machine:~/Documents$ cd FenswoodScenario myuser@my-machine:~/Documents/FenswoodScenario$ ls README.md docker-compose.windows.yml foxglove_layout.json docker-compose.linux.yml fenswood Then you will need to download the associated starling containers to run the simulator and SITL. Thankfully, this is fairly straightforward and can be run by doing the following: myuser@my-machine:~/Documents/FenswoodScenario$ docker-compose -f docker-compose.linux.yml pull Pulling simhost ... Pulling sitl ... Pulling mavros ... Pulling rosbridge-suite ... Pulling ui Breaking down this command, docker-compose is a utility which allows us to compose together multiple docker containers, i.e. run multiple containers together. -f docker-compose.linux.yml specifies that docker-compose should read the configuration yaml file given by -f . docker-compose.linux.yml is a configuration file we have written which specifies the containers and configurations required to run the fenswood scenario. In this file, we have speciifed that 4 containers will be run. pull specifies that we would like to download the required containers locally, but not run them just yet. This command may take between 20 to 30 minutes as it has to download some large content - the simulator itself is around 5Gb! It is recommended you keep reading and have a look at section 5.4 (a brief introduction to ROS) while waiting. The downloaded containers are not stored in your local directory, instead they are stored by docker in a local library on your own machine. All downloaded and local containers can be seen by typing. myuser@my-machine:~/Documents/FenswoodScenario$ docker images Running the Example Scenario \u00b6 Once the download of pull has completed, you can run the scenario. To run the example scenario, it is a similar command except now you ask it to up : myuser@my-machine:~/Documents/FenswoodScenario$ docker-compose -f docker-compose.linux.yml up This will start 4 containers: simhost : The physics simulator instance which also hosts the simulated drone model and the target spawning. sitl : The software in the loop representing the onboard autopilot software. mavros : This is the bridge between Mavlink and ROS. It also serves to send mavlink messages to a ground station. rosbridge-suite : This is a ros-web interface that is placed there for future use. ui : This is the example ui. Wait a minute or so for all of the containers to initialise. Once started, the simulator should be available to see in the browser. Open up the firefox browser (click on firefox on the sidebar, or press Win and search for firefox) Go to localhost:8080 You should see something like the following The web interface is known as Gazebo Web. You can fly your camera through the environment like many other 3D modelling software: Left Click on a location and drag does Panning/Translating Middle Click on a location and drag does rotating around a point Scrolling the Middle Mouse button will zoom you in and out. The full process as a gif: A Brief Introduction to ROS \u00b6 This sections is adapted from this article ROS stands for the Robot Operating System, yet it isn't an actual operating system. It's a framework designed to expedite the development time of robot platforms. To understand what ROS is, we should understand why ROS exists in the first place. Why does ROS exist? \u00b6 In general, software developers avoid hardware like the plague. It's messy, doesn't have consistent behavior, and there's no ctrl-z in sight. Most beginner programmers think you have to have a deep knowledge of electronics and even mechanics to program robots. They think that the hardware and software are so tightly coupled, you have to know both in depth to build anything useful. Software developers became software developers for a reason, so they don't have to deal with hardware. For example, lets say you have to debug a faulty sensor. You first have to take out the sensor from the enclosure, test the sensor thoroughly with a multi meter and various test cases, document its behavior, then examine the hardware -level code to ensure that there were no bugs, and so on. That's a lot of interaction with the hardware that's not fun for someone who just wants to write some cool software. It's harder to attract good programmers if the programming is coupled deeply with hardware. This is where ROS comes into play. With ROS, you can completely abstract the hardware from software, and instead interact with an API that gives access to that data. You can forget about the hardware, and focus on developing the software that makes the robot do what you want. What is ROS \u00b6 ROS is essentially a framework that sits on top of an operating system which defines how particular ROS compatible programs communicate and share data with each other. Essentially ROS defines an interface between which compatible programs can communicate and interact with each other. Over the years that ROS has existed, many people have developed thousands of ROS compatible packages which can be used in a modular fashion. ROS concepts through an example \u00b6 To make it more concrete, imagine that on your drone you have a camera. There are also two processes which require, as inputs, that camera image. Say, a machine learning program, and a position estimation program. Traditionally, you would have to manually serialise (compress) and stream the image over a port which the other two programs could read from. But if the port changes or, say, the camera changes, lots of things have to be reconfigured. However, this sort of interaction can be made streamlined in ROS. Let us consider the programs we have as ROS nodes , i.e. a program which is responsible for one single modular purpose, with particular inputs or outputs: A camera image streaming node OUT: camera image A machine vision system for recognising objects IN: camera image OUT: list of recognised objects A simultaneous localisation and mapping system. IN: camera image OUT: vehicle position These outputs of a node define ROS topics , i.e. a single stream of one type of data. Each topic has a particular name which can be referred to. In our example, some of the topics might be: /drone/camera for the camera image /drone/recognised_objects for the machine vision system /drone/slam_position for the SLAM system Then, we see that there are two avenues of communication created from these node inputs and outputs. graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Machine Vision] node --in--> D[SLAM] style node fill:#f9f,stroke:#333,stroke-width:4px Now ROS follows a publisher/subscriber model of communication. What that means is that nodes publish data to topics as outputs. But that data is only sent across the network if a different nodes also subscribes to the same topic. So in our example we end up having A camera image streaming node OUT: publishing to /drone/camera A machine vision system for recognising objects IN: subscribed to /drone/camera OUT: publishing to /drone/recognised_objects A simultaneous localisation and mapping system. IN: subscribed to /drone/camera OUT: publishing to /drone/slam_position graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Vision] C -->|out| node1[drone/recognised_objects] node --in--> D[SLAM] D -->|out| node2[drone/slam_position] style node fill:#f9f,stroke:#333,stroke-width:4px style node1 fill:#f9f,stroke:#333,stroke-width:4px style node2 fill:#f9f,stroke:#333,stroke-width:4px Finally, the data that is sent is not just anything. The data or message is a specifically templated packet of data containing things specified for that paricular use case. In our example for /drone/slam_position topic, the message might be of type geometry_msgs/msg/Point.msg which is defined like so: # This contains the position of a point in free space float64 x float64 y float64 z In other words the message that the /drone/slam_position topic publishes must have a msg.x , msg.y and msg.z field, and the subscriber will only receivea message with those fields. There are a number of messages in the standard ROS library, but many libraries also define their own - as have we in some parts of Starling. This can be summarised in this diagram from the ROS tutorials demonstrates it very nicely: The bottom half of this shows how topics get sent from a publisher to a subscriber. Interestingly, if you put two topics together, you get some notion of two way communication. This is the basis of a service which can be seen in the top of the diagram. A service is made of a Request topic and a Response topic, but functions as a single communication type to the user. Similar to messages, a service has a defined request and response types (e.g. see std_srvs/srv/SetBool.srv ). A service request will often wait until a response is received before continuing. Note that everything happens asyncronously and in parallel, when a node subscribes or sends a requests, it doesn't know when the response will arrive. It only knows it will (hopefully) arrive at some point. When a packet is received the subscriber can then run a method - this method is usually known as a callback , but that will be covered in a later tutorial. So in summary, the key conepts and terminology are: Nodes Topics Publishers and Subscribers Messages Services ROS2 for Starling \u00b6 There are 2 versions of ROS - ROS1 and ROS2. ROS1, initially created in 2007 by Willow Garage, has become huge among the open source robotics community. However over the years they realised that there are a number of important features which are missing - and adding all of these would simply break ROS1. Also the most recent ROS1 distribution (ROS Noetic) is soon to reach the end of its supported life (EOL 2025) with no more ROS1 there after! (See this article for more details!) Therefore, to future proof the system, and to ensure all users get a well rounded experience that will hopefully translate to industry experience, Starling has been implemented in ROS2. Specifically, Starling uses the Foxy Fitzroy Long Term Support (LTS) distribution throughout. There are some interesting changes between ROS1 and ROS2, but the core elements described above remain identical. A future tutorial will go into a few more details, but this is probably all the context you will need for now! Note: Main thing to be aware of is if you are debugging and searching for ROS questions on the internet, be aware that there are many existing questions for ROS1 which will no longer apply for ROS2. Running the Example UAV Controller \u00b6 The next step in getting the Fenswood Scenario up and running is to actually get the UAV to do something. The previous steps simply started the simulator up, but now we have to instruct and control the UAV. For now we have provided an example controller to all students (to be turned into a tutorial at a later date) The example controller repository is here: https://github.com/StarlingUAS/example_python_controller This section will take you through step by step as to how to download and run the example, a bit of information about what the example contains. This information is also available in the example repository readme. Getting the Example Controller \u00b6 Similar to the simulator, the example controller is packaged up as a github repository. This repository contains a number of example controllers for use in different scenarios and simulators. There is one particular controller which has been developed for the fenswood scenario. Again, in order to run the example scenario, you will first need to 'clone' the repository locally. Therefore navigate to a location in the file system where you want to store the repository (e.g. cd ~/Documents ) and run the following: git clone https://github.com/StarlingUAS/example_python_controller.git cd example_python_controller # Navigate into the folder Now unlike the simulator, you will not need to download the example from the internet. This example is set up so that it will build itself locally when it is run. Running the Example Controller \u00b6 First, open up a terminal and start up the Fenswood Scenario Simulator if you haven't already (Refer to the section above ). Double check it is open by going to Gazebo Web at localhost:8080 Then, open up a second terminal and navigate to the example_python_controller folder. The example controller can be started by running the following: myuser@my-machine:~/Documents/example_python_controller$ docker-compose -f docker-compose.fenswood.ap.yaml up With this command, the controller will start off and attempt to find a drone on the network. If a drone has not been found, try restarting the both the Fenswood Scenario and the controller. Once a drone has been found, it will attempt to connect to the ardupilot. Once connected the path the vehicle will take is initialised and it is ready to fly if the user sends a mission go. Now to send a mission go, you can use the provided simple UI that is started with the FenswoodScenario. Navigate to localhost:3000 in the browser, and a UI will start up: If it says connected to rosbridge in green, then you are ready to send a mission start command. Press the Green GO Button. If you go to the terminal it should start sending waypoints. In the UI the camera image should start to change as the vehicle takes off, and you should see the vehicle takeoff in gazebo web as well. Then you should hopefully see its attempt to follow a preset trajectory towards the target location. This full process can be seen in the following gif: The landing: What is the Example Controller \u00b6 So what exactly is the example controller doing? Lets first start with what it is communicating with. On a real drone, the thing that controls the lower level operations of the drone is the Autopilot or Flight controller . The autopilot contains software which allows for different flight modes and translates hight level commands to motor voltages to fly the drone. As you might have come across, the primary with of controlling the autopilot is through sending Mavlink messages. Now the autopilot software itself can be swapped out and changed. For this scenario, we use the Ardupilot Arducopter firmware for the flight controller. It is important to know the specific type of software as different flight controller firmware requires the use of different flight mode and instructions. The Fenswood Scenario simulator utilises the Ardupilot Software In The Loop (SITL) simulator. This program is identical to the firmware that would be running onboard the flight controller. Then, within the example controller repository, there are two example controllers - one for Ardupilot (suffixed with ap ), and one for the PX4 firmware (suffixed with px4 ). You can use the former to communicate with the Ardupilot SITL. The example controller talks in ROS2 to the SITL via the Mavros translation node mentioned earlier. It sends commands for things like 'takeoff', 'go there' and 'land' via topics which the Mavros node advertises. The Mavros node takes subscribes to these topics and re-publishes them to the Flight Controller using Mavlink in a way Ardupilot understands. More details will be given in the next tutorial about how this controller works under the hood, and how to develop your own. Inspecting and Debugging Starling with Docker and ROS \u00b6 Now we have the full example Fenswood Scenario running, we can start to inspect what's going on a bit more! This will be more useful when developing your own controller, but this will give you an idea of whether your systems are working or not. Inspecting Docker \u00b6 As you will hopefully be aware now, there's a fair amount going on under the hood! Because of all of these pieces which need to fit together, occasionally things fail, and therefore you will need to be able to inspect whats going on. As most of Starling relys on using Docker we can use some of the docker commands to inspect what is going on! It is useful to know what docker containers exist locally on your system and whether they are the most recent release. The following command will list your local docker containers images and when they were last released myuser@my-machine:~$ docker images uobflightlabstarling/starling-sim-iris-ap latest 02ba82372286 6 weeks ago 5.07GB uobflightlabstarling/starling-sim-iris <none> 826dab23768b 6 weeks ago 5.84GB uobflightlabstarling/starling-sim-ardupilot-gazebo latest df39b5d0181e 6 weeks ago 5.05GB uobflightlabstarling/starling-sim-ardupilot-copter <none> 6a10c0c95714 6 weeks ago 2.35GB uobflightlabstarling/starling-mavros <none> 3a5142abfc0c 6 weeks ago 2.04GB uobflightlabstarling/rosbridge-suite latest 97a05ece1aa3 6 weeks ago 887MB uobflightlabstarling/starling-ui-example latest 83e9a37ddbdf 6 weeks ago 1.33GB To see what containers are running, you can use the following. When running Fenswood you should see 5 containers running. myuser@my-machine:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dfea0bbf9b69 uobflightlabstarling/starling-ui-example \"/ros_entrypoint.sh \u2026\" 3 hours ago Up 9 minutes fenswoodscenario_ui-example_1 1b72f667a557 uobflightlabstarling/starling-mavros:latest \"/ros_entrypoint.sh \u2026\" 3 hours ago Up 9 minutes fenswoodscenario_mavros_1 9d55eb4de60e uobflightlabstarling/rosbridge-suite:latest \"/ros_entrypoint.sh \u2026\" 3 hours ago Up 9 minutes fenswoodscenario_rosbridge-suite_1 5205bcf5495f example_python_controller_controller \"/ros_entrypoint.sh \u2026\" 3 weeks ago Up 8 minutes example_python_controller_controller_1 9fa66d47e21e uobflightlabstarling/starling-sim-ardupilot-copter:latest \"/home/root/entrypoi\u2026\" 4 weeks ago Up 9 minutes fenswoodscenario_sitl_1 e5e47cc36ec1 uobflightlabstarling/starling-sim-iris-ap:latest \"/ros_entrypoint.sh \u2026\" 5 weeks ago Up 9 minutes fenswoodscenario_simhost_1 When running docker ps, it shows some useful information. You can use this information to inspect the inside of a container. Now what do we mean by the 'inside of a container'. Essentially the container allows us to run a pre-set set of instructions inside a blank mini version of linux. When we are debugging, it is sometimes really useful to have a look at what is going on inside this mini version of linux! The exec command allows us to go inside one of these containers to make changes directly. Note that when we are inside, we are no longer in your own version of the desktop, and that changes made are persistent inside the container! When inside the container, you can run some of the ROS2 comamnds in the next section. docker exec -it <container id> bash e.g. myuser@my-machine:~$ docker exec -it 1b72f667a557 bash root@my-machine:/ros_ws# If a particular docker container is not working properly, you can also kill a container: docker kill <container id> Inspecting ROS2 \u00b6 As mentioned before, everything in starling is running ROS2. Therefore all of the ROS2 nodes, topics and services can be inspected and observed. We can do this inspection using the following few commands. First ensure that you have docker exec into any of the containers. For example using the container id of the container labelled starling-mavros . Once you are inside, you first need to run the following to enable ROS2 commands. ( Tab autocompleting is available) root@my-machine:~$ source /opt/ros/foxy/setup.bash The first thing you can do is list all of the ROS2 nodes in the network node command: root@my-machine:~$ ros2 node list WARNING: Be aware that are nodes in the graph that share an exact name, this can have unintended side effects. /gazebo /rosapi /rosapi /rosbridge_websocket /vehicle_1/camera_controller /vehicle_1/example_controller /vehicle_1/gimbal_small_2d /vehicle_1/ros_bridge This will show a list of all the nodes that ROS2 can find. You should see all of the nodes from the simulator and the example controller. Then, we can inspect the list of available topics using the topic command. root@my-machine:~$ ros2 topic list /client_count /clock /connected_clients /emergency_stop /mission_start /parameter_events /performance_metrics /rosout /vehicle_1/camera/camera_info /vehicle_1/camera/image_raw /vehicle_1/camera/image_raw/compressed /vehicle_1/camera/image_raw/compressedDepth /vehicle_1/camera/image_raw/theora /vehicle_1/gimbal_tilt_cmd /vehicle_1/gimbal_tilt_status /vehicle_1/mavlink/from /vehicle_1/mavlink/to /vehicle_1/mavros/battery /vehicle_1/mavros/distance_sensor/hrlv_ez4_sonar /vehicle_1/mavros/distance_sensor/lidarlite_laser /vehicle_1/mavros/distance_sensor/rangefinder /vehicle_1/mavros/distance_sensor/temperature /vehicle_1/mavros/global_position/global /vehicle_1/mavros/global_position/velocity /vehicle_1/mavros/image/camera_image /vehicle_1/mavros/local_position/pose /vehicle_1/mavros/manual_control/control /vehicle_1/mavros/manual_control/send /vehicle_1/mavros/mission/reached /vehicle_1/mavros/mission/waypoints /vehicle_1/mavros/px4flow/ground_distance /vehicle_1/mavros/px4flow/raw/optical_flow_rad /vehicle_1/mavros/safety_area /vehicle_1/mavros/setpoint_accel/accel /vehicle_1/mavros/setpoint_attitude/attitude /vehicle_1/mavros/setpoint_attitude/cmd_vel /vehicle_1/mavros/setpoint_attitude/thrust /vehicle_1/mavros/setpoint_position/global /vehicle_1/mavros/setpoint_position/global_to_local /vehicle_1/mavros/setpoint_position/local /vehicle_1/mavros/setpoint_raw/attitude /vehicle_1/mavros/setpoint_raw/global /vehicle_1/mavros/setpoint_raw/local /vehicle_1/mavros/setpoint_velocity/cmd_vel_unstamped /vehicle_1/mavros/state /vehicle_1/mavros/vision_pose/pose /vehicle_1/mavros/vision_pose/pose_cov /vehicle_1/mavros/vision_speed/speed_twist /vehicle_1/mavros/vision_speed/speed_vector If there is a particular topic you want to inspect, you can use the echo command of topic , for example if we wanted to inspect the topic /vehicle_1/mavros/state we can run: root@my-machine:~$ ros2 topic echo /vehicle_1/mavros/state header: stamp: sec: 1639423734 nanosec: 393874329 frame_id: '' connected: true armed: false guided: true manual_input: true mode: GUIDED system_status: 3 --- header: stamp: sec: 1639423735 nanosec: 435107761 frame_id: '' connected: true armed: false guided: true manual_input: true mode: GUIDED system_status: 3 --- ^C Note: Press Ctrl + C to stop the echo stream This should (assuming the connection to all of the other elements is working) start printing out the current state of the drone. Including whether it is armed or not, and which mode it is currently in. Finally, you can also inspect the services on the network using the service command like the following: root@my-machine:~$ ros2 service list ... /unpause_physics /vehicle_1/camera_controller/describe_parameters /vehicle_1/camera_controller/get_parameter_types /vehicle_1/camera_controller/get_parameters /vehicle_1/camera_controller/list_parameters /vehicle_1/camera_controller/set_parameters /vehicle_1/camera_controller/set_parameters_atomically /vehicle_1/example_controller/describe_parameters /vehicle_1/example_controller/get_parameter_types /vehicle_1/example_controller/get_parameters /vehicle_1/example_controller/list_parameters /vehicle_1/example_controller/set_parameters /vehicle_1/example_controller/set_parameters_atomically /vehicle_1/gimbal_small_2d/describe_parameters /vehicle_1/gimbal_small_2d/get_parameter_types /vehicle_1/gimbal_small_2d/get_parameters /vehicle_1/gimbal_small_2d/list_parameters /vehicle_1/gimbal_small_2d/set_parameters /vehicle_1/gimbal_small_2d/set_parameters_atomically /vehicle_1/mavros/cmd/arming /vehicle_1/mavros/cmd/command /vehicle_1/mavros/cmd/command_int /vehicle_1/mavros/cmd/land /vehicle_1/mavros/cmd/set_home /vehicle_1/mavros/cmd/takeoff /vehicle_1/mavros/cmd/trigger_control /vehicle_1/mavros/mission/clear /vehicle_1/mavros/mission/pull /vehicle_1/mavros/mission/push /vehicle_1/mavros/mission/set_current /vehicle_1/mavros/set_mode /vehicle_1/mavros/set_stream_rate /vehicle_1/ros_bridge/describe_parameters /vehicle_1/ros_bridge/get_parameter_types /vehicle_1/ros_bridge/get_parameters /vehicle_1/ros_bridge/list_parameters /vehicle_1/ros_bridge/set_parameters /vehicle_1/ros_bridge/set_parameters_atomically /vehicle_1/set_camera_info Summary and Conclusion \u00b6 Congratulations! You have gotten to the end of this tutorial! Especially if this is your first time touching linux and using this stuff, a lot of the content here can be daunting - but feel free to read it over again and hopefully you will slowly come to understand what is going on. In this introductory tutorial, we have only really scratched the surface of a lot of the technologies that we've been using. If you wish to have a deeper understanding, I would recommend having a read through the official documentation of all of them. The docker and ROS2 tutorials are really good. But I wish to emphasise that you will hopefully not need an extremely deep understanding to use Starling. The next step after this tutorial is then to cover how you might actually build your own controller. This will go into more detail about Mavlink, ROS2 and Docker - including how to build your own projects and run them against the Fenswood Scenario Simulator!","title":"1. Introduction to Starling"},{"location":"tutorials/introduction/#introduction-to-starling-with-the-fenswood-scenario","text":"This Tutorial aims to give an introductory overview of the Starling system through the use of the Fenswood Scenario example. It will take a student from the basics of Linux, ROS and Docker to hopefully being able to run the Fenswood Scenario simulation environment with a simple controller flying the drone. This tutorial was written for the University of Bristol Aerial MSc Group Project 2021. On this page: Introduction to Starling with the Fenswood Scenario A Brief Introduction to Linux What is Linux The Terminal Navigating the file system Working with files Installing Dependencies and Useful Programs Installing Git and VSCode sudo Installing Docker Starling and Docker What is the purpose of Starling What 'magic' is docker doing Running the an Example Configuration Getting the Example Scenario Running the Example Scenario A Brief Introduction to ROS Why does ROS exist? What is ROS ROS concepts through an example ROS2 for Starling Running the Example UAV Controller Getting the Example Controller Running the Example Controller What is the Example Controller Inspecting and Debugging Starling with Docker and ROS Inspecting Docker Inspecting ROS2 Summary and Conclusion","title":"Introduction to Starling with the Fenswood Scenario"},{"location":"tutorials/introduction/#a-brief-introduction-to-linux","text":"Adapted from this digital ocean tutorial","title":"A Brief Introduction to Linux"},{"location":"tutorials/introduction/#what-is-linux","text":"Linux is a family of free and open-source operating systems based on the Linux kernel (core operating system). Operating systems based on Linux are known as Linux distributions or distros. Examples include Debian, Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. The Linux kernel has been under active development since 1991, and has proven to be extremely versatile and adaptable. You can find computers that run Linux in a wide variety of contexts all over the world, from web servers to cell phones. Today, 90% of all cloud infrastructure and 74% of the world\u2019s smartphones are powered by Linux. However, newcomers to Linux may find it somewhat difficult to approach, as Linux filesystems have a different structure than those found on Windows or MacOS. Additionally, Linux-based operating systems depend heavily on working with the command line interface, while most personal computers rely on graphical interfaces.","title":"What is Linux"},{"location":"tutorials/introduction/#the-terminal","text":"The terms \u201cterminal,\u201d \u201cshell,\u201d and \u201ccommand line interface\u201d are often used interchangeably, but there are subtle differences between them: A terminal is an input and output environment that presents a text-only window running a shell. A shell is a program that exposes the computer\u2019s operating system to a user or program. In Linux systems, the shell presented in a terminal is a command line interpreter. The default shell in Ubuntu Linux is known as bash . A command line interface is a user interface (managed by a command line interpreter program) which processes commands to a computer program and outputs the results. When someone refers to one of these three terms in the context of Linux, they generally mean a terminal environment where you can run commands and see the results printed out to the terminal, such as this: There are two ways to open a terminal: Pressing the Win or Cmd key to open the program menu and typing terminal , then pressing Enter Pressing Ctrl + Alt + T This default terminal is known as the 'gnome-terminal'. Other terminals exist such as 'terminator' Becoming a Linux user requires you to be comfortable with using a terminal. Any administrative task, including file manipulation, package installation, and user management, can be accomplished through the terminal. The terminal is interactive: you specify commands to run (after the $ sign) and the terminal outputs the results of those commands. To execute any command, you type it into the prompt and press Enter . When using the Starling system, interacting with Docker and ROS2, you'll most often be doing so through a terminal shell. Although personal computers that run Linux often come with the kind of graphical desktop environment familiar to most computer users, it is often more efficient or practical to perform certain tasks through commands entered into the terminal. As of writing, a GUI (Graphical User Interface) has not been developed for Starling, and so almost all tasks have to be achieved through the terminal shell. A basic command to try out is echo , which will print things to the terminal. For example echo hello-world will print hello-world into the terminal. You can also use it to observe the value of Environment Variables which record and keep useful variables to the operation of the Operating System. For example, when you run a command in bash , bash will look for the command executable in the locations provided by the environment variable PATH . You can print the contents of this env-var using echo $PATH . The $ before the name of the variable tells bash that the following word represents an environment variable, and that it should be looked up.","title":"The Terminal"},{"location":"tutorials/introduction/#navigating-the-file-system","text":"Like Windows and Mac, the Linux filesystems are based on a directory tree. This means that you can create directories (which are functionally identical to folders found in other operating systems) inside other directories, and files can exist in any directory. The forward slash ( / ) is used to indicate the root directory in the filesystem hierarchy. When a user logs in to the shell, they are brought to their own user directory, stored within /home/<username> . This is referred to as the user\u2019s home directory. Often you may see the tilde ( ~ ) character when specifying a file location (e.g. ~/Documents/hello.txt = /home/<username>/Documents/hello.txt ). This is shorthand for the user's home directory and gets substituted in when used. To see what directory you are currently active in you can run the pwd command, which stands for \u201cprint working directory\u201d myuser@my-machine:~$ pwd /home/myuser To see a list of files and directories that exist in your current working directory, run the ls command: myuser@my-machine:~$ ls Desktop Documents Downloads Pictures Public Wallpapers You can get more details if you run ls -al command: myuser@my-machine:~$ ls -al drwxr-xr-x 2 myuser myuser 4096 Apr 30 2021 Desktop drwxrwxr-x 8 myuser myuser 4096 Oct 29 09:27 Documents drwxrwxr-x 8 myuser myuser 4096 Dec 10 14:41 Downloads drwxrwxr-x 8 myuser myuser 4096 May 23 10:43 Pictures drwxrwxr-x 8 myuser myuser 4096 Jan 19 2017 Public drwxrwxr-x 8 myuser myuser 4096 Oct 15 09:43 Wallpapers You can create one or more new directories within your current working directory with the mkdir command, which stands for \u201cmake directory\u201d. For example, to create two new directories named testdir1 and testdir2, you might run the first command. You can create nested directories by using the -p option: myuser@my-machine:~$ mkdir testdir1 testdir2 myuser@my-machine:~$ mkdir -p testdir3/testsubdir To navigate into one of these new directories, run the cd command (which stands for \u201cchange directory\u201d) and specify the directory\u2019s name: myuser@my-machine:~$ cd testdir1 myuser@my-machine:~/testdir1$ Note that you can navigate from anywhere to anywhere. cd only requires a valid filepath. Note also that . represents the current folder and .. represents the parent folder. Note also how is shows the current working directory in the shell as well. cd # This will bring you back to home directory cd testdir3/testsubdir # Brings you into testsubdir cd ../ # Brings you back out one level into testdir3 cd ../testdir1 # Brings you back out one level and back into testdir1 cd /home/<username>/testdir2 # Absolute reference to testdir2 cd ~/testdir2 # Absolute reference using tilde to testdir2","title":"Navigating the file system"},{"location":"tutorials/introduction/#working-with-files","text":"You cannot use cd to interact with files; cd stands for \u201cchange directory\u201d, and only allows you to navigate directories. You can, however, create, edit, and view the contents of files. One way to create a file is with the touch command. This creates an empty file in your current working directory. To create a new file called file.txt: touch file.txt If you decide to rename file.txt later on, you can do so with the mv command. mv stands for \u201cmove\u201d and it can move a file or directory from one place to another. By specifying the original file, file.txt, you can \u201cmove\u201d it to a new location in the current working directory, thereby renaming it. mv file.txt newfile.txt It is also possible to copy a file to a new location with the cp command. If we want to copy newfile.txt, you can make a copy of newfile.txt named newfile_copy.txt like this: cp newfile.txt newfile_copy.txt However, files are not of much use if they don\u2019t contain anything. To edit files, a file editor is necessary. There are many options for file editors, all created by professionals for daily use. Such editors include vim, emacs, nano, and pico. nano is a suitable option for beginners: it is relatively user-friendly and doesn\u2019t overload you with cryptic options or commands. nano file.txt This will open a space where you can start typing to edit the file. In nano specifically you can save your written text by pressing Ctrl + X , Y , and then Enter . This returns you to the shell with a newly saved file.txt . Now that file.txt has some text within it, you can view it using cat or less . The cat command prints the contents of a specified file to your system\u2019s output. Try running cat and pass the file.txt file you just edited as an argument: cat file.txt Using cat to view file contents can be unwieldy and difficult to read if the file is particularly long. As an alternative, you can use the less command which will allow you to paginate the output. Use less to view the contents of the file.txt file, like this: less file.txt This will also print the contents of file.txt, but one terminal page at a time beginning at the start of the file. You can use the spacebar to advance a page, or the arrow keys to go up and down one line at a time. Press Q to quit out of less . Finally, to delete the file.txt file, pass the name of the file as an argument to rm : rm file.txt rm -d directory rmidr directory rm -r directory # If the directory you are deleting is not empty NOTE : If your question has to do with a specific Linux command, the manual pages offer detailed and insightful documentation for nearly every command. To see the man page for any command, pass the command\u2019s name as an argument to the man command - man command . For instance, man rm displays the purpose of rm , how to use it, what options are available, examples of use, and more useful information. NOTE : If a command fails or is hanging or you just want to stop it, most of the time you can stop the running process by pressing Ctrl + C . This will send a Keyboard Interrupt message to the program and hopefully stop it.","title":"Working with files"},{"location":"tutorials/introduction/#installing-dependencies-and-useful-programs","text":"Like windows and mac, individual programs can be manually downloaded (usually as a tar.gz file instead of exe ) and manually installed into your operating system (using dpkg ). However, the linux project offers a much more straight forward method through the apt (Advanced Packaging Tool) utility. apt is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian operating systems like Ubuntu. (For other distributions you may come across equivalents like yum ). This is the primary method for installing software onto your system. To use apt , and more specifically apt-get which 'gets' programs for you, you must first run the update command to get the current list of all available software. Note that because sudo is used, you will most likely need to input your password. sudo will be explained below. sudo apt-get update Note that it will hang (stop responding) or fail if you are not connected to the internet.","title":"Installing Dependencies and Useful Programs"},{"location":"tutorials/introduction/#installing-git-and-vscode","text":"You can then install your programs using apt-get install . For Starling, you will need to use the git version control software to both download Starling and eventually build your own controllers. To install git , run the following: sudo apt-get install git We also recommend the use of Visual Studio Code as your development environment or text editor, but you are free to use whatever you want (atom, notepad++ etc etc). We heavily make use of it during development and recommend a number of extensions. VScode can be installed using the snap utility. snap is a slightly more modern successor to apt for more general programs. snap comes ready installed on your linux distrubtion. sudo snap install code --classic","title":"Installing Git and VSCode"},{"location":"tutorials/introduction/#sudo","text":"Now in these commands, we have prefixed all of them with sudo . sudo these days usually stands for superuser do and allows a command to be run with the privileges of the superuser (aka the root user), if the user has been given permissions to do so. Any command which installs or modifies directories outside of the users home directory will often need superuser privileges to avoid non-superusers from changing things they shouldn't. As the above commands all seek to install programs to the system, they need superuser permissions to do so. Running without sudo will give you a permission error. Running a command with sudo will ask you for your own accounts password.","title":"sudo"},{"location":"tutorials/introduction/#installing-docker","text":"For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker's repository: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (there are security issues with this, but it doesn't matter for running Starling locally) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes Verify that it was successful (we will come back to this command): docker run hello-world That is Docker on Linux installed. See the original page for any further details.","title":"Installing Docker"},{"location":"tutorials/introduction/#starling-and-docker","text":"So now we have installed this thing called Docker - but what is it, and why does it matter to you as a user of Starling. For now, we recommend you treat docker as a special program that runs programs which we have written for you to use. For the purposes of this session we will not yet go into the details - we'll leave that for a future session, but we can at least give you a flavour of what Docker has allowed us to do.","title":"Starling and Docker"},{"location":"tutorials/introduction/#what-is-the-purpose-of-starling","text":"The purpose of Starling, for the use of this course, is to allow you to quickly and easily install and run a UAV simulation within a simulated environment, so that you can test your developed controllers against a semi-realistic scenario. Therefore Starling is a set of pre-built programs/executables, some of which are pre-configured for the following: Running a Physics Simulation with Visualisation Running the Drone autopilot control software locally (a.k.a Software In The Loop or SITL) Running the interface between Mavlink and other protocols such as the Robot Operating System (ROS) And many others... These pre-built programs, which we refer to as containers , are all available in the StarlingUAS repository on github . Together these containers form a modular ecosystem of drone systems which can be composed together to develop software for real drones. Any controllers developed via the simulator can be directly ported to run on a real drone.","title":"What is the purpose of Starling"},{"location":"tutorials/introduction/#what-magic-is-docker-doing","text":"Perhaps it is easiest to explain what we (and you) would have to do if docker did not exist! Each of the elements described above uses its own programs. For example the physics simulator is a program called gazebo . On each of your machines you would have to manually install gazebo and configure it to work. Then you would have to do the same for SITL, Mavlink, ROS and so on, and then make sure all of them work together - configuration files, scripts etc etc. And still this is without considering how to write software which will run on a drone on a different computer altogther (we don't want the 'but sir, it worked on my machine' syndrome!). With docker, we have essentially packaged up the software with all of it's dependencies and configurations to provide only the specific service we want it to provide. We have already figured out how to install and configure, say, gazebo, and how to get it running with everything else, put those instructions together and released it to you as a container . Almost as importantly, because the installation and configuration is baked into the container, the system itself runs in a consistent and replicatable environment , hopefully allowing reproducable results each time. This can be different from running 'bare metal' on your local machine where you may have accidentally changed a configuration without knowing - leading to things going wrong. All of this means that it's less work for you as the user, as you don't need to go through the difficulties of set-up and installation of all the many individual components. And it's better for us as we can get down to the interesting stuff quicker making life better for everyone!","title":"What 'magic' is docker doing"},{"location":"tutorials/introduction/#running-the-an-example-configuration","text":"A separate respository has been created that provides a configuration to simulate flight at the Univeristy of Bristol's Fenswood Facility for Aerial Research Measurement (FARM). We will use this as an example. The Fenswood scenario repository is here: https://github.com/StarlingUAS/FenswoodScenario This section takes you through step by step as to how to download and run the example, and a bit of information as to what the example contains. This information is also in the example repository README.","title":"Running the an Example Configuration"},{"location":"tutorials/introduction/#getting-the-example-scenario","text":"The example is packaged as github repository. For information on what is github see this article . In order to run the example scenario, you will first need to 'clone' the repository locally. Therefore, navigate to a location in the file system where you want to store the repository (e.g. cd ~/Documents ) and run the following: git clone https://github.com/StarlingUAS/FenswoodScenario.git Note : this is why we needed to install git earlier using apt This will download the repository folder into your local directory. Navigate into the folder using cd : myuser@my-machine:~/Documents$ cd FenswoodScenario myuser@my-machine:~/Documents/FenswoodScenario$ ls README.md docker-compose.windows.yml foxglove_layout.json docker-compose.linux.yml fenswood Then you will need to download the associated starling containers to run the simulator and SITL. Thankfully, this is fairly straightforward and can be run by doing the following: myuser@my-machine:~/Documents/FenswoodScenario$ docker-compose -f docker-compose.linux.yml pull Pulling simhost ... Pulling sitl ... Pulling mavros ... Pulling rosbridge-suite ... Pulling ui Breaking down this command, docker-compose is a utility which allows us to compose together multiple docker containers, i.e. run multiple containers together. -f docker-compose.linux.yml specifies that docker-compose should read the configuration yaml file given by -f . docker-compose.linux.yml is a configuration file we have written which specifies the containers and configurations required to run the fenswood scenario. In this file, we have speciifed that 4 containers will be run. pull specifies that we would like to download the required containers locally, but not run them just yet. This command may take between 20 to 30 minutes as it has to download some large content - the simulator itself is around 5Gb! It is recommended you keep reading and have a look at section 5.4 (a brief introduction to ROS) while waiting. The downloaded containers are not stored in your local directory, instead they are stored by docker in a local library on your own machine. All downloaded and local containers can be seen by typing. myuser@my-machine:~/Documents/FenswoodScenario$ docker images","title":"Getting the Example Scenario"},{"location":"tutorials/introduction/#running-the-example-scenario","text":"Once the download of pull has completed, you can run the scenario. To run the example scenario, it is a similar command except now you ask it to up : myuser@my-machine:~/Documents/FenswoodScenario$ docker-compose -f docker-compose.linux.yml up This will start 4 containers: simhost : The physics simulator instance which also hosts the simulated drone model and the target spawning. sitl : The software in the loop representing the onboard autopilot software. mavros : This is the bridge between Mavlink and ROS. It also serves to send mavlink messages to a ground station. rosbridge-suite : This is a ros-web interface that is placed there for future use. ui : This is the example ui. Wait a minute or so for all of the containers to initialise. Once started, the simulator should be available to see in the browser. Open up the firefox browser (click on firefox on the sidebar, or press Win and search for firefox) Go to localhost:8080 You should see something like the following The web interface is known as Gazebo Web. You can fly your camera through the environment like many other 3D modelling software: Left Click on a location and drag does Panning/Translating Middle Click on a location and drag does rotating around a point Scrolling the Middle Mouse button will zoom you in and out. The full process as a gif:","title":"Running the Example Scenario"},{"location":"tutorials/introduction/#a-brief-introduction-to-ros","text":"This sections is adapted from this article ROS stands for the Robot Operating System, yet it isn't an actual operating system. It's a framework designed to expedite the development time of robot platforms. To understand what ROS is, we should understand why ROS exists in the first place.","title":"A Brief Introduction to ROS"},{"location":"tutorials/introduction/#why-does-ros-exist","text":"In general, software developers avoid hardware like the plague. It's messy, doesn't have consistent behavior, and there's no ctrl-z in sight. Most beginner programmers think you have to have a deep knowledge of electronics and even mechanics to program robots. They think that the hardware and software are so tightly coupled, you have to know both in depth to build anything useful. Software developers became software developers for a reason, so they don't have to deal with hardware. For example, lets say you have to debug a faulty sensor. You first have to take out the sensor from the enclosure, test the sensor thoroughly with a multi meter and various test cases, document its behavior, then examine the hardware -level code to ensure that there were no bugs, and so on. That's a lot of interaction with the hardware that's not fun for someone who just wants to write some cool software. It's harder to attract good programmers if the programming is coupled deeply with hardware. This is where ROS comes into play. With ROS, you can completely abstract the hardware from software, and instead interact with an API that gives access to that data. You can forget about the hardware, and focus on developing the software that makes the robot do what you want.","title":"Why does ROS exist?"},{"location":"tutorials/introduction/#what-is-ros","text":"ROS is essentially a framework that sits on top of an operating system which defines how particular ROS compatible programs communicate and share data with each other. Essentially ROS defines an interface between which compatible programs can communicate and interact with each other. Over the years that ROS has existed, many people have developed thousands of ROS compatible packages which can be used in a modular fashion.","title":"What is ROS"},{"location":"tutorials/introduction/#ros-concepts-through-an-example","text":"To make it more concrete, imagine that on your drone you have a camera. There are also two processes which require, as inputs, that camera image. Say, a machine learning program, and a position estimation program. Traditionally, you would have to manually serialise (compress) and stream the image over a port which the other two programs could read from. But if the port changes or, say, the camera changes, lots of things have to be reconfigured. However, this sort of interaction can be made streamlined in ROS. Let us consider the programs we have as ROS nodes , i.e. a program which is responsible for one single modular purpose, with particular inputs or outputs: A camera image streaming node OUT: camera image A machine vision system for recognising objects IN: camera image OUT: list of recognised objects A simultaneous localisation and mapping system. IN: camera image OUT: vehicle position These outputs of a node define ROS topics , i.e. a single stream of one type of data. Each topic has a particular name which can be referred to. In our example, some of the topics might be: /drone/camera for the camera image /drone/recognised_objects for the machine vision system /drone/slam_position for the SLAM system Then, we see that there are two avenues of communication created from these node inputs and outputs. graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Machine Vision] node --in--> D[SLAM] style node fill:#f9f,stroke:#333,stroke-width:4px Now ROS follows a publisher/subscriber model of communication. What that means is that nodes publish data to topics as outputs. But that data is only sent across the network if a different nodes also subscribes to the same topic. So in our example we end up having A camera image streaming node OUT: publishing to /drone/camera A machine vision system for recognising objects IN: subscribed to /drone/camera OUT: publishing to /drone/recognised_objects A simultaneous localisation and mapping system. IN: subscribed to /drone/camera OUT: publishing to /drone/slam_position graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Vision] C -->|out| node1[drone/recognised_objects] node --in--> D[SLAM] D -->|out| node2[drone/slam_position] style node fill:#f9f,stroke:#333,stroke-width:4px style node1 fill:#f9f,stroke:#333,stroke-width:4px style node2 fill:#f9f,stroke:#333,stroke-width:4px Finally, the data that is sent is not just anything. The data or message is a specifically templated packet of data containing things specified for that paricular use case. In our example for /drone/slam_position topic, the message might be of type geometry_msgs/msg/Point.msg which is defined like so: # This contains the position of a point in free space float64 x float64 y float64 z In other words the message that the /drone/slam_position topic publishes must have a msg.x , msg.y and msg.z field, and the subscriber will only receivea message with those fields. There are a number of messages in the standard ROS library, but many libraries also define their own - as have we in some parts of Starling. This can be summarised in this diagram from the ROS tutorials demonstrates it very nicely: The bottom half of this shows how topics get sent from a publisher to a subscriber. Interestingly, if you put two topics together, you get some notion of two way communication. This is the basis of a service which can be seen in the top of the diagram. A service is made of a Request topic and a Response topic, but functions as a single communication type to the user. Similar to messages, a service has a defined request and response types (e.g. see std_srvs/srv/SetBool.srv ). A service request will often wait until a response is received before continuing. Note that everything happens asyncronously and in parallel, when a node subscribes or sends a requests, it doesn't know when the response will arrive. It only knows it will (hopefully) arrive at some point. When a packet is received the subscriber can then run a method - this method is usually known as a callback , but that will be covered in a later tutorial. So in summary, the key conepts and terminology are: Nodes Topics Publishers and Subscribers Messages Services","title":"ROS concepts through an example"},{"location":"tutorials/introduction/#ros2-for-starling","text":"There are 2 versions of ROS - ROS1 and ROS2. ROS1, initially created in 2007 by Willow Garage, has become huge among the open source robotics community. However over the years they realised that there are a number of important features which are missing - and adding all of these would simply break ROS1. Also the most recent ROS1 distribution (ROS Noetic) is soon to reach the end of its supported life (EOL 2025) with no more ROS1 there after! (See this article for more details!) Therefore, to future proof the system, and to ensure all users get a well rounded experience that will hopefully translate to industry experience, Starling has been implemented in ROS2. Specifically, Starling uses the Foxy Fitzroy Long Term Support (LTS) distribution throughout. There are some interesting changes between ROS1 and ROS2, but the core elements described above remain identical. A future tutorial will go into a few more details, but this is probably all the context you will need for now! Note: Main thing to be aware of is if you are debugging and searching for ROS questions on the internet, be aware that there are many existing questions for ROS1 which will no longer apply for ROS2.","title":"ROS2 for Starling"},{"location":"tutorials/introduction/#running-the-example-uav-controller","text":"The next step in getting the Fenswood Scenario up and running is to actually get the UAV to do something. The previous steps simply started the simulator up, but now we have to instruct and control the UAV. For now we have provided an example controller to all students (to be turned into a tutorial at a later date) The example controller repository is here: https://github.com/StarlingUAS/example_python_controller This section will take you through step by step as to how to download and run the example, a bit of information about what the example contains. This information is also available in the example repository readme.","title":"Running the Example UAV Controller"},{"location":"tutorials/introduction/#getting-the-example-controller","text":"Similar to the simulator, the example controller is packaged up as a github repository. This repository contains a number of example controllers for use in different scenarios and simulators. There is one particular controller which has been developed for the fenswood scenario. Again, in order to run the example scenario, you will first need to 'clone' the repository locally. Therefore navigate to a location in the file system where you want to store the repository (e.g. cd ~/Documents ) and run the following: git clone https://github.com/StarlingUAS/example_python_controller.git cd example_python_controller # Navigate into the folder Now unlike the simulator, you will not need to download the example from the internet. This example is set up so that it will build itself locally when it is run.","title":"Getting the Example Controller"},{"location":"tutorials/introduction/#running-the-example-controller","text":"First, open up a terminal and start up the Fenswood Scenario Simulator if you haven't already (Refer to the section above ). Double check it is open by going to Gazebo Web at localhost:8080 Then, open up a second terminal and navigate to the example_python_controller folder. The example controller can be started by running the following: myuser@my-machine:~/Documents/example_python_controller$ docker-compose -f docker-compose.fenswood.ap.yaml up With this command, the controller will start off and attempt to find a drone on the network. If a drone has not been found, try restarting the both the Fenswood Scenario and the controller. Once a drone has been found, it will attempt to connect to the ardupilot. Once connected the path the vehicle will take is initialised and it is ready to fly if the user sends a mission go. Now to send a mission go, you can use the provided simple UI that is started with the FenswoodScenario. Navigate to localhost:3000 in the browser, and a UI will start up: If it says connected to rosbridge in green, then you are ready to send a mission start command. Press the Green GO Button. If you go to the terminal it should start sending waypoints. In the UI the camera image should start to change as the vehicle takes off, and you should see the vehicle takeoff in gazebo web as well. Then you should hopefully see its attempt to follow a preset trajectory towards the target location. This full process can be seen in the following gif: The landing:","title":"Running the Example Controller"},{"location":"tutorials/introduction/#what-is-the-example-controller","text":"So what exactly is the example controller doing? Lets first start with what it is communicating with. On a real drone, the thing that controls the lower level operations of the drone is the Autopilot or Flight controller . The autopilot contains software which allows for different flight modes and translates hight level commands to motor voltages to fly the drone. As you might have come across, the primary with of controlling the autopilot is through sending Mavlink messages. Now the autopilot software itself can be swapped out and changed. For this scenario, we use the Ardupilot Arducopter firmware for the flight controller. It is important to know the specific type of software as different flight controller firmware requires the use of different flight mode and instructions. The Fenswood Scenario simulator utilises the Ardupilot Software In The Loop (SITL) simulator. This program is identical to the firmware that would be running onboard the flight controller. Then, within the example controller repository, there are two example controllers - one for Ardupilot (suffixed with ap ), and one for the PX4 firmware (suffixed with px4 ). You can use the former to communicate with the Ardupilot SITL. The example controller talks in ROS2 to the SITL via the Mavros translation node mentioned earlier. It sends commands for things like 'takeoff', 'go there' and 'land' via topics which the Mavros node advertises. The Mavros node takes subscribes to these topics and re-publishes them to the Flight Controller using Mavlink in a way Ardupilot understands. More details will be given in the next tutorial about how this controller works under the hood, and how to develop your own.","title":"What is the Example Controller"},{"location":"tutorials/introduction/#inspecting-and-debugging-starling-with-docker-and-ros","text":"Now we have the full example Fenswood Scenario running, we can start to inspect what's going on a bit more! This will be more useful when developing your own controller, but this will give you an idea of whether your systems are working or not.","title":"Inspecting and Debugging Starling with Docker and ROS"},{"location":"tutorials/introduction/#inspecting-docker","text":"As you will hopefully be aware now, there's a fair amount going on under the hood! Because of all of these pieces which need to fit together, occasionally things fail, and therefore you will need to be able to inspect whats going on. As most of Starling relys on using Docker we can use some of the docker commands to inspect what is going on! It is useful to know what docker containers exist locally on your system and whether they are the most recent release. The following command will list your local docker containers images and when they were last released myuser@my-machine:~$ docker images uobflightlabstarling/starling-sim-iris-ap latest 02ba82372286 6 weeks ago 5.07GB uobflightlabstarling/starling-sim-iris <none> 826dab23768b 6 weeks ago 5.84GB uobflightlabstarling/starling-sim-ardupilot-gazebo latest df39b5d0181e 6 weeks ago 5.05GB uobflightlabstarling/starling-sim-ardupilot-copter <none> 6a10c0c95714 6 weeks ago 2.35GB uobflightlabstarling/starling-mavros <none> 3a5142abfc0c 6 weeks ago 2.04GB uobflightlabstarling/rosbridge-suite latest 97a05ece1aa3 6 weeks ago 887MB uobflightlabstarling/starling-ui-example latest 83e9a37ddbdf 6 weeks ago 1.33GB To see what containers are running, you can use the following. When running Fenswood you should see 5 containers running. myuser@my-machine:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dfea0bbf9b69 uobflightlabstarling/starling-ui-example \"/ros_entrypoint.sh \u2026\" 3 hours ago Up 9 minutes fenswoodscenario_ui-example_1 1b72f667a557 uobflightlabstarling/starling-mavros:latest \"/ros_entrypoint.sh \u2026\" 3 hours ago Up 9 minutes fenswoodscenario_mavros_1 9d55eb4de60e uobflightlabstarling/rosbridge-suite:latest \"/ros_entrypoint.sh \u2026\" 3 hours ago Up 9 minutes fenswoodscenario_rosbridge-suite_1 5205bcf5495f example_python_controller_controller \"/ros_entrypoint.sh \u2026\" 3 weeks ago Up 8 minutes example_python_controller_controller_1 9fa66d47e21e uobflightlabstarling/starling-sim-ardupilot-copter:latest \"/home/root/entrypoi\u2026\" 4 weeks ago Up 9 minutes fenswoodscenario_sitl_1 e5e47cc36ec1 uobflightlabstarling/starling-sim-iris-ap:latest \"/ros_entrypoint.sh \u2026\" 5 weeks ago Up 9 minutes fenswoodscenario_simhost_1 When running docker ps, it shows some useful information. You can use this information to inspect the inside of a container. Now what do we mean by the 'inside of a container'. Essentially the container allows us to run a pre-set set of instructions inside a blank mini version of linux. When we are debugging, it is sometimes really useful to have a look at what is going on inside this mini version of linux! The exec command allows us to go inside one of these containers to make changes directly. Note that when we are inside, we are no longer in your own version of the desktop, and that changes made are persistent inside the container! When inside the container, you can run some of the ROS2 comamnds in the next section. docker exec -it <container id> bash e.g. myuser@my-machine:~$ docker exec -it 1b72f667a557 bash root@my-machine:/ros_ws# If a particular docker container is not working properly, you can also kill a container: docker kill <container id>","title":"Inspecting Docker"},{"location":"tutorials/introduction/#inspecting-ros2","text":"As mentioned before, everything in starling is running ROS2. Therefore all of the ROS2 nodes, topics and services can be inspected and observed. We can do this inspection using the following few commands. First ensure that you have docker exec into any of the containers. For example using the container id of the container labelled starling-mavros . Once you are inside, you first need to run the following to enable ROS2 commands. ( Tab autocompleting is available) root@my-machine:~$ source /opt/ros/foxy/setup.bash The first thing you can do is list all of the ROS2 nodes in the network node command: root@my-machine:~$ ros2 node list WARNING: Be aware that are nodes in the graph that share an exact name, this can have unintended side effects. /gazebo /rosapi /rosapi /rosbridge_websocket /vehicle_1/camera_controller /vehicle_1/example_controller /vehicle_1/gimbal_small_2d /vehicle_1/ros_bridge This will show a list of all the nodes that ROS2 can find. You should see all of the nodes from the simulator and the example controller. Then, we can inspect the list of available topics using the topic command. root@my-machine:~$ ros2 topic list /client_count /clock /connected_clients /emergency_stop /mission_start /parameter_events /performance_metrics /rosout /vehicle_1/camera/camera_info /vehicle_1/camera/image_raw /vehicle_1/camera/image_raw/compressed /vehicle_1/camera/image_raw/compressedDepth /vehicle_1/camera/image_raw/theora /vehicle_1/gimbal_tilt_cmd /vehicle_1/gimbal_tilt_status /vehicle_1/mavlink/from /vehicle_1/mavlink/to /vehicle_1/mavros/battery /vehicle_1/mavros/distance_sensor/hrlv_ez4_sonar /vehicle_1/mavros/distance_sensor/lidarlite_laser /vehicle_1/mavros/distance_sensor/rangefinder /vehicle_1/mavros/distance_sensor/temperature /vehicle_1/mavros/global_position/global /vehicle_1/mavros/global_position/velocity /vehicle_1/mavros/image/camera_image /vehicle_1/mavros/local_position/pose /vehicle_1/mavros/manual_control/control /vehicle_1/mavros/manual_control/send /vehicle_1/mavros/mission/reached /vehicle_1/mavros/mission/waypoints /vehicle_1/mavros/px4flow/ground_distance /vehicle_1/mavros/px4flow/raw/optical_flow_rad /vehicle_1/mavros/safety_area /vehicle_1/mavros/setpoint_accel/accel /vehicle_1/mavros/setpoint_attitude/attitude /vehicle_1/mavros/setpoint_attitude/cmd_vel /vehicle_1/mavros/setpoint_attitude/thrust /vehicle_1/mavros/setpoint_position/global /vehicle_1/mavros/setpoint_position/global_to_local /vehicle_1/mavros/setpoint_position/local /vehicle_1/mavros/setpoint_raw/attitude /vehicle_1/mavros/setpoint_raw/global /vehicle_1/mavros/setpoint_raw/local /vehicle_1/mavros/setpoint_velocity/cmd_vel_unstamped /vehicle_1/mavros/state /vehicle_1/mavros/vision_pose/pose /vehicle_1/mavros/vision_pose/pose_cov /vehicle_1/mavros/vision_speed/speed_twist /vehicle_1/mavros/vision_speed/speed_vector If there is a particular topic you want to inspect, you can use the echo command of topic , for example if we wanted to inspect the topic /vehicle_1/mavros/state we can run: root@my-machine:~$ ros2 topic echo /vehicle_1/mavros/state header: stamp: sec: 1639423734 nanosec: 393874329 frame_id: '' connected: true armed: false guided: true manual_input: true mode: GUIDED system_status: 3 --- header: stamp: sec: 1639423735 nanosec: 435107761 frame_id: '' connected: true armed: false guided: true manual_input: true mode: GUIDED system_status: 3 --- ^C Note: Press Ctrl + C to stop the echo stream This should (assuming the connection to all of the other elements is working) start printing out the current state of the drone. Including whether it is armed or not, and which mode it is currently in. Finally, you can also inspect the services on the network using the service command like the following: root@my-machine:~$ ros2 service list ... /unpause_physics /vehicle_1/camera_controller/describe_parameters /vehicle_1/camera_controller/get_parameter_types /vehicle_1/camera_controller/get_parameters /vehicle_1/camera_controller/list_parameters /vehicle_1/camera_controller/set_parameters /vehicle_1/camera_controller/set_parameters_atomically /vehicle_1/example_controller/describe_parameters /vehicle_1/example_controller/get_parameter_types /vehicle_1/example_controller/get_parameters /vehicle_1/example_controller/list_parameters /vehicle_1/example_controller/set_parameters /vehicle_1/example_controller/set_parameters_atomically /vehicle_1/gimbal_small_2d/describe_parameters /vehicle_1/gimbal_small_2d/get_parameter_types /vehicle_1/gimbal_small_2d/get_parameters /vehicle_1/gimbal_small_2d/list_parameters /vehicle_1/gimbal_small_2d/set_parameters /vehicle_1/gimbal_small_2d/set_parameters_atomically /vehicle_1/mavros/cmd/arming /vehicle_1/mavros/cmd/command /vehicle_1/mavros/cmd/command_int /vehicle_1/mavros/cmd/land /vehicle_1/mavros/cmd/set_home /vehicle_1/mavros/cmd/takeoff /vehicle_1/mavros/cmd/trigger_control /vehicle_1/mavros/mission/clear /vehicle_1/mavros/mission/pull /vehicle_1/mavros/mission/push /vehicle_1/mavros/mission/set_current /vehicle_1/mavros/set_mode /vehicle_1/mavros/set_stream_rate /vehicle_1/ros_bridge/describe_parameters /vehicle_1/ros_bridge/get_parameter_types /vehicle_1/ros_bridge/get_parameters /vehicle_1/ros_bridge/list_parameters /vehicle_1/ros_bridge/set_parameters /vehicle_1/ros_bridge/set_parameters_atomically /vehicle_1/set_camera_info","title":"Inspecting ROS2"},{"location":"tutorials/introduction/#summary-and-conclusion","text":"Congratulations! You have gotten to the end of this tutorial! Especially if this is your first time touching linux and using this stuff, a lot of the content here can be daunting - but feel free to read it over again and hopefully you will slowly come to understand what is going on. In this introductory tutorial, we have only really scratched the surface of a lot of the technologies that we've been using. If you wish to have a deeper understanding, I would recommend having a read through the official documentation of all of them. The docker and ROS2 tutorials are really good. But I wish to emphasise that you will hopefully not need an extremely deep understanding to use Starling. The next step after this tutorial is then to cover how you might actually build your own controller. This will go into more detail about Mavlink, ROS2 and Docker - including how to build your own projects and run them against the Fenswood Scenario Simulator!","title":"Summary and Conclusion"}]}